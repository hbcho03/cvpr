{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49438a6c-991a-4f73-a8f2-0c3bda332e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Dassl.pytorch'...\n",
      "remote: Enumerating objects: 2477, done.\u001b[K\n",
      "remote: Counting objects: 100% (993/993), done.\u001b[K\n",
      "remote: Compressing objects: 100% (288/288), done.\u001b[K\n",
      "remote: Total 2477 (delta 777), reused 861 (delta 705), pack-reused 1484 (from 1)\u001b[K\n",
      "Receiving objects: 100% (2477/2477), 428.00 KiB | 17.83 MiB/s, done.\n",
      "Resolving deltas: 100% (1658/1658), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/KaiyangZhou/Dassl.pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f6d5ad2-24c0-4e6a-976b-8937c1802a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Dassl.pytorch'\n",
      "/home/hbcho991/prometar/Dassl.pytorch\n"
     ]
    }
   ],
   "source": [
    "cd Dassl.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73a24076-6a39-486d-969f-637d30afe9cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flake8==3.7.9 (from -r requirements.txt (line 1))\n",
      "  Downloading flake8-3.7.9-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting yapf==0.29.0 (from -r requirements.txt (line 2))\n",
      "  Downloading yapf-0.29.0-py2.py3-none-any.whl.metadata (30 kB)\n",
      "Collecting isort==4.3.21 (from -r requirements.txt (line 3))\n",
      "  Downloading isort-4.3.21-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting yacs (from -r requirements.txt (line 4))\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting gdown (from -r requirements.txt (line 5))\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tb-nightly (from -r requirements.txt (line 6))\n",
      "  Downloading tb_nightly-2.14.0a20230808-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting future (from -r requirements.txt (line 7))\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting scipy (from -r requirements.txt (line 8))\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "Collecting scikit-learn (from -r requirements.txt (line 9))\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting tqdm (from -r requirements.txt (line 10))\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting ftfy (from -r requirements.txt (line 11))\n",
      "  Using cached ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting regex (from -r requirements.txt (line 12))\n",
      "  Using cached regex-2024.7.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting wilds==1.2.2 (from -r requirements.txt (line 13))\n",
      "  Downloading wilds-1.2.2-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tabulate (from -r requirements.txt (line 14))\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting entrypoints<0.4.0,>=0.3.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
      "  Downloading entrypoints-0.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting pyflakes<2.2.0,>=2.1.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
      "  Downloading pyflakes-2.1.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pycodestyle<2.6.0,>=2.5.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
      "  Downloading pycodestyle-2.5.0-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Collecting mccabe<0.7.0,>=0.6.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: numpy>=1.19.1 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (1.24.4)\n",
      "Collecting ogb>=1.2.6 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
      "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting outdated>=0.2.0 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
      "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pandas>=1.1.0 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
      "  Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pillow>=7.2.0 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (10.4.0)\n",
      "Requirement already satisfied: pytz>=2020.4 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2024.2)\n",
      "Requirement already satisfied: torch>=1.7.0 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (1.9.0+cu111)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (0.10.0+cu111)\n",
      "Requirement already satisfied: PyYAML in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from yacs->-r requirements.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from gdown->-r requirements.txt (line 5)) (4.12.3)\n",
      "Collecting filelock (from gdown->-r requirements.txt (line 5))\n",
      "  Downloading filelock-3.16.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: requests[socks] in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from gdown->-r requirements.txt (line 5)) (2.32.3)\n",
      "Collecting absl-py>=0.4 (from tb-nightly->-r requirements.txt (line 6))\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tb-nightly->-r requirements.txt (line 6))\n",
      "  Downloading grpcio-1.66.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tb-nightly->-r requirements.txt (line 6))\n",
      "  Downloading google_auth-2.34.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tb-nightly->-r requirements.txt (line 6))\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tb-nightly->-r requirements.txt (line 6))\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting protobuf>=3.19.6 (from tb-nightly->-r requirements.txt (line 6))\n",
      "  Downloading protobuf-5.28.0-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from tb-nightly->-r requirements.txt (line 6)) (72.1.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tb-nightly->-r requirements.txt (line 6))\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tb-nightly->-r requirements.txt (line 6))\n",
      "  Downloading werkzeug-3.0.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from tb-nightly->-r requirements.txt (line 6)) (0.43.0)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn->-r requirements.txt (line 9))\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->-r requirements.txt (line 9))\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from ftfy->-r requirements.txt (line 11)) (0.2.13)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tb-nightly->-r requirements.txt (line 6))\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tb-nightly->-r requirements.txt (line 6))\n",
      "  Downloading pyasn1_modules-0.4.1.tar.gz (310 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tb-nightly->-r requirements.txt (line 6))\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tb-nightly->-r requirements.txt (line 6))\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from markdown>=2.6.8->tb-nightly->-r requirements.txt (line 6)) (8.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from ogb>=1.2.6->wilds==1.2.2->-r requirements.txt (line 13)) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from ogb>=1.2.6->wilds==1.2.2->-r requirements.txt (line 13)) (2.2.2)\n",
      "Collecting littleutils (from outdated>=0.2.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
      "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.1 (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (2024.8.30)\n",
      "Requirement already satisfied: typing-extensions in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from werkzeug>=1.0.1->tb-nightly->-r requirements.txt (line 6)) (2.1.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from beautifulsoup4->gdown->-r requirements.txt (line 5)) (2.6)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown->-r requirements.txt (line 5))\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly->-r requirements.txt (line 6)) (3.20.1)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly->-r requirements.txt (line 6))\n",
      "  Downloading pyasn1-0.6.1.tar.gz (145 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tb-nightly->-r requirements.txt (line 6))\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading flake8-3.7.9-py2.py3-none-any.whl (69 kB)\n",
      "Downloading yapf-0.29.0-py2.py3-none-any.whl (185 kB)\n",
      "Downloading isort-4.3.21-py2.py3-none-any.whl (42 kB)\n",
      "Downloading wilds-1.2.2-py3-none-any.whl (92 kB)\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading tb_nightly-2.14.0a20230808-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached ftfy-6.2.3-py3-none-any.whl (43 kB)\n",
      "Using cached regex-2024.7.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (778 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
      "Downloading google_auth-2.34.0-py2.py3-none-any.whl (200 kB)\n",
      "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading grpcio-1.66.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
      "Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
      "Downloading pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.28.0-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading pycodestyle-2.5.0-py2.py3-none-any.whl (51 kB)\n",
      "Downloading pyflakes-2.1.1-py2.py3-none-any.whl (59 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
      "Downloading filelock-3.16.0-py3-none-any.whl (16 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Downloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: pyasn1-modules, pyasn1\n",
      "  Building wheel for pyasn1-modules (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyasn1-modules: filename=pyasn1_modules-0.4.1-py3-none-any.whl size=181223 sha256=0158ae4fa9871b05326c97bd05fcafd54329bbe8937da43ce51a9f4bccc6b9d0\n",
      "  Stored in directory: /home/hbcho991/.cache/pip/wheels/29/e1/27/a497865d96b1705ed544ce485c5770085336dc9e3ed716e3e2\n",
      "  Building wheel for pyasn1 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyasn1: filename=pyasn1-0.6.1-py3-none-any.whl size=83486 sha256=25b79351eb300e1b9e913f0a92eb997351be48d8776586857ff962493752b289\n",
      "  Stored in directory: /home/hbcho991/.cache/pip/wheels/b3/dd/69/475a061dc1161d711f3f19f413af53f0d1055dea682eb6a748\n",
      "Successfully built pyasn1-modules pyasn1\n",
      "Installing collected packages: yapf, mccabe, yacs, werkzeug, tzdata, tqdm, threadpoolctl, tensorboard-data-server, tabulate, scipy, regex, PySocks, pyflakes, pycodestyle, pyasn1, protobuf, oauthlib, littleutils, joblib, isort, grpcio, future, ftfy, filelock, entrypoints, cachetools, absl-py, scikit-learn, rsa, requests-oauthlib, pyasn1-modules, pandas, outdated, markdown, flake8, ogb, google-auth, gdown, wilds, google-auth-oauthlib, tb-nightly\n",
      "Successfully installed PySocks-1.7.1 absl-py-2.1.0 cachetools-5.5.0 entrypoints-0.3 filelock-3.16.0 flake8-3.7.9 ftfy-6.2.3 future-1.0.0 gdown-5.2.0 google-auth-2.34.0 google-auth-oauthlib-1.0.0 grpcio-1.66.1 isort-4.3.21 joblib-1.4.2 littleutils-0.2.4 markdown-3.7 mccabe-0.6.1 oauthlib-3.2.2 ogb-1.3.6 outdated-0.2.2 pandas-2.0.3 protobuf-5.28.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 pycodestyle-2.5.0 pyflakes-2.1.1 regex-2024.7.24 requests-oauthlib-2.0.0 rsa-4.9 scikit-learn-1.3.2 scipy-1.10.1 tabulate-0.9.0 tb-nightly-2.14.0a20230808 tensorboard-data-server-0.7.2 threadpoolctl-3.5.0 tqdm-4.66.5 tzdata-2024.1 werkzeug-3.0.4 wilds-1.2.2 yacs-0.1.8 yapf-0.29.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03cc7b2f-5bf9-45dd-81eb-05d5f1409c27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running develop\n",
      "/home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages/setuptools/command/develop.py:42: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` and ``easy_install``.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  easy_install.initialize_options(self)\n",
      "/home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "running egg_info\n",
      "creating dassl.egg-info\n",
      "writing dassl.egg-info/PKG-INFO\n",
      "writing dependency_links to dassl.egg-info/dependency_links.txt\n",
      "writing requirements to dassl.egg-info/requires.txt\n",
      "writing top-level names to dassl.egg-info/top_level.txt\n",
      "writing manifest file 'dassl.egg-info/SOURCES.txt'\n",
      "reading manifest file 'dassl.egg-info/SOURCES.txt'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'dassl.egg-info/SOURCES.txt'\n",
      "running build_ext\n",
      "Creating /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages/dassl.egg-link (link to .)\n",
      "Adding dassl 0.6.3 to easy-install.pth file\n",
      "\n",
      "Installed /home/hbcho991/prometar/Dassl.pytorch\n",
      "Processing dependencies for dassl==0.6.3\n",
      "Searching for tabulate==0.9.0\n",
      "Best match: tabulate 0.9.0\n",
      "Adding tabulate 0.9.0 to easy-install.pth file\n",
      "Installing tabulate script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for wilds==1.2.2\n",
      "Best match: wilds 1.2.2\n",
      "Adding wilds 1.2.2 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for regex==2024.7.24\n",
      "Best match: regex 2024.7.24\n",
      "Adding regex 2024.7.24 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for ftfy==6.2.3\n",
      "Best match: ftfy 6.2.3\n",
      "Adding ftfy 6.2.3 to easy-install.pth file\n",
      "Installing ftfy script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for tqdm==4.66.5\n",
      "Best match: tqdm 4.66.5\n",
      "Adding tqdm 4.66.5 to easy-install.pth file\n",
      "Installing tqdm script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for scikit-learn==1.3.2\n",
      "Best match: scikit-learn 1.3.2\n",
      "Adding scikit-learn 1.3.2 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for scipy==1.10.1\n",
      "Best match: scipy 1.10.1\n",
      "Adding scipy 1.10.1 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for future==1.0.0\n",
      "Best match: future 1.0.0\n",
      "Adding future 1.0.0 to easy-install.pth file\n",
      "Installing futurize script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "Installing pasteurize script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for tb-nightly==2.14.0a20230808\n",
      "Best match: tb-nightly 2.14.0a20230808\n",
      "Adding tb-nightly 2.14.0a20230808 to easy-install.pth file\n",
      "Installing tensorboard script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for gdown==5.2.0\n",
      "Best match: gdown 5.2.0\n",
      "Adding gdown 5.2.0 to easy-install.pth file\n",
      "Installing gdown script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for yacs==0.1.8\n",
      "Best match: yacs 0.1.8\n",
      "Adding yacs 0.1.8 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for isort==4.3.21\n",
      "Best match: isort 4.3.21\n",
      "Adding isort 4.3.21 to easy-install.pth file\n",
      "Installing isort script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for yapf==0.29.0\n",
      "Best match: yapf 0.29.0\n",
      "Adding yapf 0.29.0 to easy-install.pth file\n",
      "Installing yapf script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for flake8==3.7.9\n",
      "Best match: flake8 3.7.9\n",
      "Adding flake8 3.7.9 to easy-install.pth file\n",
      "Installing flake8 script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for torchvision==0.10.0+cu111\n",
      "Best match: torchvision 0.10.0+cu111\n",
      "Adding torchvision 0.10.0+cu111 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for torch==1.9.0+cu111\n",
      "Best match: torch 1.9.0+cu111\n",
      "Adding torch 1.9.0+cu111 to easy-install.pth file\n",
      "Installing convert-caffe2-to-onnx script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "Installing convert-onnx-to-caffe2 script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for pytz==2024.2\n",
      "Best match: pytz 2024.2\n",
      "Adding pytz 2024.2 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for pillow==10.4.0\n",
      "Best match: pillow 10.4.0\n",
      "Adding pillow 10.4.0 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for pandas==2.0.3\n",
      "Best match: pandas 2.0.3\n",
      "Adding pandas 2.0.3 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for outdated==0.2.2\n",
      "Best match: outdated 0.2.2\n",
      "Adding outdated 0.2.2 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for ogb==1.3.6\n",
      "Best match: ogb 1.3.6\n",
      "Adding ogb 1.3.6 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for numpy==1.24.4\n",
      "Best match: numpy 1.24.4\n",
      "Adding numpy 1.24.4 to easy-install.pth file\n",
      "Installing f2py script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "Installing f2py3 script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "Installing f2py3.8 script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for wcwidth==0.2.13\n",
      "Best match: wcwidth 0.2.13\n",
      "Adding wcwidth 0.2.13 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for threadpoolctl==3.5.0\n",
      "Best match: threadpoolctl 3.5.0\n",
      "Adding threadpoolctl 3.5.0 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for joblib==1.4.2\n",
      "Best match: joblib 1.4.2\n",
      "Adding joblib 1.4.2 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for wheel==0.43.0\n",
      "Best match: wheel 0.43.0\n",
      "Adding wheel 0.43.0 to easy-install.pth file\n",
      "Installing wheel script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages/setuptools/_vendor\n",
      "Searching for werkzeug==3.0.4\n",
      "Best match: werkzeug 3.0.4\n",
      "Adding werkzeug 3.0.4 to easy-install.pth file\n",
      "detected new path './setuptools/_vendor'\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for tensorboard-data-server==0.7.2\n",
      "Best match: tensorboard-data-server 0.7.2\n",
      "Adding tensorboard-data-server 0.7.2 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for setuptools==72.1.0\n",
      "Best match: setuptools 72.1.0\n",
      "Adding setuptools 72.1.0 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for requests==2.32.3\n",
      "Best match: requests 2.32.3\n",
      "Adding requests 2.32.3 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for protobuf==5.28.0\n",
      "Best match: protobuf 5.28.0\n",
      "Adding protobuf 5.28.0 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for Markdown==3.7\n",
      "Best match: Markdown 3.7\n",
      "Adding Markdown 3.7 to easy-install.pth file\n",
      "Installing markdown_py script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for google-auth-oauthlib==1.0.0\n",
      "Best match: google-auth-oauthlib 1.0.0\n",
      "Adding google-auth-oauthlib 1.0.0 to easy-install.pth file\n",
      "Installing google-oauthlib-tool script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for google-auth==2.34.0\n",
      "Best match: google-auth 2.34.0\n",
      "Adding google-auth 2.34.0 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for grpcio==1.66.1\n",
      "Best match: grpcio 1.66.1\n",
      "Adding grpcio 1.66.1 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for absl-py==2.1.0\n",
      "Best match: absl-py 2.1.0\n",
      "Adding absl-py 2.1.0 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for filelock==3.16.0\n",
      "Best match: filelock 3.16.0\n",
      "Adding filelock 3.16.0 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for beautifulsoup4==4.12.3\n",
      "Best match: beautifulsoup4 4.12.3\n",
      "Adding beautifulsoup4 4.12.3 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for PyYAML==6.0.2\n",
      "Best match: PyYAML 6.0.2\n",
      "Adding PyYAML 6.0.2 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for mccabe==0.6.1\n",
      "Best match: mccabe 0.6.1\n",
      "Adding mccabe 0.6.1 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for pycodestyle==2.5.0\n",
      "Best match: pycodestyle 2.5.0\n",
      "Adding pycodestyle 2.5.0 to easy-install.pth file\n",
      "Installing pycodestyle script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for pyflakes==2.1.1\n",
      "Best match: pyflakes 2.1.1\n",
      "Adding pyflakes 2.1.1 to easy-install.pth file\n",
      "Installing pyflakes script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for entrypoints==0.3\n",
      "Best match: entrypoints 0.3\n",
      "Adding entrypoints 0.3 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for typing-extensions==4.12.2\n",
      "Best match: typing-extensions 4.12.2\n",
      "typing-extensions 4.12.2 is already the active version in easy-install.pth\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages/setuptools/_vendor\n",
      "Searching for tzdata==2024.1\n",
      "Best match: tzdata 2024.1\n",
      "Adding tzdata 2024.1 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for python-dateutil==2.9.0.post0\n",
      "Best match: python-dateutil 2.9.0.post0\n",
      "Adding python-dateutil 2.9.0.post0 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for littleutils==0.2.4\n",
      "Best match: littleutils 0.2.4\n",
      "Adding littleutils 0.2.4 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for urllib3==2.2.2\n",
      "Best match: urllib3 2.2.2\n",
      "Adding urllib3 2.2.2 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for six==1.16.0\n",
      "Best match: six 1.16.0\n",
      "Adding six 1.16.0 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for MarkupSafe==2.1.5\n",
      "Best match: MarkupSafe 2.1.5\n",
      "Adding MarkupSafe 2.1.5 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for certifi==2024.8.30\n",
      "Best match: certifi 2024.8.30\n",
      "Adding certifi 2024.8.30 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for idna==3.8\n",
      "Best match: idna 3.8\n",
      "Adding idna 3.8 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for charset-normalizer==3.3.2\n",
      "Best match: charset-normalizer 3.3.2\n",
      "Adding charset-normalizer 3.3.2 to easy-install.pth file\n",
      "Installing normalizer script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for importlib-metadata==8.4.0\n",
      "Best match: importlib-metadata 8.4.0\n",
      "Adding importlib-metadata 8.4.0 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for requests-oauthlib==2.0.0\n",
      "Best match: requests-oauthlib 2.0.0\n",
      "Adding requests-oauthlib 2.0.0 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for rsa==4.9\n",
      "Best match: rsa 4.9\n",
      "Adding rsa 4.9 to easy-install.pth file\n",
      "Installing pyrsa-decrypt script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "Installing pyrsa-encrypt script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "Installing pyrsa-keygen script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "Installing pyrsa-priv2pub script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "Installing pyrsa-sign script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "Installing pyrsa-verify script to /home/hbcho991/.conda/envs/prometar/bin\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for pyasn1-modules==0.4.1\n",
      "Best match: pyasn1-modules 0.4.1\n",
      "Adding pyasn1-modules 0.4.1 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for cachetools==5.5.0\n",
      "Best match: cachetools 5.5.0\n",
      "Adding cachetools 5.5.0 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for PySocks==1.7.1\n",
      "Best match: PySocks 1.7.1\n",
      "Adding PySocks 1.7.1 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for soupsieve==2.6\n",
      "Best match: soupsieve 2.6\n",
      "Adding soupsieve 2.6 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for zipp==3.20.1\n",
      "Best match: zipp 3.20.1\n",
      "Adding zipp 3.20.1 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for oauthlib==3.2.2\n",
      "Best match: oauthlib 3.2.2\n",
      "Adding oauthlib 3.2.2 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Searching for pyasn1==0.6.1\n",
      "Best match: pyasn1 0.6.1\n",
      "Adding pyasn1 0.6.1 to easy-install.pth file\n",
      "\n",
      "Using /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages\n",
      "Finished processing dependencies for dassl==0.6.3\n"
     ]
    }
   ],
   "source": [
    "!python setup.py develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6b005e7-bac7-4863-b700-d5412b147cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hbcho991/prometar\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4798782-38ee-4578-9571-d8dc7180bae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ProMetaR'...\n",
      "remote: Enumerating objects: 129, done.\u001b[K\n",
      "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
      "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
      "remote: Total 129 (delta 37), reused 101 (delta 24), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (129/129), 2.90 MiB | 5.24 MiB/s, done.\n",
      "Resolving deltas: 100% (37/37), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mlvlab/ProMetaR.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d69520d-09a1-4f8b-805f-fc8c8f8ab478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hbcho991/prometar/ProMetaR\n"
     ]
    }
   ],
   "source": [
    "cd ProMetaR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2306e214-51a5-4097-a230-d9dc54f7fd55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy==6.1.1 (from -r requirements.txt (line 1))\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: regex in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (4.66.5)\n",
      "Collecting learn2learn==0.2.0 (from -r requirements.txt (line 4))\n",
      "  Downloading learn2learn-0.2.0.tar.gz (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from ftfy==6.1.1->-r requirements.txt (line 1)) (0.2.13)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (1.24.4)\n",
      "Collecting gym>=0.14.0 (from learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (1.9.0+cu111)\n",
      "Requirement already satisfied: torchvision>=0.3.0 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (0.10.0+cu111)\n",
      "Requirement already satisfied: scipy in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (1.10.1)\n",
      "Requirement already satisfied: requests in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (2.32.3)\n",
      "Collecting gsutil (from learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading gsutil-5.30.tar.gz (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting qpth>=0.0.15 (from learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading qpth-0.0.18.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cloudpickle>=1.2.0 (from gym>=0.14.0->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gym-notices>=0.0.4 (from gym>=0.14.0->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from gym>=0.14.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (8.4.0)\n",
      "Collecting cvxpy>=1.1.0 (from qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading cvxpy-1.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from torchvision>=0.3.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (10.4.0)\n",
      "Collecting argcomplete>=1.9.4 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading argcomplete-3.5.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting crcmod>=1.7 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fasteners>=0.14.1 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting gcs-oauth2-boto-plugin>=3.2 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading gcs-oauth2-boto-plugin-3.2.tar.gz (22 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-apitools>=0.5.32 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading google_apitools-0.5.32-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httplib2==0.20.4 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading httplib2-0.20.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting google-reauth>=0.1.0 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading google_reauth-0.1.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting monotonic>=1.4 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pyOpenSSL>=0.13 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading pyOpenSSL-24.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting retry_decorator>=1.0.0 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading retry_decorator-1.1.1.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.16.0 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.16.0)\n",
      "Collecting google-auth==2.17.0 (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading google_auth-2.17.0-py2.py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting google-auth-httplib2>=0.2.0 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.9)\n",
      "Collecting aiohttp<4.0.0dev,>=3.6.2 (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading aiohttp-3.10.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2==0.20.4->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (2024.8.30)\n",
      "Collecting osqp>=0.6.2 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading osqp-0.6.7.post1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting ecos>=2 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading ecos-2.0.14-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
      "Collecting clarabel>=0.5.0 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading clarabel-0.9.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting scs>=3.2.4.post1 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading scs-3.2.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting boto>=2.29.1 (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading boto-2.49.0-py2.py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting oauth2client>=2.2.0 (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from rsa<5,>=3.1.4->google-auth==2.17.0->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.6.1)\n",
      "Collecting pyu2f (from google-reauth>=0.1.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading pyu2f-0.1.5.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym>=0.14.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.20.1)\n",
      "Collecting cryptography<44,>=41.0.5 (from pyOpenSSL>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading cryptography-43.0.1-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading yarl-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from cryptography<44,>=41.0.5->pyOpenSSL>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.17.1)\n",
      "Collecting qdldl (from osqp>=0.6.2->cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading qdldl-0.1.7.post4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pycparser in /home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages (from cffi>=1.12->cryptography<44,>=41.0.5->pyOpenSSL>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.22)\n",
      "Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Downloading google_auth-2.17.0-py2.py3-none-any.whl (178 kB)\n",
      "Downloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
      "Downloading argcomplete-3.5.0-py3-none-any.whl (43 kB)\n",
      "Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading cvxpy-1.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Downloading google_apitools-0.5.32-py3-none-any.whl (135 kB)\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading google_reauth-0.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading pyOpenSSL-24.2.1-py3-none-any.whl (58 kB)\n",
      "Downloading aiohttp-3.10.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading clarabel-0.9.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cryptography-43.0.1-cp37-abi3-manylinux_2_28_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading ecos-2.0.14-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (221 kB)\n",
      "Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Downloading osqp-0.6.7.post1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Downloading scs-3.2.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
      "Downloading multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Downloading yarl-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (460 kB)\n",
      "Downloading qdldl-0.1.7.post4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: learn2learn, gym, qpth, gsutil, crcmod, gcs-oauth2-boto-plugin, retry_decorator, pyu2f\n",
      "  Building wheel for learn2learn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for learn2learn: filename=learn2learn-0.2.0-cp38-cp38-linux_x86_64.whl size=1402422 sha256=441b9c36f84756447ed828f3cfc85b89b8d37970dd482a98ef375f8fe50f821f\n",
      "  Stored in directory: /home/hbcho991/.cache/pip/wheels/42/4a/b8/3c38ddaf860acfe33801dd466fcb8a777f1334e78b43269832\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827622 sha256=97c564fdbf5c7f85b8d009b7703c3a12618bf43dc5581dc03524953efb0ce205\n",
      "  Stored in directory: /home/hbcho991/.cache/pip/wheels/17/79/65/7afedc162d858b02708a3b8f7a6dd5b1000dcd5b0f894f7cc1\n",
      "  Building wheel for qpth (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for qpth: filename=qpth-0.0.18-py3-none-any.whl size=19545 sha256=d56c5a6a7104628fee459a006d69d94711e09595a0eef63c19b1941c690dd867\n",
      "  Stored in directory: /home/hbcho991/.cache/pip/wheels/67/94/1c/fb03682ccfed3eefde8000558afff06b3ef26017f6f93478f2\n",
      "  Building wheel for gsutil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gsutil: filename=gsutil-5.30-py3-none-any.whl size=3789295 sha256=8f0a5375a46ec38cc448d9228ab9caabb609469313a9f27af2ffd2f83bff9214\n",
      "  Stored in directory: /home/hbcho991/.cache/pip/wheels/b0/c1/ce/942b46a53b2e7257a8b20ad1a11f4e481e40bfe5c4c90e8598\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp38-cp38-linux_x86_64.whl size=30974 sha256=6a931788664c9fa3200945503b3789b2d450a8981dd7cacbc99c741652fc8ee0\n",
      "  Stored in directory: /home/hbcho991/.cache/pip/wheels/ca/5a/02/f3acf982a026f3319fb3e798a8dca2d48fafee7761788562e9\n",
      "  Building wheel for gcs-oauth2-boto-plugin (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gcs-oauth2-boto-plugin: filename=gcs_oauth2_boto_plugin-3.2-py3-none-any.whl size=24466 sha256=9864c24c2fa99a4fc820605b4e7829a8085692e7fae2773734a220ed432ea5e1\n",
      "  Stored in directory: /home/hbcho991/.cache/pip/wheels/c9/93/3f/3471c73cf01db2ef3aeafdc116ff3b7cc9a75258d2e8a2f311\n",
      "  Building wheel for retry_decorator (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for retry_decorator: filename=retry_decorator-1.1.1-py2.py3-none-any.whl size=3636 sha256=63e4002fa4416076018903b89308daeabc4ffea6a67b59e1e68076ca11239c72\n",
      "  Stored in directory: /home/hbcho991/.cache/pip/wheels/1e/bb/cb/5d7a05561d7ec60b8c0dc65f156a33909a3eb5c56340959fa9\n",
      "  Building wheel for pyu2f (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyu2f: filename=pyu2f-0.1.5-py3-none-any.whl size=39402 sha256=5a345a75b9a3f24429417b5871235b46adb526172dd2bd306eedb5e74de4b4ea\n",
      "  Stored in directory: /home/hbcho991/.cache/pip/wheels/8d/38/8f/378b4491d760a176f261d56310fcb051e53090a894534acbf0\n",
      "Successfully built learn2learn gym qpth gsutil crcmod gcs-oauth2-boto-plugin retry_decorator pyu2f\n",
      "Installing collected packages: retry_decorator, monotonic, gym-notices, crcmod, boto, rsa, pyu2f, pyparsing, multidict, ftfy, frozenlist, fasteners, cloudpickle, async-timeout, argcomplete, aiohappyeyeballs, yarl, scs, qdldl, httplib2, gym, google-reauth, google-auth, ecos, cryptography, clarabel, aiosignal, pyOpenSSL, osqp, oauth2client, google-auth-httplib2, aiohttp, google-apitools, gcs-oauth2-boto-plugin, cvxpy, qpth, gsutil, learn2learn\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.9\n",
      "    Uninstalling rsa-4.9:\n",
      "      Successfully uninstalled rsa-4.9\n",
      "  Attempting uninstall: ftfy\n",
      "    Found existing installation: ftfy 6.2.3\n",
      "    Uninstalling ftfy-6.2.3:\n",
      "      Successfully uninstalled ftfy-6.2.3\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.34.0\n",
      "    Uninstalling google-auth-2.34.0:\n",
      "      Successfully uninstalled google-auth-2.34.0\n",
      "Successfully installed aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 argcomplete-3.5.0 async-timeout-4.0.3 boto-2.49.0 clarabel-0.9.0 cloudpickle-3.0.0 crcmod-1.7 cryptography-43.0.1 cvxpy-1.5.2 ecos-2.0.14 fasteners-0.19 frozenlist-1.4.1 ftfy-6.1.1 gcs-oauth2-boto-plugin-3.2 google-apitools-0.5.32 google-auth-2.17.0 google-auth-httplib2-0.2.0 google-reauth-0.1.1 gsutil-5.30 gym-0.26.2 gym-notices-0.0.8 httplib2-0.20.4 learn2learn-0.2.0 monotonic-1.6 multidict-6.1.0 oauth2client-4.1.3 osqp-0.6.7.post1 pyOpenSSL-24.2.1 pyparsing-3.1.4 pyu2f-0.1.5 qdldl-0.1.7.post4 qpth-0.0.18 retry_decorator-1.1.1 rsa-4.7.2 scs-3.2.7 yarl-1.11.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb80031e-ab7b-4731-975b-a5485bc99fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools==59.5.0\n",
      "  Downloading setuptools-59.5.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.4/952.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 72.1.0\n",
      "    Uninstalling setuptools-72.1.0:\n",
      "      Successfully uninstalled setuptools-72.1.0\n",
      "Successfully installed setuptools-59.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install setuptools==59.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f75b0f-7c9b-4688-bedc-40918c9da1ac",
   "metadata": {},
   "source": [
    "# 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5888c659-47a6-4343-b53a-fc4cd7402a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hbcho991/prometar/ProMetaR\n"
     ]
    }
   ],
   "source": [
    "%cd /home/hbcho991/prometar/ProMetaR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2873cdb-8f85-4c66-8740-453897786d44",
   "metadata": {},
   "source": [
    "### seed1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb945fd7-d869-4353-9518-a79e76ef8b09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this job and save the output to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']\n",
      "output_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: base\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_1.pkl\n",
      "SUBSAMPLE BASE CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,053\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.ctx', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.VPT', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1/tensorboard)\n",
      "/home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "epoch [1/10] batch [20/204] time 0.267 (0.287) data 0.000 (0.015) loss 0.0006 (0.0011) lr 1.0000e-05 eta 0:09:40\n",
      "epoch [1/10] batch [40/204] time 0.275 (0.277) data 0.000 (0.008) loss 0.0025 (0.0014) lr 1.0000e-05 eta 0:09:13\n",
      "epoch [1/10] batch [60/204] time 0.271 (0.276) data 0.000 (0.006) loss 0.0001 (0.0014) lr 1.0000e-05 eta 0:09:07\n",
      "epoch [1/10] batch [80/204] time 0.280 (0.278) data 0.000 (0.004) loss 0.0019 (0.0013) lr 1.0000e-05 eta 0:09:04\n",
      "epoch [1/10] batch [100/204] time 0.266 (0.278) data 0.000 (0.004) loss 0.0008 (0.0013) lr 1.0000e-05 eta 0:08:58\n",
      "epoch [1/10] batch [120/204] time 0.265 (0.277) data 0.000 (0.003) loss 0.0000 (0.0013) lr 1.0000e-05 eta 0:08:51\n",
      "epoch [1/10] batch [140/204] time 0.271 (0.276) data 0.005 (0.003) loss 0.0017 (0.0012) lr 1.0000e-05 eta 0:08:43\n",
      "epoch [1/10] batch [160/204] time 0.273 (0.275) data 0.003 (0.003) loss 0.0019 (0.0012) lr 1.0000e-05 eta 0:08:36\n",
      "epoch [1/10] batch [180/204] time 0.268 (0.274) data 0.000 (0.002) loss 0.0009 (0.0012) lr 1.0000e-05 eta 0:08:29\n",
      "epoch [1/10] batch [200/204] time 0.271 (0.273) data 0.000 (0.002) loss 0.0009 (0.0012) lr 1.0000e-05 eta 0:08:23\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [2/10] batch [20/204] time 0.271 (0.281) data 0.000 (0.013) loss 0.0008 (0.0009) lr 2.5000e-03 eta 0:08:31\n",
      "epoch [2/10] batch [40/204] time 0.273 (0.277) data 0.000 (0.007) loss 0.0006 (0.0010) lr 2.5000e-03 eta 0:08:16\n",
      "epoch [2/10] batch [60/204] time 0.266 (0.274) data 0.000 (0.005) loss 0.0004 (0.0009) lr 2.5000e-03 eta 0:08:07\n",
      "epoch [2/10] batch [80/204] time 0.273 (0.273) data 0.000 (0.004) loss 0.0020 (0.0009) lr 2.5000e-03 eta 0:07:59\n",
      "epoch [2/10] batch [100/204] time 0.268 (0.272) data 0.000 (0.003) loss 0.0002 (0.0009) lr 2.5000e-03 eta 0:07:52\n",
      "epoch [2/10] batch [120/204] time 0.276 (0.272) data 0.002 (0.003) loss 0.0006 (0.0009) lr 2.5000e-03 eta 0:07:46\n",
      "epoch [2/10] batch [140/204] time 0.271 (0.271) data 0.000 (0.002) loss 0.0001 (0.0009) lr 2.5000e-03 eta 0:07:40\n",
      "epoch [2/10] batch [160/204] time 0.266 (0.269) data 0.000 (0.002) loss 0.0002 (0.0009) lr 2.5000e-03 eta 0:07:30\n",
      "epoch [2/10] batch [180/204] time 0.271 (0.269) data 0.000 (0.002) loss 0.0005 (0.0009) lr 2.5000e-03 eta 0:07:25\n",
      "epoch [2/10] batch [200/204] time 0.266 (0.269) data 0.000 (0.002) loss 0.0009 (0.0009) lr 2.5000e-03 eta 0:07:20\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [3/10] batch [20/204] time 0.270 (0.281) data 0.003 (0.013) loss 0.0006 (0.0010) lr 2.4388e-03 eta 0:07:32\n",
      "epoch [3/10] batch [40/204] time 0.269 (0.277) data 0.000 (0.007) loss 0.0004 (0.0010) lr 2.4388e-03 eta 0:07:20\n",
      "epoch [3/10] batch [60/204] time 0.269 (0.274) data 0.000 (0.005) loss 0.0017 (0.0009) lr 2.4388e-03 eta 0:07:10\n",
      "epoch [3/10] batch [80/204] time 0.267 (0.273) data 0.000 (0.004) loss 0.0002 (0.0009) lr 2.4388e-03 eta 0:07:04\n",
      "epoch [3/10] batch [100/204] time 0.268 (0.273) data 0.000 (0.004) loss 0.0008 (0.0009) lr 2.4388e-03 eta 0:06:58\n",
      "epoch [3/10] batch [120/204] time 0.272 (0.273) data 0.005 (0.003) loss 0.0002 (0.0009) lr 2.4388e-03 eta 0:06:52\n",
      "epoch [3/10] batch [140/204] time 0.259 (0.272) data 0.000 (0.003) loss 0.0004 (0.0009) lr 2.4388e-03 eta 0:06:46\n",
      "epoch [3/10] batch [160/204] time 0.267 (0.272) data 0.000 (0.003) loss 0.0007 (0.0009) lr 2.4388e-03 eta 0:06:40\n",
      "epoch [3/10] batch [180/204] time 0.264 (0.272) data 0.000 (0.002) loss 0.0019 (0.0009) lr 2.4388e-03 eta 0:06:35\n",
      "epoch [3/10] batch [200/204] time 0.291 (0.273) data 0.004 (0.002) loss 0.0014 (0.0009) lr 2.4388e-03 eta 0:06:30\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [4/10] batch [20/204] time 0.272 (0.286) data 0.004 (0.014) loss 0.0014 (0.0008) lr 2.2613e-03 eta 0:06:43\n",
      "epoch [4/10] batch [40/204] time 0.268 (0.278) data 0.000 (0.007) loss 0.0000 (0.0007) lr 2.2613e-03 eta 0:06:25\n",
      "epoch [4/10] batch [60/204] time 0.278 (0.278) data 0.000 (0.005) loss 0.0016 (0.0008) lr 2.2613e-03 eta 0:06:19\n",
      "epoch [4/10] batch [80/204] time 0.274 (0.277) data 0.000 (0.004) loss 0.0011 (0.0008) lr 2.2613e-03 eta 0:06:13\n",
      "epoch [4/10] batch [100/204] time 0.265 (0.279) data 0.000 (0.004) loss 0.0009 (0.0008) lr 2.2613e-03 eta 0:06:09\n",
      "epoch [4/10] batch [120/204] time 0.263 (0.277) data 0.000 (0.003) loss 0.0019 (0.0008) lr 2.2613e-03 eta 0:06:02\n",
      "epoch [4/10] batch [140/204] time 0.266 (0.277) data 0.000 (0.003) loss 0.0022 (0.0008) lr 2.2613e-03 eta 0:05:56\n",
      "epoch [4/10] batch [160/204] time 0.263 (0.276) data 0.000 (0.003) loss 0.0006 (0.0008) lr 2.2613e-03 eta 0:05:49\n",
      "epoch [4/10] batch [180/204] time 0.249 (0.275) data 0.000 (0.002) loss 0.0006 (0.0008) lr 2.2613e-03 eta 0:05:43\n",
      "epoch [4/10] batch [200/204] time 0.269 (0.274) data 0.000 (0.002) loss 0.0009 (0.0008) lr 2.2613e-03 eta 0:05:36\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [5/10] batch [20/204] time 0.262 (0.267) data 0.000 (0.013) loss 0.0020 (0.0008) lr 1.9847e-03 eta 0:05:22\n",
      "epoch [5/10] batch [40/204] time 0.277 (0.278) data 0.000 (0.007) loss 0.0005 (0.0007) lr 1.9847e-03 eta 0:05:29\n",
      "epoch [5/10] batch [60/204] time 0.281 (0.279) data 0.001 (0.005) loss 0.0021 (0.0007) lr 1.9847e-03 eta 0:05:25\n",
      "epoch [5/10] batch [80/204] time 0.268 (0.279) data 0.000 (0.004) loss 0.0004 (0.0007) lr 1.9847e-03 eta 0:05:19\n",
      "epoch [5/10] batch [100/204] time 0.272 (0.278) data 0.000 (0.003) loss 0.0002 (0.0008) lr 1.9847e-03 eta 0:05:12\n",
      "epoch [5/10] batch [120/204] time 0.268 (0.277) data 0.000 (0.003) loss 0.0011 (0.0008) lr 1.9847e-03 eta 0:05:05\n",
      "epoch [5/10] batch [140/204] time 0.281 (0.276) data 0.000 (0.003) loss 0.0000 (0.0008) lr 1.9847e-03 eta 0:04:59\n",
      "epoch [5/10] batch [160/204] time 0.268 (0.275) data 0.000 (0.002) loss 0.0005 (0.0008) lr 1.9847e-03 eta 0:04:52\n",
      "epoch [5/10] batch [180/204] time 0.274 (0.275) data 0.000 (0.002) loss 0.0017 (0.0008) lr 1.9847e-03 eta 0:04:47\n",
      "epoch [5/10] batch [200/204] time 0.260 (0.274) data 0.000 (0.002) loss 0.0000 (0.0008) lr 1.9847e-03 eta 0:04:40\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [6/10] batch [20/204] time 0.270 (0.280) data 0.000 (0.011) loss 0.0007 (0.0008) lr 1.6363e-03 eta 0:04:40\n",
      "epoch [6/10] batch [40/204] time 0.277 (0.278) data 0.004 (0.006) loss 0.0005 (0.0008) lr 1.6363e-03 eta 0:04:32\n",
      "epoch [6/10] batch [60/204] time 0.276 (0.275) data 0.000 (0.005) loss 0.0010 (0.0008) lr 1.6363e-03 eta 0:04:23\n",
      "epoch [6/10] batch [80/204] time 0.270 (0.273) data 0.000 (0.004) loss 0.0009 (0.0008) lr 1.6363e-03 eta 0:04:16\n",
      "epoch [6/10] batch [100/204] time 0.276 (0.273) data 0.000 (0.003) loss 0.0001 (0.0008) lr 1.6363e-03 eta 0:04:10\n",
      "epoch [6/10] batch [120/204] time 0.279 (0.272) data 0.002 (0.003) loss 0.0004 (0.0008) lr 1.6363e-03 eta 0:04:05\n",
      "epoch [6/10] batch [140/204] time 0.276 (0.272) data 0.000 (0.002) loss 0.0017 (0.0007) lr 1.6363e-03 eta 0:03:59\n",
      "epoch [6/10] batch [160/204] time 0.266 (0.272) data 0.000 (0.002) loss 0.0002 (0.0007) lr 1.6363e-03 eta 0:03:53\n",
      "epoch [6/10] batch [180/204] time 0.267 (0.271) data 0.001 (0.002) loss 0.0011 (0.0007) lr 1.6363e-03 eta 0:03:47\n",
      "epoch [6/10] batch [200/204] time 0.268 (0.271) data 0.000 (0.002) loss 0.0005 (0.0007) lr 1.6363e-03 eta 0:03:42\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [7/10] batch [20/204] time 0.265 (0.280) data 0.000 (0.011) loss 0.0018 (0.0010) lr 1.2500e-03 eta 0:03:42\n",
      "epoch [7/10] batch [40/204] time 0.264 (0.273) data 0.000 (0.006) loss 0.0001 (0.0008) lr 1.2500e-03 eta 0:03:32\n",
      "epoch [7/10] batch [60/204] time 0.274 (0.272) data 0.000 (0.004) loss 0.0000 (0.0007) lr 1.2500e-03 eta 0:03:25\n",
      "epoch [7/10] batch [80/204] time 0.264 (0.271) data 0.000 (0.003) loss 0.0005 (0.0007) lr 1.2500e-03 eta 0:03:19\n",
      "epoch [7/10] batch [100/204] time 0.266 (0.271) data 0.002 (0.003) loss 0.0004 (0.0007) lr 1.2500e-03 eta 0:03:14\n",
      "epoch [7/10] batch [120/204] time 0.267 (0.271) data 0.003 (0.002) loss 0.0008 (0.0007) lr 1.2500e-03 eta 0:03:08\n",
      "epoch [7/10] batch [140/204] time 0.274 (0.270) data 0.001 (0.002) loss 0.0008 (0.0007) lr 1.2500e-03 eta 0:03:02\n",
      "epoch [7/10] batch [160/204] time 0.269 (0.270) data 0.000 (0.002) loss 0.0003 (0.0007) lr 1.2500e-03 eta 0:02:57\n",
      "epoch [7/10] batch [180/204] time 0.266 (0.270) data 0.000 (0.002) loss 0.0011 (0.0007) lr 1.2500e-03 eta 0:02:51\n",
      "epoch [7/10] batch [200/204] time 0.265 (0.270) data 0.000 (0.002) loss 0.0001 (0.0007) lr 1.2500e-03 eta 0:02:46\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [8/10] batch [20/204] time 0.273 (0.277) data 0.000 (0.010) loss 0.0011 (0.0008) lr 8.6373e-04 eta 0:02:43\n",
      "epoch [8/10] batch [40/204] time 0.272 (0.275) data 0.000 (0.005) loss 0.0003 (0.0007) lr 8.6373e-04 eta 0:02:37\n",
      "epoch [8/10] batch [60/204] time 0.271 (0.273) data 0.000 (0.004) loss 0.0001 (0.0007) lr 8.6373e-04 eta 0:02:30\n",
      "epoch [8/10] batch [80/204] time 0.270 (0.272) data 0.000 (0.003) loss 0.0004 (0.0007) lr 8.6373e-04 eta 0:02:24\n",
      "epoch [8/10] batch [100/204] time 0.288 (0.273) data 0.000 (0.002) loss 0.0015 (0.0007) lr 8.6373e-04 eta 0:02:19\n",
      "epoch [8/10] batch [120/204] time 0.266 (0.273) data 0.000 (0.002) loss 0.0001 (0.0007) lr 8.6373e-04 eta 0:02:14\n",
      "epoch [8/10] batch [140/204] time 0.265 (0.272) data 0.000 (0.002) loss 0.0019 (0.0007) lr 8.6373e-04 eta 0:02:08\n",
      "epoch [8/10] batch [160/204] time 0.268 (0.272) data 0.000 (0.002) loss 0.0006 (0.0007) lr 8.6373e-04 eta 0:02:02\n",
      "epoch [8/10] batch [180/204] time 0.282 (0.271) data 0.003 (0.002) loss 0.0006 (0.0007) lr 8.6373e-04 eta 0:01:57\n",
      "epoch [8/10] batch [200/204] time 0.259 (0.271) data 0.000 (0.002) loss 0.0002 (0.0007) lr 8.6373e-04 eta 0:01:51\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [9/10] batch [20/204] time 0.269 (0.281) data 0.000 (0.012) loss 0.0008 (0.0007) lr 5.1527e-04 eta 0:01:48\n",
      "epoch [9/10] batch [40/204] time 0.276 (0.277) data 0.004 (0.006) loss 0.0001 (0.0007) lr 5.1527e-04 eta 0:01:41\n",
      "epoch [9/10] batch [60/204] time 0.265 (0.274) data 0.000 (0.004) loss 0.0001 (0.0008) lr 5.1527e-04 eta 0:01:35\n",
      "epoch [9/10] batch [80/204] time 0.270 (0.273) data 0.000 (0.003) loss 0.0007 (0.0008) lr 5.1527e-04 eta 0:01:29\n",
      "epoch [9/10] batch [100/204] time 0.268 (0.272) data 0.000 (0.003) loss 0.0013 (0.0008) lr 5.1527e-04 eta 0:01:23\n",
      "epoch [9/10] batch [120/204] time 0.265 (0.272) data 0.000 (0.003) loss 0.0003 (0.0008) lr 5.1527e-04 eta 0:01:18\n",
      "epoch [9/10] batch [140/204] time 0.264 (0.271) data 0.000 (0.002) loss 0.0009 (0.0008) lr 5.1527e-04 eta 0:01:12\n",
      "epoch [9/10] batch [160/204] time 0.273 (0.272) data 0.000 (0.002) loss 0.0012 (0.0007) lr 5.1527e-04 eta 0:01:07\n",
      "epoch [9/10] batch [180/204] time 0.281 (0.272) data 0.000 (0.002) loss 0.0008 (0.0007) lr 5.1527e-04 eta 0:01:02\n",
      "epoch [9/10] batch [200/204] time 0.268 (0.272) data 0.000 (0.002) loss 0.0009 (0.0007) lr 5.1527e-04 eta 0:00:56\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [10/10] batch [20/204] time 0.273 (0.281) data 0.000 (0.012) loss 0.0002 (0.0005) lr 2.3873e-04 eta 0:00:51\n",
      "epoch [10/10] batch [40/204] time 0.265 (0.280) data 0.000 (0.006) loss 0.0000 (0.0005) lr 2.3873e-04 eta 0:00:45\n",
      "epoch [10/10] batch [60/204] time 0.268 (0.277) data 0.000 (0.004) loss 0.0003 (0.0006) lr 2.3873e-04 eta 0:00:39\n",
      "epoch [10/10] batch [80/204] time 0.266 (0.275) data 0.000 (0.003) loss 0.0001 (0.0007) lr 2.3873e-04 eta 0:00:34\n",
      "epoch [10/10] batch [100/204] time 0.271 (0.274) data 0.000 (0.003) loss 0.0003 (0.0007) lr 2.3873e-04 eta 0:00:28\n",
      "epoch [10/10] batch [120/204] time 0.272 (0.274) data 0.000 (0.003) loss 0.0014 (0.0007) lr 2.3873e-04 eta 0:00:23\n",
      "epoch [10/10] batch [140/204] time 0.270 (0.274) data 0.004 (0.002) loss 0.0001 (0.0007) lr 2.3873e-04 eta 0:00:17\n",
      "epoch [10/10] batch [160/204] time 0.263 (0.274) data 0.000 (0.002) loss 0.0001 (0.0007) lr 2.3873e-04 eta 0:00:12\n",
      "epoch [10/10] batch [180/204] time 0.267 (0.273) data 0.000 (0.002) loss 0.0008 (0.0007) lr 2.3873e-04 eta 0:00:06\n",
      "epoch [10/10] batch [200/204] time 0.263 (0.272) data 0.000 (0.002) loss 0.0007 (0.0007) lr 2.3873e-04 eta 0:00:01\n",
      "Checkpoint saved to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1/VLPromptLearner/model.pth.tar-10\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 11/11 [00:02<00:00,  5.37it/s]\n",
      "=> result\n",
      "* total: 1,053\n",
      "* correct: 1,025\n",
      "* accuracy: 97.3%\n",
      "* error: 2.7%\n",
      "* macro_f1: 97.2%\n",
      "Elapsed: 0:09:18\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_train.sh oxford_flowers 1 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d226628-c498-451a-ada5-ea6fb4143bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9adb1302-b271-4e18-bb2a-58af852c43f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 10\n",
      "model_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']\n",
      "output_dir: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: new\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_1.pkl\n",
      "SUBSAMPLE NEW CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,410\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'image_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'prompt_learner.ctx', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.VPT', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "Loading weights to VLPromptLearner from \"output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1/VLPromptLearner/model.pth.tar-10\" (epoch = 10)\n",
      "Evaluate on the *test* set\n",
      "100%|███████████████████████████████████████████| 15/15 [00:02<00:00,  5.71it/s]\n",
      "=> result\n",
      "* total: 1,410\n",
      "* correct: 1,087\n",
      "* accuracy: 77.1%\n",
      "* error: 22.9%\n",
      "* macro_f1: 72.0%\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_test.sh oxford_flowers 1 2 new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8babb35-c43c-4d46-961f-eedaa2db3f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def5744-ab07-4627-a9bf-8326abd65e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6493ff1-9c23-472c-b07a-b64bf0935381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f54aa24-cf7f-4f40-a16d-d38ce3c42e23",
   "metadata": {},
   "source": [
    "### seed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe556276-6c41-4406-ac0f-c8e4a8b75af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this job and save the output to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']\n",
      "output_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: base\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_2.pkl\n",
      "SUBSAMPLE BASE CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,053\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.ctx', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.VPT', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2/tensorboard)\n",
      "/home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "epoch [1/10] batch [20/204] time 0.253 (0.283) data 0.000 (0.017) loss 0.0008 (0.0011) lr 1.0000e-05 eta 0:09:31\n",
      "epoch [1/10] batch [40/204] time 0.260 (0.272) data 0.000 (0.009) loss 0.0013 (0.0012) lr 1.0000e-05 eta 0:09:03\n",
      "epoch [1/10] batch [60/204] time 0.255 (0.270) data 0.001 (0.006) loss 0.0003 (0.0013) lr 1.0000e-05 eta 0:08:54\n",
      "epoch [1/10] batch [80/204] time 0.261 (0.268) data 0.000 (0.005) loss 0.0022 (0.0012) lr 1.0000e-05 eta 0:08:45\n",
      "epoch [1/10] batch [100/204] time 0.257 (0.267) data 0.000 (0.004) loss 0.0010 (0.0012) lr 1.0000e-05 eta 0:08:38\n",
      "epoch [1/10] batch [120/204] time 0.263 (0.266) data 0.000 (0.004) loss 0.0009 (0.0012) lr 1.0000e-05 eta 0:08:31\n",
      "epoch [1/10] batch [140/204] time 0.267 (0.266) data 0.000 (0.003) loss 0.0006 (0.0012) lr 1.0000e-05 eta 0:08:24\n",
      "epoch [1/10] batch [160/204] time 0.268 (0.265) data 0.000 (0.003) loss 0.0017 (0.0012) lr 1.0000e-05 eta 0:08:17\n",
      "epoch [1/10] batch [180/204] time 0.254 (0.264) data 0.000 (0.003) loss 0.0003 (0.0012) lr 1.0000e-05 eta 0:08:11\n",
      "epoch [1/10] batch [200/204] time 0.259 (0.264) data 0.000 (0.002) loss 0.0013 (0.0012) lr 1.0000e-05 eta 0:08:06\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [2/10] batch [20/204] time 0.262 (0.274) data 0.000 (0.014) loss 0.0010 (0.0011) lr 2.5000e-03 eta 0:08:17\n",
      "epoch [2/10] batch [40/204] time 0.261 (0.269) data 0.000 (0.007) loss 0.0001 (0.0010) lr 2.5000e-03 eta 0:08:02\n",
      "epoch [2/10] batch [60/204] time 0.261 (0.267) data 0.000 (0.005) loss 0.0003 (0.0010) lr 2.5000e-03 eta 0:07:54\n",
      "epoch [2/10] batch [80/204] time 0.256 (0.267) data 0.000 (0.004) loss 0.0004 (0.0010) lr 2.5000e-03 eta 0:07:49\n",
      "epoch [2/10] batch [100/204] time 0.258 (0.267) data 0.000 (0.003) loss 0.0015 (0.0010) lr 2.5000e-03 eta 0:07:42\n",
      "epoch [2/10] batch [120/204] time 0.270 (0.266) data 0.000 (0.003) loss 0.0002 (0.0009) lr 2.5000e-03 eta 0:07:36\n",
      "epoch [2/10] batch [140/204] time 0.255 (0.265) data 0.000 (0.003) loss 0.0013 (0.0009) lr 2.5000e-03 eta 0:07:29\n",
      "epoch [2/10] batch [160/204] time 0.262 (0.265) data 0.000 (0.002) loss 0.0002 (0.0009) lr 2.5000e-03 eta 0:07:23\n",
      "epoch [2/10] batch [180/204] time 0.271 (0.264) data 0.002 (0.002) loss 0.0000 (0.0009) lr 2.5000e-03 eta 0:07:17\n",
      "epoch [2/10] batch [200/204] time 0.260 (0.264) data 0.000 (0.002) loss 0.0005 (0.0009) lr 2.5000e-03 eta 0:07:12\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [3/10] batch [20/204] time 0.260 (0.274) data 0.000 (0.013) loss 0.0001 (0.0006) lr 2.4388e-03 eta 0:07:21\n",
      "epoch [3/10] batch [40/204] time 0.268 (0.269) data 0.004 (0.007) loss 0.0010 (0.0007) lr 2.4388e-03 eta 0:07:08\n",
      "epoch [3/10] batch [60/204] time 0.270 (0.266) data 0.004 (0.005) loss 0.0008 (0.0008) lr 2.4388e-03 eta 0:06:58\n",
      "epoch [3/10] batch [80/204] time 0.261 (0.265) data 0.000 (0.004) loss 0.0002 (0.0008) lr 2.4388e-03 eta 0:06:51\n",
      "epoch [3/10] batch [100/204] time 0.261 (0.265) data 0.000 (0.003) loss 0.0003 (0.0008) lr 2.4388e-03 eta 0:06:45\n",
      "epoch [3/10] batch [120/204] time 0.255 (0.264) data 0.000 (0.003) loss 0.0004 (0.0008) lr 2.4388e-03 eta 0:06:39\n",
      "epoch [3/10] batch [140/204] time 0.260 (0.264) data 0.000 (0.002) loss 0.0006 (0.0008) lr 2.4388e-03 eta 0:06:33\n",
      "epoch [3/10] batch [160/204] time 0.260 (0.264) data 0.000 (0.002) loss 0.0011 (0.0008) lr 2.4388e-03 eta 0:06:28\n",
      "epoch [3/10] batch [180/204] time 0.254 (0.264) data 0.000 (0.002) loss 0.0001 (0.0008) lr 2.4388e-03 eta 0:06:23\n",
      "epoch [3/10] batch [200/204] time 0.263 (0.264) data 0.002 (0.002) loss 0.0016 (0.0008) lr 2.4388e-03 eta 0:06:17\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [4/10] batch [20/204] time 0.257 (0.277) data 0.000 (0.013) loss 0.0003 (0.0009) lr 2.2613e-03 eta 0:06:29\n",
      "epoch [4/10] batch [40/204] time 0.260 (0.271) data 0.000 (0.007) loss 0.0002 (0.0007) lr 2.2613e-03 eta 0:06:15\n",
      "epoch [4/10] batch [60/204] time 0.332 (0.270) data 0.000 (0.005) loss 0.0007 (0.0007) lr 2.2613e-03 eta 0:06:08\n",
      "epoch [4/10] batch [80/204] time 0.265 (0.268) data 0.000 (0.004) loss 0.0007 (0.0007) lr 2.2613e-03 eta 0:06:01\n",
      "epoch [4/10] batch [100/204] time 0.267 (0.267) data 0.000 (0.004) loss 0.0006 (0.0007) lr 2.2613e-03 eta 0:05:54\n",
      "epoch [4/10] batch [120/204] time 0.270 (0.267) data 0.002 (0.003) loss 0.0012 (0.0008) lr 2.2613e-03 eta 0:05:48\n",
      "epoch [4/10] batch [140/204] time 0.272 (0.267) data 0.004 (0.003) loss 0.0009 (0.0008) lr 2.2613e-03 eta 0:05:43\n",
      "epoch [4/10] batch [160/204] time 0.260 (0.266) data 0.000 (0.002) loss 0.0006 (0.0008) lr 2.2613e-03 eta 0:05:37\n",
      "epoch [4/10] batch [180/204] time 0.259 (0.266) data 0.000 (0.002) loss 0.0012 (0.0008) lr 2.2613e-03 eta 0:05:31\n",
      "epoch [4/10] batch [200/204] time 0.257 (0.266) data 0.000 (0.002) loss 0.0011 (0.0008) lr 2.2613e-03 eta 0:05:27\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [5/10] batch [20/204] time 0.264 (0.274) data 0.000 (0.013) loss 0.0017 (0.0008) lr 1.9847e-03 eta 0:05:29\n",
      "epoch [5/10] batch [40/204] time 0.268 (0.270) data 0.001 (0.007) loss 0.0008 (0.0007) lr 1.9847e-03 eta 0:05:20\n",
      "epoch [5/10] batch [60/204] time 0.270 (0.268) data 0.000 (0.005) loss 0.0005 (0.0007) lr 1.9847e-03 eta 0:05:11\n",
      "epoch [5/10] batch [80/204] time 0.311 (0.270) data 0.004 (0.004) loss 0.0005 (0.0007) lr 1.9847e-03 eta 0:05:09\n",
      "epoch [5/10] batch [100/204] time 0.268 (0.272) data 0.000 (0.003) loss 0.0015 (0.0007) lr 1.9847e-03 eta 0:05:05\n",
      "epoch [5/10] batch [120/204] time 0.264 (0.272) data 0.000 (0.003) loss 0.0004 (0.0007) lr 1.9847e-03 eta 0:05:00\n",
      "epoch [5/10] batch [140/204] time 0.259 (0.271) data 0.000 (0.003) loss 0.0008 (0.0007) lr 1.9847e-03 eta 0:04:53\n",
      "epoch [5/10] batch [160/204] time 0.264 (0.269) data 0.000 (0.002) loss 0.0013 (0.0007) lr 1.9847e-03 eta 0:04:46\n",
      "epoch [5/10] batch [180/204] time 0.260 (0.269) data 0.000 (0.002) loss 0.0000 (0.0007) lr 1.9847e-03 eta 0:04:41\n",
      "epoch [5/10] batch [200/204] time 0.256 (0.269) data 0.000 (0.002) loss 0.0011 (0.0007) lr 1.9847e-03 eta 0:04:35\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [6/10] batch [20/204] time 0.343 (0.288) data 0.000 (0.011) loss 0.0001 (0.0007) lr 1.6363e-03 eta 0:04:48\n",
      "epoch [6/10] batch [40/204] time 0.276 (0.277) data 0.000 (0.006) loss 0.0008 (0.0007) lr 1.6363e-03 eta 0:04:31\n",
      "epoch [6/10] batch [60/204] time 0.268 (0.272) data 0.000 (0.004) loss 0.0004 (0.0007) lr 1.6363e-03 eta 0:04:21\n",
      "epoch [6/10] batch [80/204] time 0.277 (0.272) data 0.000 (0.003) loss 0.0001 (0.0007) lr 1.6363e-03 eta 0:04:15\n",
      "epoch [6/10] batch [100/204] time 0.261 (0.272) data 0.000 (0.003) loss 0.0008 (0.0007) lr 1.6363e-03 eta 0:04:09\n",
      "epoch [6/10] batch [120/204] time 0.259 (0.270) data 0.000 (0.002) loss 0.0001 (0.0007) lr 1.6363e-03 eta 0:04:03\n",
      "epoch [6/10] batch [140/204] time 0.265 (0.270) data 0.003 (0.002) loss 0.0002 (0.0007) lr 1.6363e-03 eta 0:03:57\n",
      "epoch [6/10] batch [160/204] time 0.264 (0.269) data 0.004 (0.002) loss 0.0010 (0.0007) lr 1.6363e-03 eta 0:03:50\n",
      "epoch [6/10] batch [180/204] time 0.265 (0.268) data 0.001 (0.002) loss 0.0005 (0.0007) lr 1.6363e-03 eta 0:03:45\n",
      "epoch [6/10] batch [200/204] time 0.254 (0.267) data 0.000 (0.002) loss 0.0006 (0.0007) lr 1.6363e-03 eta 0:03:39\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [7/10] batch [20/204] time 0.267 (0.279) data 0.000 (0.011) loss 0.0007 (0.0008) lr 1.2500e-03 eta 0:03:42\n",
      "epoch [7/10] batch [40/204] time 0.264 (0.270) data 0.003 (0.006) loss 0.0013 (0.0007) lr 1.2500e-03 eta 0:03:29\n",
      "epoch [7/10] batch [60/204] time 0.267 (0.268) data 0.000 (0.004) loss 0.0015 (0.0007) lr 1.2500e-03 eta 0:03:22\n",
      "epoch [7/10] batch [80/204] time 0.258 (0.267) data 0.000 (0.004) loss 0.0013 (0.0007) lr 1.2500e-03 eta 0:03:16\n",
      "epoch [7/10] batch [100/204] time 0.258 (0.267) data 0.000 (0.003) loss 0.0006 (0.0007) lr 1.2500e-03 eta 0:03:10\n",
      "epoch [7/10] batch [120/204] time 0.262 (0.266) data 0.000 (0.003) loss 0.0005 (0.0007) lr 1.2500e-03 eta 0:03:05\n",
      "epoch [7/10] batch [140/204] time 0.273 (0.266) data 0.000 (0.003) loss 0.0007 (0.0007) lr 1.2500e-03 eta 0:02:59\n",
      "epoch [7/10] batch [160/204] time 0.255 (0.265) data 0.000 (0.002) loss 0.0004 (0.0007) lr 1.2500e-03 eta 0:02:54\n",
      "epoch [7/10] batch [180/204] time 0.270 (0.266) data 0.005 (0.002) loss 0.0003 (0.0007) lr 1.2500e-03 eta 0:02:49\n",
      "epoch [7/10] batch [200/204] time 0.262 (0.265) data 0.003 (0.002) loss 0.0010 (0.0007) lr 1.2500e-03 eta 0:02:43\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [8/10] batch [20/204] time 0.263 (0.276) data 0.000 (0.011) loss 0.0001 (0.0007) lr 8.6373e-04 eta 0:02:43\n",
      "epoch [8/10] batch [40/204] time 0.260 (0.270) data 0.000 (0.005) loss 0.0008 (0.0008) lr 8.6373e-04 eta 0:02:34\n",
      "epoch [8/10] batch [60/204] time 0.259 (0.270) data 0.000 (0.004) loss 0.0014 (0.0007) lr 8.6373e-04 eta 0:02:28\n",
      "epoch [8/10] batch [80/204] time 0.264 (0.268) data 0.000 (0.003) loss 0.0014 (0.0007) lr 8.6373e-04 eta 0:02:22\n",
      "epoch [8/10] batch [100/204] time 0.273 (0.270) data 0.003 (0.003) loss 0.0007 (0.0007) lr 8.6373e-04 eta 0:02:18\n",
      "epoch [8/10] batch [120/204] time 0.265 (0.268) data 0.000 (0.002) loss 0.0001 (0.0007) lr 8.6373e-04 eta 0:02:12\n",
      "epoch [8/10] batch [140/204] time 0.267 (0.268) data 0.000 (0.002) loss 0.0002 (0.0007) lr 8.6373e-04 eta 0:02:06\n",
      "epoch [8/10] batch [160/204] time 0.276 (0.268) data 0.000 (0.002) loss 0.0016 (0.0007) lr 8.6373e-04 eta 0:02:01\n",
      "epoch [8/10] batch [180/204] time 0.277 (0.269) data 0.000 (0.002) loss 0.0009 (0.0007) lr 8.6373e-04 eta 0:01:56\n",
      "epoch [8/10] batch [200/204] time 0.266 (0.270) data 0.000 (0.002) loss 0.0001 (0.0007) lr 8.6373e-04 eta 0:01:51\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [9/10] batch [20/204] time 0.262 (0.274) data 0.000 (0.011) loss 0.0003 (0.0006) lr 5.1527e-04 eta 0:01:46\n",
      "epoch [9/10] batch [40/204] time 0.269 (0.271) data 0.003 (0.006) loss 0.0012 (0.0007) lr 5.1527e-04 eta 0:01:39\n",
      "epoch [9/10] batch [60/204] time 0.264 (0.268) data 0.000 (0.004) loss 0.0005 (0.0007) lr 5.1527e-04 eta 0:01:33\n",
      "epoch [9/10] batch [80/204] time 0.257 (0.266) data 0.000 (0.004) loss 0.0001 (0.0007) lr 5.1527e-04 eta 0:01:27\n",
      "epoch [9/10] batch [100/204] time 0.259 (0.265) data 0.000 (0.003) loss 0.0009 (0.0007) lr 5.1527e-04 eta 0:01:21\n",
      "epoch [9/10] batch [120/204] time 0.258 (0.264) data 0.000 (0.003) loss 0.0006 (0.0007) lr 5.1527e-04 eta 0:01:16\n",
      "epoch [9/10] batch [140/204] time 0.253 (0.265) data 0.000 (0.003) loss 0.0004 (0.0007) lr 5.1527e-04 eta 0:01:10\n",
      "epoch [9/10] batch [160/204] time 0.254 (0.264) data 0.000 (0.002) loss 0.0001 (0.0007) lr 5.1527e-04 eta 0:01:05\n",
      "epoch [9/10] batch [180/204] time 0.269 (0.264) data 0.004 (0.002) loss 0.0009 (0.0007) lr 5.1527e-04 eta 0:01:00\n",
      "epoch [9/10] batch [200/204] time 0.266 (0.264) data 0.001 (0.002) loss 0.0005 (0.0007) lr 5.1527e-04 eta 0:00:54\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [10/10] batch [20/204] time 0.257 (0.276) data 0.000 (0.011) loss 0.0004 (0.0006) lr 2.3873e-04 eta 0:00:50\n",
      "epoch [10/10] batch [40/204] time 0.268 (0.271) data 0.000 (0.006) loss 0.0001 (0.0005) lr 2.3873e-04 eta 0:00:44\n",
      "epoch [10/10] batch [60/204] time 0.265 (0.268) data 0.003 (0.004) loss 0.0003 (0.0006) lr 2.3873e-04 eta 0:00:38\n",
      "epoch [10/10] batch [80/204] time 0.264 (0.266) data 0.000 (0.003) loss 0.0010 (0.0006) lr 2.3873e-04 eta 0:00:32\n",
      "epoch [10/10] batch [100/204] time 0.265 (0.266) data 0.000 (0.003) loss 0.0010 (0.0006) lr 2.3873e-04 eta 0:00:27\n",
      "epoch [10/10] batch [120/204] time 0.265 (0.266) data 0.000 (0.002) loss 0.0002 (0.0007) lr 2.3873e-04 eta 0:00:22\n",
      "epoch [10/10] batch [140/204] time 0.255 (0.265) data 0.000 (0.002) loss 0.0006 (0.0007) lr 2.3873e-04 eta 0:00:16\n",
      "epoch [10/10] batch [160/204] time 0.262 (0.264) data 0.000 (0.002) loss 0.0013 (0.0007) lr 2.3873e-04 eta 0:00:11\n",
      "epoch [10/10] batch [180/204] time 0.263 (0.264) data 0.000 (0.002) loss 0.0010 (0.0007) lr 2.3873e-04 eta 0:00:06\n",
      "epoch [10/10] batch [200/204] time 0.256 (0.264) data 0.000 (0.002) loss 0.0001 (0.0007) lr 2.3873e-04 eta 0:00:01\n",
      "Checkpoint saved to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2/VLPromptLearner/model.pth.tar-10\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 11/11 [00:02<00:00,  5.47it/s]\n",
      "=> result\n",
      "* total: 1,053\n",
      "* correct: 1,031\n",
      "* accuracy: 97.9%\n",
      "* error: 2.1%\n",
      "* macro_f1: 97.7%\n",
      "Elapsed: 0:09:06\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_train.sh oxford_flowers 2 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c29eca-dd2b-4a9a-9e1c-b5d5850ea517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "517311b3-20f8-4d5a-b2a6-52d812dbebe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 10\n",
      "model_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']\n",
      "output_dir: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: new\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_2.pkl\n",
      "SUBSAMPLE NEW CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,410\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'prompt_learner.ctx', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.VPT', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "Loading weights to VLPromptLearner from \"output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2/VLPromptLearner/model.pth.tar-10\" (epoch = 10)\n",
      "Evaluate on the *test* set\n",
      "100%|███████████████████████████████████████████| 15/15 [00:02<00:00,  5.89it/s]\n",
      "=> result\n",
      "* total: 1,410\n",
      "* correct: 1,076\n",
      "* accuracy: 76.3%\n",
      "* error: 23.7%\n",
      "* macro_f1: 71.2%\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_test.sh oxford_flowers 2 2 new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a3ee5-5dbe-4028-ace9-2804a74ed11e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d0960-e826-48ee-bca9-f3216d084dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280788ea-7f82-4fb2-9aee-93e24d5feef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb1ff602-c07e-42b0-8b20-50437079d20c",
   "metadata": {},
   "source": [
    "### seed3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "502f953c-7765-4396-b6b7-9523cd6b4116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this job and save the output to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "Setting fixed seed: 3\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']\n",
      "output_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 3\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: base\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "RESUME: \n",
      "SEED: 3\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_3.pkl\n",
      "SUBSAMPLE BASE CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,053\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'image_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'prompt_learner.ctx', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.VPT', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3/tensorboard)\n",
      "/home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "epoch [1/10] batch [20/204] time 0.272 (0.290) data 0.000 (0.016) loss 0.0015 (0.0014) lr 1.0000e-05 eta 0:09:45\n",
      "epoch [1/10] batch [40/204] time 0.293 (0.287) data 0.000 (0.009) loss 0.0045 (0.0013) lr 1.0000e-05 eta 0:09:33\n",
      "epoch [1/10] batch [60/204] time 0.286 (0.285) data 0.000 (0.006) loss 0.0025 (0.0014) lr 1.0000e-05 eta 0:09:23\n",
      "epoch [1/10] batch [80/204] time 0.285 (0.284) data 0.000 (0.005) loss 0.0015 (0.0014) lr 1.0000e-05 eta 0:09:15\n",
      "epoch [1/10] batch [100/204] time 0.292 (0.283) data 0.000 (0.004) loss 0.0005 (0.0013) lr 1.0000e-05 eta 0:09:09\n",
      "epoch [1/10] batch [120/204] time 0.274 (0.281) data 0.001 (0.003) loss 0.0015 (0.0014) lr 1.0000e-05 eta 0:08:59\n",
      "epoch [1/10] batch [140/204] time 0.273 (0.280) data 0.000 (0.003) loss 0.0018 (0.0013) lr 1.0000e-05 eta 0:08:51\n",
      "epoch [1/10] batch [160/204] time 0.277 (0.279) data 0.000 (0.003) loss 0.0011 (0.0013) lr 1.0000e-05 eta 0:08:44\n",
      "epoch [1/10] batch [180/204] time 0.263 (0.278) data 0.000 (0.003) loss 0.0022 (0.0013) lr 1.0000e-05 eta 0:08:36\n",
      "epoch [1/10] batch [200/204] time 0.263 (0.277) data 0.000 (0.002) loss 0.0016 (0.0012) lr 1.0000e-05 eta 0:08:30\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [2/10] batch [20/204] time 0.289 (0.288) data 0.000 (0.014) loss 0.0009 (0.0007) lr 2.5000e-03 eta 0:08:42\n",
      "epoch [2/10] batch [40/204] time 0.294 (0.281) data 0.015 (0.008) loss 0.0010 (0.0009) lr 2.5000e-03 eta 0:08:23\n",
      "epoch [2/10] batch [60/204] time 0.278 (0.279) data 0.003 (0.006) loss 0.0011 (0.0010) lr 2.5000e-03 eta 0:08:15\n",
      "epoch [2/10] batch [80/204] time 0.273 (0.277) data 0.001 (0.004) loss 0.0005 (0.0010) lr 2.5000e-03 eta 0:08:06\n",
      "epoch [2/10] batch [100/204] time 0.269 (0.276) data 0.000 (0.004) loss 0.0008 (0.0010) lr 2.5000e-03 eta 0:07:58\n",
      "epoch [2/10] batch [120/204] time 0.272 (0.275) data 0.000 (0.003) loss 0.0006 (0.0010) lr 2.5000e-03 eta 0:07:52\n",
      "epoch [2/10] batch [140/204] time 0.267 (0.274) data 0.000 (0.003) loss 0.0024 (0.0010) lr 2.5000e-03 eta 0:07:45\n",
      "epoch [2/10] batch [160/204] time 0.272 (0.274) data 0.000 (0.003) loss 0.0004 (0.0009) lr 2.5000e-03 eta 0:07:39\n",
      "epoch [2/10] batch [180/204] time 0.283 (0.274) data 0.000 (0.002) loss 0.0006 (0.0009) lr 2.5000e-03 eta 0:07:33\n",
      "epoch [2/10] batch [200/204] time 0.269 (0.274) data 0.000 (0.002) loss 0.0005 (0.0009) lr 2.5000e-03 eta 0:07:28\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [3/10] batch [20/204] time 0.293 (0.288) data 0.004 (0.014) loss 0.0036 (0.0012) lr 2.4388e-03 eta 0:07:44\n",
      "epoch [3/10] batch [40/204] time 0.279 (0.282) data 0.002 (0.008) loss 0.0016 (0.0010) lr 2.4388e-03 eta 0:07:29\n",
      "epoch [3/10] batch [60/204] time 0.283 (0.280) data 0.005 (0.005) loss 0.0003 (0.0009) lr 2.4388e-03 eta 0:07:19\n",
      "epoch [3/10] batch [80/204] time 0.294 (0.278) data 0.001 (0.004) loss 0.0011 (0.0009) lr 2.4388e-03 eta 0:07:11\n",
      "epoch [3/10] batch [100/204] time 0.277 (0.277) data 0.000 (0.004) loss 0.0016 (0.0009) lr 2.4388e-03 eta 0:07:04\n",
      "epoch [3/10] batch [120/204] time 0.276 (0.276) data 0.000 (0.003) loss 0.0006 (0.0009) lr 2.4388e-03 eta 0:06:57\n",
      "epoch [3/10] batch [140/204] time 0.278 (0.278) data 0.001 (0.003) loss 0.0004 (0.0009) lr 2.4388e-03 eta 0:06:54\n",
      "epoch [3/10] batch [160/204] time 0.263 (0.277) data 0.000 (0.003) loss 0.0008 (0.0008) lr 2.4388e-03 eta 0:06:47\n",
      "epoch [3/10] batch [180/204] time 0.271 (0.277) data 0.000 (0.003) loss 0.0006 (0.0008) lr 2.4388e-03 eta 0:06:41\n",
      "epoch [3/10] batch [200/204] time 0.265 (0.276) data 0.000 (0.002) loss 0.0028 (0.0008) lr 2.4388e-03 eta 0:06:35\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [4/10] batch [20/204] time 0.275 (0.288) data 0.000 (0.013) loss 0.0003 (0.0009) lr 2.2613e-03 eta 0:06:45\n",
      "epoch [4/10] batch [40/204] time 0.281 (0.282) data 0.000 (0.007) loss 0.0012 (0.0009) lr 2.2613e-03 eta 0:06:31\n",
      "epoch [4/10] batch [60/204] time 0.281 (0.279) data 0.000 (0.005) loss 0.0005 (0.0009) lr 2.2613e-03 eta 0:06:21\n",
      "epoch [4/10] batch [80/204] time 0.273 (0.278) data 0.000 (0.004) loss 0.0003 (0.0009) lr 2.2613e-03 eta 0:06:14\n",
      "epoch [4/10] batch [100/204] time 0.279 (0.277) data 0.001 (0.003) loss 0.0006 (0.0008) lr 2.2613e-03 eta 0:06:07\n",
      "epoch [4/10] batch [120/204] time 0.275 (0.276) data 0.001 (0.003) loss 0.0007 (0.0008) lr 2.2613e-03 eta 0:06:00\n",
      "epoch [4/10] batch [140/204] time 0.285 (0.275) data 0.000 (0.002) loss 0.0013 (0.0008) lr 2.2613e-03 eta 0:05:54\n",
      "epoch [4/10] batch [160/204] time 0.268 (0.276) data 0.000 (0.002) loss 0.0014 (0.0008) lr 2.2613e-03 eta 0:05:49\n",
      "epoch [4/10] batch [180/204] time 0.267 (0.275) data 0.000 (0.002) loss 0.0002 (0.0008) lr 2.2613e-03 eta 0:05:43\n",
      "epoch [4/10] batch [200/204] time 0.269 (0.274) data 0.003 (0.002) loss 0.0005 (0.0008) lr 2.2613e-03 eta 0:05:37\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [5/10] batch [20/204] time 0.267 (0.288) data 0.000 (0.013) loss 0.0009 (0.0005) lr 1.9847e-03 eta 0:05:46\n",
      "epoch [5/10] batch [40/204] time 0.274 (0.282) data 0.000 (0.007) loss 0.0012 (0.0006) lr 1.9847e-03 eta 0:05:34\n",
      "epoch [5/10] batch [60/204] time 0.275 (0.279) data 0.000 (0.005) loss 0.0012 (0.0006) lr 1.9847e-03 eta 0:05:24\n",
      "epoch [5/10] batch [80/204] time 0.273 (0.277) data 0.000 (0.004) loss 0.0008 (0.0006) lr 1.9847e-03 eta 0:05:17\n",
      "epoch [5/10] batch [100/204] time 0.275 (0.277) data 0.000 (0.003) loss 0.0006 (0.0006) lr 1.9847e-03 eta 0:05:11\n",
      "epoch [5/10] batch [120/204] time 0.276 (0.277) data 0.000 (0.003) loss 0.0013 (0.0007) lr 1.9847e-03 eta 0:05:05\n",
      "epoch [5/10] batch [140/204] time 0.271 (0.277) data 0.003 (0.003) loss 0.0012 (0.0007) lr 1.9847e-03 eta 0:04:59\n",
      "epoch [5/10] batch [160/204] time 0.277 (0.276) data 0.000 (0.002) loss 0.0009 (0.0007) lr 1.9847e-03 eta 0:04:53\n",
      "epoch [5/10] batch [180/204] time 0.274 (0.275) data 0.001 (0.002) loss 0.0019 (0.0007) lr 1.9847e-03 eta 0:04:47\n",
      "epoch [5/10] batch [200/204] time 0.289 (0.275) data 0.004 (0.002) loss 0.0004 (0.0007) lr 1.9847e-03 eta 0:04:41\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [6/10] batch [20/204] time 0.282 (0.290) data 0.000 (0.012) loss 0.0001 (0.0007) lr 1.6363e-03 eta 0:04:49\n",
      "epoch [6/10] batch [40/204] time 0.280 (0.285) data 0.000 (0.006) loss 0.0002 (0.0007) lr 1.6363e-03 eta 0:04:39\n",
      "epoch [6/10] batch [60/204] time 0.268 (0.280) data 0.000 (0.004) loss 0.0010 (0.0007) lr 1.6363e-03 eta 0:04:29\n",
      "epoch [6/10] batch [80/204] time 0.267 (0.278) data 0.000 (0.003) loss 0.0007 (0.0007) lr 1.6363e-03 eta 0:04:21\n",
      "epoch [6/10] batch [100/204] time 0.272 (0.277) data 0.000 (0.003) loss 0.0012 (0.0007) lr 1.6363e-03 eta 0:04:14\n",
      "epoch [6/10] batch [120/204] time 0.278 (0.277) data 0.000 (0.002) loss 0.0001 (0.0007) lr 1.6363e-03 eta 0:04:09\n",
      "epoch [6/10] batch [140/204] time 0.275 (0.278) data 0.000 (0.002) loss 0.0008 (0.0007) lr 1.6363e-03 eta 0:04:04\n",
      "epoch [6/10] batch [160/204] time 0.277 (0.278) data 0.000 (0.002) loss 0.0013 (0.0008) lr 1.6363e-03 eta 0:03:58\n",
      "epoch [6/10] batch [180/204] time 0.270 (0.277) data 0.000 (0.002) loss 0.0000 (0.0007) lr 1.6363e-03 eta 0:03:52\n",
      "epoch [6/10] batch [200/204] time 0.264 (0.277) data 0.000 (0.002) loss 0.0007 (0.0007) lr 1.6363e-03 eta 0:03:46\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [7/10] batch [20/204] time 0.259 (0.284) data 0.000 (0.010) loss 0.0001 (0.0007) lr 1.2500e-03 eta 0:03:46\n",
      "epoch [7/10] batch [40/204] time 0.268 (0.279) data 0.000 (0.005) loss 0.0003 (0.0007) lr 1.2500e-03 eta 0:03:36\n",
      "epoch [7/10] batch [60/204] time 0.270 (0.277) data 0.000 (0.004) loss 0.0012 (0.0008) lr 1.2500e-03 eta 0:03:29\n",
      "epoch [7/10] batch [80/204] time 0.274 (0.275) data 0.000 (0.003) loss 0.0004 (0.0008) lr 1.2500e-03 eta 0:03:22\n",
      "epoch [7/10] batch [100/204] time 0.272 (0.276) data 0.000 (0.003) loss 0.0000 (0.0008) lr 1.2500e-03 eta 0:03:17\n",
      "epoch [7/10] batch [120/204] time 0.278 (0.276) data 0.000 (0.002) loss 0.0005 (0.0008) lr 1.2500e-03 eta 0:03:12\n",
      "epoch [7/10] batch [140/204] time 0.287 (0.277) data 0.004 (0.002) loss 0.0008 (0.0008) lr 1.2500e-03 eta 0:03:07\n",
      "epoch [7/10] batch [160/204] time 0.277 (0.277) data 0.000 (0.002) loss 0.0010 (0.0008) lr 1.2500e-03 eta 0:03:01\n",
      "epoch [7/10] batch [180/204] time 0.266 (0.277) data 0.000 (0.002) loss 0.0008 (0.0008) lr 1.2500e-03 eta 0:02:56\n",
      "epoch [7/10] batch [200/204] time 0.272 (0.277) data 0.000 (0.002) loss 0.0014 (0.0008) lr 1.2500e-03 eta 0:02:50\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [8/10] batch [20/204] time 0.292 (0.291) data 0.000 (0.012) loss 0.0007 (0.0009) lr 8.6373e-04 eta 0:02:52\n",
      "epoch [8/10] batch [40/204] time 0.275 (0.284) data 0.002 (0.006) loss 0.0008 (0.0007) lr 8.6373e-04 eta 0:02:42\n",
      "epoch [8/10] batch [60/204] time 0.273 (0.281) data 0.000 (0.005) loss 0.0005 (0.0007) lr 8.6373e-04 eta 0:02:34\n",
      "epoch [8/10] batch [80/204] time 0.270 (0.279) data 0.000 (0.004) loss 0.0006 (0.0006) lr 8.6373e-04 eta 0:02:28\n",
      "epoch [8/10] batch [100/204] time 0.272 (0.278) data 0.000 (0.003) loss 0.0004 (0.0006) lr 8.6373e-04 eta 0:02:22\n",
      "epoch [8/10] batch [120/204] time 0.273 (0.278) data 0.000 (0.003) loss 0.0004 (0.0007) lr 8.6373e-04 eta 0:02:16\n",
      "epoch [8/10] batch [140/204] time 0.268 (0.278) data 0.000 (0.002) loss 0.0002 (0.0007) lr 8.6373e-04 eta 0:02:11\n",
      "epoch [8/10] batch [160/204] time 0.279 (0.279) data 0.001 (0.002) loss 0.0007 (0.0007) lr 8.6373e-04 eta 0:02:05\n",
      "epoch [8/10] batch [180/204] time 0.284 (0.279) data 0.005 (0.002) loss 0.0005 (0.0006) lr 8.6373e-04 eta 0:02:00\n",
      "epoch [8/10] batch [200/204] time 0.274 (0.279) data 0.000 (0.002) loss 0.0004 (0.0006) lr 8.6373e-04 eta 0:01:55\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [9/10] batch [20/204] time 0.270 (0.283) data 0.002 (0.012) loss 0.0004 (0.0006) lr 5.1527e-04 eta 0:01:49\n",
      "epoch [9/10] batch [40/204] time 0.264 (0.278) data 0.004 (0.006) loss 0.0008 (0.0007) lr 5.1527e-04 eta 0:01:42\n",
      "epoch [9/10] batch [60/204] time 0.263 (0.271) data 0.005 (0.005) loss 0.0003 (0.0007) lr 5.1527e-04 eta 0:01:34\n",
      "epoch [9/10] batch [80/204] time 0.255 (0.267) data 0.000 (0.004) loss 0.0000 (0.0007) lr 5.1527e-04 eta 0:01:27\n",
      "epoch [9/10] batch [100/204] time 0.255 (0.266) data 0.000 (0.003) loss 0.0010 (0.0007) lr 5.1527e-04 eta 0:01:21\n",
      "epoch [9/10] batch [120/204] time 0.259 (0.264) data 0.000 (0.003) loss 0.0001 (0.0007) lr 5.1527e-04 eta 0:01:16\n",
      "epoch [9/10] batch [140/204] time 0.291 (0.264) data 0.000 (0.002) loss 0.0004 (0.0007) lr 5.1527e-04 eta 0:01:10\n",
      "epoch [9/10] batch [160/204] time 0.253 (0.264) data 0.001 (0.002) loss 0.0001 (0.0007) lr 5.1527e-04 eta 0:01:05\n",
      "epoch [9/10] batch [180/204] time 0.256 (0.263) data 0.000 (0.002) loss 0.0001 (0.0007) lr 5.1527e-04 eta 0:00:59\n",
      "epoch [9/10] batch [200/204] time 0.262 (0.262) data 0.002 (0.002) loss 0.0007 (0.0007) lr 5.1527e-04 eta 0:00:54\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [10/10] batch [20/204] time 0.249 (0.270) data 0.000 (0.011) loss 0.0000 (0.0008) lr 2.3873e-04 eta 0:00:49\n",
      "epoch [10/10] batch [40/204] time 0.260 (0.266) data 0.004 (0.006) loss 0.0009 (0.0008) lr 2.3873e-04 eta 0:00:43\n",
      "epoch [10/10] batch [60/204] time 0.261 (0.263) data 0.002 (0.004) loss 0.0007 (0.0007) lr 2.3873e-04 eta 0:00:37\n",
      "epoch [10/10] batch [80/204] time 0.271 (0.263) data 0.004 (0.003) loss 0.0005 (0.0007) lr 2.3873e-04 eta 0:00:32\n",
      "epoch [10/10] batch [100/204] time 0.258 (0.264) data 0.000 (0.003) loss 0.0013 (0.0007) lr 2.3873e-04 eta 0:00:27\n",
      "epoch [10/10] batch [120/204] time 0.260 (0.264) data 0.004 (0.003) loss 0.0009 (0.0007) lr 2.3873e-04 eta 0:00:22\n",
      "epoch [10/10] batch [140/204] time 0.267 (0.263) data 0.000 (0.002) loss 0.0001 (0.0007) lr 2.3873e-04 eta 0:00:16\n",
      "epoch [10/10] batch [160/204] time 0.256 (0.263) data 0.000 (0.002) loss 0.0001 (0.0007) lr 2.3873e-04 eta 0:00:11\n",
      "epoch [10/10] batch [180/204] time 0.257 (0.262) data 0.000 (0.002) loss 0.0006 (0.0007) lr 2.3873e-04 eta 0:00:06\n",
      "epoch [10/10] batch [200/204] time 0.259 (0.262) data 0.004 (0.002) loss 0.0007 (0.0007) lr 2.3873e-04 eta 0:00:01\n",
      "Checkpoint saved to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3/VLPromptLearner/model.pth.tar-10\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 11/11 [00:02<00:00,  5.43it/s]\n",
      "=> result\n",
      "* total: 1,053\n",
      "* correct: 1,032\n",
      "* accuracy: 98.0%\n",
      "* error: 2.0%\n",
      "* macro_f1: 98.0%\n",
      "Elapsed: 0:09:22\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_train.sh oxford_flowers 3 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d0bd47-860e-4ba9-9992-893d9ea2c8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b263bbe-ab3a-446c-adab-75e6bd7d4f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting fixed seed: 3\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 10\n",
      "model_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']\n",
      "output_dir: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 3\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: new\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "RESUME: \n",
      "SEED: 3\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_3.pkl\n",
      "SUBSAMPLE NEW CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,410\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'text_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.VPT', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'prompt_learner.ctx', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "Loading weights to VLPromptLearner from \"output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3/VLPromptLearner/model.pth.tar-10\" (epoch = 10)\n",
      "Evaluate on the *test* set\n",
      "100%|███████████████████████████████████████████| 15/15 [00:02<00:00,  5.67it/s]\n",
      "=> result\n",
      "* total: 1,410\n",
      "* correct: 1,092\n",
      "* accuracy: 77.4%\n",
      "* error: 22.6%\n",
      "* macro_f1: 72.4%\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_test.sh oxford_flowers 3 2 new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab3f35-1afe-4d64-a4f5-8cd12b0dd390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116fa675-687a-483a-a2d7-cca0172c430c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942dd6af-a76c-49e9-8f39-5e7cef6d58a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14ace382-409c-4709-aedd-e1d8bc835ddd",
   "metadata": {},
   "source": [
    "## prometar1 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "006442e5-4a52-43e6-8ca4-9721c98264d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hbcho991/prometar/ProMetaR\n"
     ]
    }
   ],
   "source": [
    "%cd /home/hbcho991/prometar/ProMetaR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdccb8a-c00a-4f62-a33a-58fa71ea8934",
   "metadata": {},
   "source": [
    "### seed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a88f7ac8-acb7-4258-a6f7-6c5f70770336",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this job and save the output to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']\n",
      "output_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: base\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_1.pkl\n",
      "SUBSAMPLE BASE CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,053\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'image_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.VPT', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'prompt_learner.ctx', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1/tensorboard)\n",
      "/home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "epoch [1/10] batch [20/204] time 0.261 (0.276) data 0.000 (0.016) loss 0.0022 (0.0012) lr 1.0000e-05 eta 0:09:18\n",
      "epoch [1/10] batch [40/204] time 0.312 (0.282) data 0.000 (0.008) loss 0.0018 (0.0010) lr 1.0000e-05 eta 0:09:23\n",
      "epoch [1/10] batch [60/204] time 0.312 (0.292) data 0.000 (0.005) loss 0.0013 (0.0011) lr 1.0000e-05 eta 0:09:38\n",
      "epoch [1/10] batch [80/204] time 0.312 (0.300) data 0.001 (0.004) loss 0.0008 (0.0012) lr 1.0000e-05 eta 0:09:47\n",
      "epoch [1/10] batch [100/204] time 0.316 (0.303) data 0.000 (0.003) loss 0.0001 (0.0012) lr 1.0000e-05 eta 0:09:48\n",
      "epoch [1/10] batch [120/204] time 0.317 (0.305) data 0.000 (0.003) loss 0.0009 (0.0012) lr 1.0000e-05 eta 0:09:46\n",
      "epoch [1/10] batch [140/204] time 0.340 (0.309) data 0.000 (0.002) loss 0.0024 (0.0012) lr 1.0000e-05 eta 0:09:47\n",
      "epoch [1/10] batch [160/204] time 0.333 (0.313) data 0.000 (0.002) loss 0.0008 (0.0012) lr 1.0000e-05 eta 0:09:47\n",
      "epoch [1/10] batch [180/204] time 0.316 (0.315) data 0.000 (0.002) loss 0.0007 (0.0011) lr 1.0000e-05 eta 0:09:45\n",
      "epoch [1/10] batch [200/204] time 0.320 (0.315) data 0.000 (0.002) loss 0.0014 (0.0012) lr 1.0000e-05 eta 0:09:39\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [2/10] batch [20/204] time 0.326 (0.350) data 0.000 (0.014) loss 0.0013 (0.0009) lr 2.5000e-03 eta 0:10:36\n",
      "epoch [2/10] batch [40/204] time 0.338 (0.345) data 0.000 (0.007) loss 0.0001 (0.0010) lr 2.5000e-03 eta 0:10:19\n",
      "epoch [2/10] batch [60/204] time 0.339 (0.342) data 0.000 (0.005) loss 0.0025 (0.0010) lr 2.5000e-03 eta 0:10:06\n",
      "epoch [2/10] batch [80/204] time 0.336 (0.340) data 0.000 (0.004) loss 0.0007 (0.0010) lr 2.5000e-03 eta 0:09:57\n",
      "epoch [2/10] batch [100/204] time 0.338 (0.339) data 0.000 (0.003) loss 0.0023 (0.0010) lr 2.5000e-03 eta 0:09:49\n",
      "epoch [2/10] batch [120/204] time 0.333 (0.339) data 0.000 (0.003) loss 0.0011 (0.0010) lr 2.5000e-03 eta 0:09:41\n",
      "epoch [2/10] batch [140/204] time 0.333 (0.338) data 0.000 (0.002) loss 0.0026 (0.0010) lr 2.5000e-03 eta 0:09:34\n",
      "epoch [2/10] batch [160/204] time 0.334 (0.338) data 0.000 (0.002) loss 0.0013 (0.0010) lr 2.5000e-03 eta 0:09:26\n",
      "epoch [2/10] batch [180/204] time 0.335 (0.337) data 0.000 (0.002) loss 0.0003 (0.0010) lr 2.5000e-03 eta 0:09:18\n",
      "epoch [2/10] batch [200/204] time 0.333 (0.337) data 0.000 (0.002) loss 0.0005 (0.0010) lr 2.5000e-03 eta 0:09:11\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [3/10] batch [20/204] time 0.337 (0.353) data 0.000 (0.015) loss 0.0010 (0.0006) lr 2.4388e-03 eta 0:09:29\n",
      "epoch [3/10] batch [40/204] time 0.334 (0.347) data 0.000 (0.008) loss 0.0007 (0.0007) lr 2.4388e-03 eta 0:09:12\n",
      "epoch [3/10] batch [60/204] time 0.334 (0.343) data 0.000 (0.005) loss 0.0002 (0.0007) lr 2.4388e-03 eta 0:08:58\n",
      "epoch [3/10] batch [80/204] time 0.331 (0.339) data 0.000 (0.004) loss 0.0014 (0.0007) lr 2.4388e-03 eta 0:08:46\n",
      "epoch [3/10] batch [100/204] time 0.328 (0.337) data 0.000 (0.003) loss 0.0008 (0.0008) lr 2.4388e-03 eta 0:08:35\n",
      "epoch [3/10] batch [120/204] time 0.324 (0.332) data 0.000 (0.003) loss 0.0018 (0.0009) lr 2.4388e-03 eta 0:08:22\n",
      "epoch [3/10] batch [140/204] time 0.307 (0.330) data 0.000 (0.002) loss 0.0013 (0.0008) lr 2.4388e-03 eta 0:08:11\n",
      "epoch [3/10] batch [160/204] time 0.324 (0.328) data 0.000 (0.002) loss 0.0020 (0.0009) lr 2.4388e-03 eta 0:08:03\n",
      "epoch [3/10] batch [180/204] time 0.327 (0.328) data 0.000 (0.002) loss 0.0006 (0.0009) lr 2.4388e-03 eta 0:07:56\n",
      "epoch [3/10] batch [200/204] time 0.323 (0.328) data 0.000 (0.002) loss 0.0001 (0.0009) lr 2.4388e-03 eta 0:07:49\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [4/10] batch [20/204] time 0.323 (0.343) data 0.000 (0.015) loss 0.0005 (0.0006) lr 2.2613e-03 eta 0:08:03\n",
      "epoch [4/10] batch [40/204] time 0.325 (0.334) data 0.000 (0.007) loss 0.0007 (0.0006) lr 2.2613e-03 eta 0:07:43\n",
      "epoch [4/10] batch [60/204] time 0.324 (0.330) data 0.000 (0.005) loss 0.0005 (0.0007) lr 2.2613e-03 eta 0:07:32\n",
      "epoch [4/10] batch [80/204] time 0.326 (0.329) data 0.000 (0.004) loss 0.0009 (0.0007) lr 2.2613e-03 eta 0:07:23\n",
      "epoch [4/10] batch [100/204] time 0.332 (0.328) data 0.000 (0.003) loss 0.0001 (0.0007) lr 2.2613e-03 eta 0:07:16\n",
      "epoch [4/10] batch [120/204] time 0.325 (0.328) data 0.000 (0.003) loss 0.0007 (0.0007) lr 2.2613e-03 eta 0:07:08\n",
      "epoch [4/10] batch [140/204] time 0.325 (0.327) data 0.000 (0.002) loss 0.0010 (0.0007) lr 2.2613e-03 eta 0:07:01\n",
      "epoch [4/10] batch [160/204] time 0.323 (0.327) data 0.000 (0.002) loss 0.0010 (0.0007) lr 2.2613e-03 eta 0:06:54\n",
      "epoch [4/10] batch [180/204] time 0.324 (0.327) data 0.000 (0.002) loss 0.0001 (0.0007) lr 2.2613e-03 eta 0:06:47\n",
      "epoch [4/10] batch [200/204] time 0.321 (0.326) data 0.000 (0.002) loss 0.0008 (0.0007) lr 2.2613e-03 eta 0:06:40\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [5/10] batch [20/204] time 0.324 (0.341) data 0.000 (0.015) loss 0.0014 (0.0009) lr 1.9847e-03 eta 0:06:50\n",
      "epoch [5/10] batch [40/204] time 0.323 (0.336) data 0.000 (0.007) loss 0.0002 (0.0008) lr 1.9847e-03 eta 0:06:37\n",
      "epoch [5/10] batch [60/204] time 0.329 (0.333) data 0.000 (0.005) loss 0.0003 (0.0008) lr 1.9847e-03 eta 0:06:28\n",
      "epoch [5/10] batch [80/204] time 0.324 (0.331) data 0.000 (0.004) loss 0.0003 (0.0008) lr 1.9847e-03 eta 0:06:19\n",
      "epoch [5/10] batch [100/204] time 0.323 (0.330) data 0.000 (0.003) loss 0.0011 (0.0008) lr 1.9847e-03 eta 0:06:10\n",
      "epoch [5/10] batch [120/204] time 0.325 (0.329) data 0.000 (0.003) loss 0.0009 (0.0008) lr 1.9847e-03 eta 0:06:03\n",
      "epoch [5/10] batch [140/204] time 0.324 (0.328) data 0.000 (0.002) loss 0.0017 (0.0007) lr 1.9847e-03 eta 0:05:55\n",
      "epoch [5/10] batch [160/204] time 0.326 (0.328) data 0.000 (0.002) loss 0.0026 (0.0008) lr 1.9847e-03 eta 0:05:48\n",
      "epoch [5/10] batch [180/204] time 0.322 (0.327) data 0.000 (0.002) loss 0.0009 (0.0008) lr 1.9847e-03 eta 0:05:41\n",
      "epoch [5/10] batch [200/204] time 0.326 (0.327) data 0.000 (0.002) loss 0.0008 (0.0008) lr 1.9847e-03 eta 0:05:35\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [6/10] batch [20/204] time 0.329 (0.340) data 0.000 (0.012) loss 0.0017 (0.0006) lr 1.6363e-03 eta 0:05:39\n",
      "epoch [6/10] batch [40/204] time 0.331 (0.336) data 0.000 (0.006) loss 0.0004 (0.0007) lr 1.6363e-03 eta 0:05:29\n",
      "epoch [6/10] batch [60/204] time 0.331 (0.334) data 0.000 (0.004) loss 0.0002 (0.0007) lr 1.6363e-03 eta 0:05:21\n",
      "epoch [6/10] batch [80/204] time 0.332 (0.334) data 0.000 (0.003) loss 0.0008 (0.0007) lr 1.6363e-03 eta 0:05:13\n",
      "epoch [6/10] batch [100/204] time 0.330 (0.333) data 0.000 (0.003) loss 0.0001 (0.0007) lr 1.6363e-03 eta 0:05:06\n",
      "epoch [6/10] batch [120/204] time 0.329 (0.333) data 0.000 (0.002) loss 0.0007 (0.0007) lr 1.6363e-03 eta 0:04:59\n",
      "epoch [6/10] batch [140/204] time 0.324 (0.332) data 0.000 (0.002) loss 0.0009 (0.0007) lr 1.6363e-03 eta 0:04:52\n",
      "epoch [6/10] batch [160/204] time 0.328 (0.331) data 0.000 (0.002) loss 0.0009 (0.0007) lr 1.6363e-03 eta 0:04:44\n",
      "epoch [6/10] batch [180/204] time 0.278 (0.329) data 0.000 (0.002) loss 0.0007 (0.0007) lr 1.6363e-03 eta 0:04:36\n",
      "epoch [6/10] batch [200/204] time 0.280 (0.324) data 0.000 (0.001) loss 0.0001 (0.0007) lr 1.6363e-03 eta 0:04:25\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [7/10] batch [20/204] time 0.279 (0.299) data 0.000 (0.013) loss 0.0002 (0.0006) lr 1.2500e-03 eta 0:03:57\n",
      "epoch [7/10] batch [40/204] time 0.286 (0.290) data 0.000 (0.006) loss 0.0001 (0.0006) lr 1.2500e-03 eta 0:03:44\n",
      "epoch [7/10] batch [60/204] time 0.276 (0.286) data 0.000 (0.004) loss 0.0006 (0.0007) lr 1.2500e-03 eta 0:03:36\n",
      "epoch [7/10] batch [80/204] time 0.333 (0.294) data 0.000 (0.003) loss 0.0004 (0.0007) lr 1.2500e-03 eta 0:03:36\n",
      "epoch [7/10] batch [100/204] time 0.329 (0.302) data 0.000 (0.003) loss 0.0001 (0.0007) lr 1.2500e-03 eta 0:03:36\n",
      "epoch [7/10] batch [120/204] time 0.345 (0.309) data 0.000 (0.002) loss 0.0006 (0.0007) lr 1.2500e-03 eta 0:03:34\n",
      "epoch [7/10] batch [140/204] time 0.345 (0.314) data 0.001 (0.002) loss 0.0002 (0.0007) lr 1.2500e-03 eta 0:03:32\n",
      "epoch [7/10] batch [160/204] time 0.360 (0.318) data 0.000 (0.002) loss 0.0000 (0.0007) lr 1.2500e-03 eta 0:03:28\n",
      "epoch [7/10] batch [180/204] time 0.343 (0.321) data 0.000 (0.002) loss 0.0003 (0.0007) lr 1.2500e-03 eta 0:03:24\n",
      "epoch [7/10] batch [200/204] time 0.343 (0.324) data 0.000 (0.002) loss 0.0015 (0.0007) lr 1.2500e-03 eta 0:03:19\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [8/10] batch [20/204] time 0.341 (0.358) data 0.000 (0.014) loss 0.0005 (0.0007) lr 8.6373e-04 eta 0:03:31\n",
      "epoch [8/10] batch [40/204] time 0.343 (0.355) data 0.000 (0.007) loss 0.0007 (0.0006) lr 8.6373e-04 eta 0:03:23\n",
      "epoch [8/10] batch [60/204] time 0.340 (0.348) data 0.000 (0.005) loss 0.0006 (0.0007) lr 8.6373e-04 eta 0:03:12\n",
      "epoch [8/10] batch [80/204] time 0.326 (0.342) data 0.000 (0.004) loss 0.0007 (0.0007) lr 8.6373e-04 eta 0:03:02\n",
      "epoch [8/10] batch [100/204] time 0.325 (0.339) data 0.000 (0.003) loss 0.0003 (0.0007) lr 8.6373e-04 eta 0:02:53\n",
      "epoch [8/10] batch [120/204] time 0.327 (0.336) data 0.000 (0.003) loss 0.0009 (0.0008) lr 8.6373e-04 eta 0:02:45\n",
      "epoch [8/10] batch [140/204] time 0.324 (0.334) data 0.000 (0.002) loss 0.0008 (0.0008) lr 8.6373e-04 eta 0:02:37\n",
      "epoch [8/10] batch [160/204] time 0.328 (0.334) data 0.000 (0.002) loss 0.0006 (0.0007) lr 8.6373e-04 eta 0:02:30\n",
      "epoch [8/10] batch [180/204] time 0.296 (0.333) data 0.000 (0.002) loss 0.0004 (0.0007) lr 8.6373e-04 eta 0:02:23\n",
      "epoch [8/10] batch [200/204] time 0.291 (0.329) data 0.000 (0.002) loss 0.0004 (0.0008) lr 8.6373e-04 eta 0:02:15\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [9/10] batch [20/204] time 0.342 (0.354) data 0.000 (0.014) loss 0.0012 (0.0005) lr 5.1527e-04 eta 0:02:17\n",
      "epoch [9/10] batch [40/204] time 0.325 (0.351) data 0.000 (0.007) loss 0.0011 (0.0006) lr 5.1527e-04 eta 0:02:09\n",
      "epoch [9/10] batch [60/204] time 0.331 (0.343) data 0.001 (0.005) loss 0.0008 (0.0006) lr 5.1527e-04 eta 0:01:59\n",
      "epoch [9/10] batch [80/204] time 0.326 (0.340) data 0.000 (0.004) loss 0.0002 (0.0006) lr 5.1527e-04 eta 0:01:51\n",
      "epoch [9/10] batch [100/204] time 0.310 (0.333) data 0.000 (0.003) loss 0.0006 (0.0007) lr 5.1527e-04 eta 0:01:42\n",
      "epoch [9/10] batch [120/204] time 0.311 (0.329) data 0.000 (0.002) loss 0.0009 (0.0006) lr 5.1527e-04 eta 0:01:34\n",
      "epoch [9/10] batch [140/204] time 0.311 (0.328) data 0.000 (0.002) loss 0.0004 (0.0006) lr 5.1527e-04 eta 0:01:27\n",
      "epoch [9/10] batch [160/204] time 0.313 (0.326) data 0.000 (0.002) loss 0.0008 (0.0006) lr 5.1527e-04 eta 0:01:20\n",
      "epoch [9/10] batch [180/204] time 0.331 (0.325) data 0.000 (0.002) loss 0.0001 (0.0006) lr 5.1527e-04 eta 0:01:14\n",
      "epoch [9/10] batch [200/204] time 0.329 (0.325) data 0.000 (0.002) loss 0.0008 (0.0007) lr 5.1527e-04 eta 0:01:07\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [10/10] batch [20/204] time 0.344 (0.347) data 0.000 (0.013) loss 0.0001 (0.0008) lr 2.3873e-04 eta 0:01:03\n",
      "epoch [10/10] batch [40/204] time 0.346 (0.347) data 0.001 (0.007) loss 0.0005 (0.0007) lr 2.3873e-04 eta 0:00:56\n",
      "epoch [10/10] batch [60/204] time 0.292 (0.340) data 0.000 (0.005) loss 0.0001 (0.0007) lr 2.3873e-04 eta 0:00:49\n",
      "epoch [10/10] batch [80/204] time 0.345 (0.329) data 0.000 (0.003) loss 0.0004 (0.0007) lr 2.3873e-04 eta 0:00:40\n",
      "epoch [10/10] batch [100/204] time 0.293 (0.329) data 0.000 (0.003) loss 0.0006 (0.0008) lr 2.3873e-04 eta 0:00:34\n",
      "epoch [10/10] batch [120/204] time 0.343 (0.326) data 0.000 (0.002) loss 0.0012 (0.0008) lr 2.3873e-04 eta 0:00:27\n",
      "epoch [10/10] batch [140/204] time 0.343 (0.329) data 0.000 (0.002) loss 0.0009 (0.0008) lr 2.3873e-04 eta 0:00:21\n",
      "epoch [10/10] batch [160/204] time 0.345 (0.330) data 0.000 (0.002) loss 0.0006 (0.0007) lr 2.3873e-04 eta 0:00:14\n",
      "epoch [10/10] batch [180/204] time 0.347 (0.332) data 0.000 (0.002) loss 0.0000 (0.0007) lr 2.3873e-04 eta 0:00:07\n",
      "epoch [10/10] batch [200/204] time 0.331 (0.333) data 0.000 (0.002) loss 0.0006 (0.0007) lr 2.3873e-04 eta 0:00:01\n",
      "Checkpoint saved to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1/VLPromptLearner/model.pth.tar-10\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 11/11 [00:02<00:00,  4.70it/s]\n",
      "=> result\n",
      "* total: 1,053\n",
      "* correct: 1,030\n",
      "* accuracy: 97.8%\n",
      "* error: 2.2%\n",
      "* macro_f1: 97.7%\n",
      "Elapsed: 0:11:11\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_train.sh oxford_flowers 1 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c94005-9299-4c9c-84b6-c416be7fb9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f9dbe5-c065-439a-a5c1-b8fe468e2788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 10\n",
      "model_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']\n",
      "output_dir: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: new\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_1.pkl\n",
      "SUBSAMPLE NEW CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,410\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'text_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'prompt_learner.ctx', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.VPT', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "Loading weights to VLPromptLearner from \"output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1/VLPromptLearner/model.pth.tar-10\" (epoch = 10)\n",
      "Evaluate on the *test* set\n",
      "100%|███████████████████████████████████████████| 15/15 [00:02<00:00,  5.59it/s]\n",
      "=> result\n",
      "* total: 1,410\n",
      "* correct: 1,088\n",
      "* accuracy: 77.2%\n",
      "* error: 22.8%\n",
      "* macro_f1: 71.6%\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_test.sh oxford_flowers 1 2 new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466ae33-281a-4e0f-988d-b88575e849b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee06e98e-a214-4b7e-8d19-d3f30eaef2be",
   "metadata": {},
   "source": [
    "### seed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f54cb6f5-dad5-41ab-bf76-d9753d04e95e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this job and save the output to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']\n",
      "output_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: base\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_2.pkl\n",
      "SUBSAMPLE BASE CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,053\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'text_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'prompt_learner.ctx', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.VPT', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2/tensorboard)\n",
      "/home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "epoch [1/10] batch [20/204] time 0.251 (0.270) data 0.000 (0.017) loss 0.0023 (0.0012) lr 1.0000e-05 eta 0:09:04\n",
      "epoch [1/10] batch [40/204] time 0.274 (0.265) data 0.000 (0.008) loss 0.0006 (0.0013) lr 1.0000e-05 eta 0:08:50\n",
      "epoch [1/10] batch [60/204] time 0.253 (0.262) data 0.000 (0.006) loss 0.0019 (0.0012) lr 1.0000e-05 eta 0:08:38\n",
      "epoch [1/10] batch [80/204] time 0.254 (0.261) data 0.000 (0.004) loss 0.0044 (0.0012) lr 1.0000e-05 eta 0:08:32\n",
      "epoch [1/10] batch [100/204] time 0.254 (0.260) data 0.000 (0.003) loss 0.0006 (0.0011) lr 1.0000e-05 eta 0:08:24\n",
      "epoch [1/10] batch [120/204] time 0.259 (0.262) data 0.000 (0.003) loss 0.0013 (0.0011) lr 1.0000e-05 eta 0:08:22\n",
      "epoch [1/10] batch [140/204] time 0.255 (0.262) data 0.000 (0.003) loss 0.0005 (0.0012) lr 1.0000e-05 eta 0:08:17\n",
      "epoch [1/10] batch [160/204] time 0.292 (0.262) data 0.000 (0.002) loss 0.0014 (0.0012) lr 1.0000e-05 eta 0:08:13\n",
      "epoch [1/10] batch [180/204] time 0.289 (0.266) data 0.000 (0.002) loss 0.0010 (0.0012) lr 1.0000e-05 eta 0:08:13\n",
      "epoch [1/10] batch [200/204] time 0.289 (0.268) data 0.000 (0.002) loss 0.0004 (0.0012) lr 1.0000e-05 eta 0:08:12\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [2/10] batch [20/204] time 0.347 (0.352) data 0.000 (0.020) loss 0.0000 (0.0009) lr 2.5000e-03 eta 0:10:39\n",
      "epoch [2/10] batch [40/204] time 0.370 (0.354) data 0.000 (0.010) loss 0.0001 (0.0010) lr 2.5000e-03 eta 0:10:35\n",
      "epoch [2/10] batch [60/204] time 0.345 (0.352) data 0.000 (0.007) loss 0.0016 (0.0010) lr 2.5000e-03 eta 0:10:25\n",
      "epoch [2/10] batch [80/204] time 0.343 (0.351) data 0.000 (0.005) loss 0.0007 (0.0010) lr 2.5000e-03 eta 0:10:16\n",
      "epoch [2/10] batch [100/204] time 0.356 (0.350) data 0.000 (0.004) loss 0.0012 (0.0010) lr 2.5000e-03 eta 0:10:07\n",
      "epoch [2/10] batch [120/204] time 0.347 (0.350) data 0.000 (0.004) loss 0.0004 (0.0010) lr 2.5000e-03 eta 0:09:59\n",
      "epoch [2/10] batch [140/204] time 0.347 (0.349) data 0.000 (0.003) loss 0.0001 (0.0010) lr 2.5000e-03 eta 0:09:52\n",
      "epoch [2/10] batch [160/204] time 0.343 (0.349) data 0.000 (0.003) loss 0.0000 (0.0010) lr 2.5000e-03 eta 0:09:45\n",
      "epoch [2/10] batch [180/204] time 0.345 (0.349) data 0.000 (0.002) loss 0.0005 (0.0010) lr 2.5000e-03 eta 0:09:37\n",
      "epoch [2/10] batch [200/204] time 0.340 (0.348) data 0.000 (0.002) loss 0.0009 (0.0010) lr 2.5000e-03 eta 0:09:28\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [3/10] batch [20/204] time 0.338 (0.362) data 0.000 (0.019) loss 0.0013 (0.0008) lr 2.4388e-03 eta 0:09:43\n",
      "epoch [3/10] batch [40/204] time 0.341 (0.353) data 0.000 (0.010) loss 0.0012 (0.0007) lr 2.4388e-03 eta 0:09:22\n",
      "epoch [3/10] batch [60/204] time 0.342 (0.349) data 0.000 (0.006) loss 0.0005 (0.0008) lr 2.4388e-03 eta 0:09:09\n",
      "epoch [3/10] batch [80/204] time 0.338 (0.347) data 0.000 (0.005) loss 0.0014 (0.0008) lr 2.4388e-03 eta 0:08:58\n",
      "epoch [3/10] batch [100/204] time 0.342 (0.346) data 0.000 (0.004) loss 0.0020 (0.0008) lr 2.4388e-03 eta 0:08:49\n",
      "epoch [3/10] batch [120/204] time 0.340 (0.345) data 0.000 (0.003) loss 0.0006 (0.0008) lr 2.4388e-03 eta 0:08:41\n",
      "epoch [3/10] batch [140/204] time 0.342 (0.345) data 0.000 (0.003) loss 0.0016 (0.0008) lr 2.4388e-03 eta 0:08:34\n",
      "epoch [3/10] batch [160/204] time 0.342 (0.344) data 0.000 (0.003) loss 0.0007 (0.0009) lr 2.4388e-03 eta 0:08:26\n",
      "epoch [3/10] batch [180/204] time 0.340 (0.344) data 0.000 (0.002) loss 0.0007 (0.0008) lr 2.4388e-03 eta 0:08:19\n",
      "epoch [3/10] batch [200/204] time 0.375 (0.343) data 0.000 (0.002) loss 0.0011 (0.0008) lr 2.4388e-03 eta 0:08:11\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [4/10] batch [20/204] time 0.338 (0.363) data 0.000 (0.019) loss 0.0007 (0.0008) lr 2.2613e-03 eta 0:08:31\n",
      "epoch [4/10] batch [40/204] time 0.338 (0.351) data 0.000 (0.010) loss 0.0002 (0.0007) lr 2.2613e-03 eta 0:08:07\n",
      "epoch [4/10] batch [60/204] time 0.344 (0.347) data 0.000 (0.006) loss 0.0008 (0.0007) lr 2.2613e-03 eta 0:07:55\n",
      "epoch [4/10] batch [80/204] time 0.339 (0.345) data 0.000 (0.005) loss 0.0001 (0.0006) lr 2.2613e-03 eta 0:07:45\n",
      "epoch [4/10] batch [100/204] time 0.341 (0.345) data 0.000 (0.004) loss 0.0015 (0.0007) lr 2.2613e-03 eta 0:07:38\n",
      "epoch [4/10] batch [120/204] time 0.338 (0.344) data 0.000 (0.003) loss 0.0007 (0.0007) lr 2.2613e-03 eta 0:07:30\n",
      "epoch [4/10] batch [140/204] time 0.339 (0.343) data 0.000 (0.003) loss 0.0007 (0.0006) lr 2.2613e-03 eta 0:07:22\n",
      "epoch [4/10] batch [160/204] time 0.340 (0.343) data 0.000 (0.003) loss 0.0008 (0.0006) lr 2.2613e-03 eta 0:07:14\n",
      "epoch [4/10] batch [180/204] time 0.339 (0.342) data 0.000 (0.002) loss 0.0009 (0.0007) lr 2.2613e-03 eta 0:07:07\n",
      "epoch [4/10] batch [200/204] time 0.338 (0.342) data 0.000 (0.002) loss 0.0027 (0.0007) lr 2.2613e-03 eta 0:07:00\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [5/10] batch [20/204] time 0.336 (0.359) data 0.000 (0.020) loss 0.0022 (0.0008) lr 1.9847e-03 eta 0:07:12\n",
      "epoch [5/10] batch [40/204] time 0.340 (0.351) data 0.000 (0.010) loss 0.0008 (0.0008) lr 1.9847e-03 eta 0:06:56\n",
      "epoch [5/10] batch [60/204] time 0.336 (0.347) data 0.000 (0.007) loss 0.0013 (0.0008) lr 1.9847e-03 eta 0:06:43\n",
      "epoch [5/10] batch [80/204] time 0.341 (0.345) data 0.000 (0.005) loss 0.0007 (0.0008) lr 1.9847e-03 eta 0:06:34\n",
      "epoch [5/10] batch [100/204] time 0.340 (0.344) data 0.000 (0.004) loss 0.0021 (0.0008) lr 1.9847e-03 eta 0:06:26\n",
      "epoch [5/10] batch [120/204] time 0.338 (0.343) data 0.000 (0.003) loss 0.0002 (0.0008) lr 1.9847e-03 eta 0:06:18\n",
      "epoch [5/10] batch [140/204] time 0.341 (0.343) data 0.000 (0.003) loss 0.0002 (0.0008) lr 1.9847e-03 eta 0:06:11\n",
      "epoch [5/10] batch [160/204] time 0.341 (0.342) data 0.000 (0.003) loss 0.0005 (0.0008) lr 1.9847e-03 eta 0:06:03\n",
      "epoch [5/10] batch [180/204] time 0.338 (0.342) data 0.000 (0.002) loss 0.0005 (0.0008) lr 1.9847e-03 eta 0:05:57\n",
      "epoch [5/10] batch [200/204] time 0.334 (0.341) data 0.000 (0.002) loss 0.0006 (0.0008) lr 1.9847e-03 eta 0:05:49\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [6/10] batch [20/204] time 0.338 (0.356) data 0.000 (0.017) loss 0.0010 (0.0006) lr 1.6363e-03 eta 0:05:55\n",
      "epoch [6/10] batch [40/204] time 0.333 (0.348) data 0.000 (0.009) loss 0.0015 (0.0007) lr 1.6363e-03 eta 0:05:41\n",
      "epoch [6/10] batch [60/204] time 0.333 (0.344) data 0.000 (0.006) loss 0.0002 (0.0007) lr 1.6363e-03 eta 0:05:29\n",
      "epoch [6/10] batch [80/204] time 0.333 (0.345) data 0.000 (0.004) loss 0.0012 (0.0007) lr 1.6363e-03 eta 0:05:23\n",
      "epoch [6/10] batch [100/204] time 0.333 (0.343) data 0.000 (0.004) loss 0.0010 (0.0007) lr 1.6363e-03 eta 0:05:15\n",
      "epoch [6/10] batch [120/204] time 0.332 (0.342) data 0.001 (0.003) loss 0.0002 (0.0007) lr 1.6363e-03 eta 0:05:07\n",
      "epoch [6/10] batch [140/204] time 0.331 (0.341) data 0.000 (0.003) loss 0.0004 (0.0007) lr 1.6363e-03 eta 0:05:00\n",
      "epoch [6/10] batch [160/204] time 0.333 (0.340) data 0.000 (0.002) loss 0.0008 (0.0007) lr 1.6363e-03 eta 0:04:52\n",
      "epoch [6/10] batch [180/204] time 0.332 (0.340) data 0.000 (0.002) loss 0.0015 (0.0007) lr 1.6363e-03 eta 0:04:45\n",
      "epoch [6/10] batch [200/204] time 0.339 (0.339) data 0.000 (0.002) loss 0.0009 (0.0007) lr 1.6363e-03 eta 0:04:38\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [7/10] batch [20/204] time 0.337 (0.357) data 0.000 (0.017) loss 0.0009 (0.0008) lr 1.2500e-03 eta 0:04:43\n",
      "epoch [7/10] batch [40/204] time 0.332 (0.346) data 0.000 (0.009) loss 0.0004 (0.0007) lr 1.2500e-03 eta 0:04:28\n",
      "epoch [7/10] batch [60/204] time 0.376 (0.343) data 0.000 (0.006) loss 0.0007 (0.0006) lr 1.2500e-03 eta 0:04:19\n",
      "epoch [7/10] batch [80/204] time 0.334 (0.341) data 0.000 (0.004) loss 0.0010 (0.0007) lr 1.2500e-03 eta 0:04:11\n",
      "epoch [7/10] batch [100/204] time 0.331 (0.340) data 0.000 (0.004) loss 0.0001 (0.0007) lr 1.2500e-03 eta 0:04:03\n",
      "epoch [7/10] batch [120/204] time 0.330 (0.340) data 0.000 (0.003) loss 0.0006 (0.0007) lr 1.2500e-03 eta 0:03:56\n",
      "epoch [7/10] batch [140/204] time 0.331 (0.339) data 0.000 (0.003) loss 0.0006 (0.0007) lr 1.2500e-03 eta 0:03:49\n",
      "epoch [7/10] batch [160/204] time 0.331 (0.339) data 0.000 (0.002) loss 0.0015 (0.0006) lr 1.2500e-03 eta 0:03:42\n",
      "epoch [7/10] batch [180/204] time 0.330 (0.338) data 0.000 (0.002) loss 0.0014 (0.0006) lr 1.2500e-03 eta 0:03:35\n",
      "epoch [7/10] batch [200/204] time 0.343 (0.338) data 0.000 (0.002) loss 0.0003 (0.0006) lr 1.2500e-03 eta 0:03:28\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [8/10] batch [20/204] time 0.332 (0.352) data 0.000 (0.016) loss 0.0012 (0.0006) lr 8.6373e-04 eta 0:03:28\n",
      "epoch [8/10] batch [40/204] time 0.333 (0.346) data 0.000 (0.008) loss 0.0005 (0.0006) lr 8.6373e-04 eta 0:03:17\n",
      "epoch [8/10] batch [60/204] time 0.340 (0.343) data 0.000 (0.006) loss 0.0007 (0.0006) lr 8.6373e-04 eta 0:03:09\n",
      "epoch [8/10] batch [80/204] time 0.338 (0.341) data 0.000 (0.004) loss 0.0008 (0.0006) lr 8.6373e-04 eta 0:03:01\n",
      "epoch [8/10] batch [100/204] time 0.336 (0.341) data 0.000 (0.003) loss 0.0012 (0.0006) lr 8.6373e-04 eta 0:02:54\n",
      "epoch [8/10] batch [120/204] time 0.340 (0.340) data 0.000 (0.003) loss 0.0005 (0.0006) lr 8.6373e-04 eta 0:02:47\n",
      "epoch [8/10] batch [140/204] time 0.337 (0.339) data 0.000 (0.003) loss 0.0004 (0.0006) lr 8.6373e-04 eta 0:02:40\n",
      "epoch [8/10] batch [160/204] time 0.338 (0.339) data 0.000 (0.002) loss 0.0010 (0.0006) lr 8.6373e-04 eta 0:02:33\n",
      "epoch [8/10] batch [180/204] time 0.332 (0.338) data 0.000 (0.002) loss 0.0013 (0.0007) lr 8.6373e-04 eta 0:02:26\n",
      "epoch [8/10] batch [200/204] time 0.329 (0.338) data 0.000 (0.002) loss 0.0011 (0.0007) lr 8.6373e-04 eta 0:02:19\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [9/10] batch [20/204] time 0.332 (0.352) data 0.000 (0.017) loss 0.0008 (0.0007) lr 5.1527e-04 eta 0:02:16\n",
      "epoch [9/10] batch [40/204] time 0.336 (0.346) data 0.000 (0.008) loss 0.0002 (0.0007) lr 5.1527e-04 eta 0:02:07\n",
      "epoch [9/10] batch [60/204] time 0.339 (0.342) data 0.000 (0.006) loss 0.0009 (0.0007) lr 5.1527e-04 eta 0:01:59\n",
      "epoch [9/10] batch [80/204] time 0.337 (0.341) data 0.000 (0.004) loss 0.0007 (0.0007) lr 5.1527e-04 eta 0:01:51\n",
      "epoch [9/10] batch [100/204] time 0.339 (0.340) data 0.000 (0.003) loss 0.0015 (0.0007) lr 5.1527e-04 eta 0:01:44\n",
      "epoch [9/10] batch [120/204] time 0.337 (0.339) data 0.000 (0.003) loss 0.0000 (0.0007) lr 5.1527e-04 eta 0:01:37\n",
      "epoch [9/10] batch [140/204] time 0.334 (0.339) data 0.000 (0.003) loss 0.0003 (0.0007) lr 5.1527e-04 eta 0:01:30\n",
      "epoch [9/10] batch [160/204] time 0.334 (0.339) data 0.000 (0.002) loss 0.0012 (0.0007) lr 5.1527e-04 eta 0:01:24\n",
      "epoch [9/10] batch [180/204] time 0.334 (0.339) data 0.000 (0.002) loss 0.0012 (0.0007) lr 5.1527e-04 eta 0:01:17\n",
      "epoch [9/10] batch [200/204] time 0.332 (0.338) data 0.000 (0.002) loss 0.0003 (0.0007) lr 5.1527e-04 eta 0:01:10\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [10/10] batch [20/204] time 0.332 (0.356) data 0.000 (0.017) loss 0.0020 (0.0007) lr 2.3873e-04 eta 0:01:05\n",
      "epoch [10/10] batch [40/204] time 0.332 (0.347) data 0.000 (0.008) loss 0.0009 (0.0007) lr 2.3873e-04 eta 0:00:56\n",
      "epoch [10/10] batch [60/204] time 0.334 (0.343) data 0.000 (0.006) loss 0.0000 (0.0007) lr 2.3873e-04 eta 0:00:49\n",
      "epoch [10/10] batch [80/204] time 0.334 (0.341) data 0.000 (0.004) loss 0.0007 (0.0007) lr 2.3873e-04 eta 0:00:42\n",
      "epoch [10/10] batch [100/204] time 0.332 (0.340) data 0.000 (0.003) loss 0.0006 (0.0006) lr 2.3873e-04 eta 0:00:35\n",
      "epoch [10/10] batch [120/204] time 0.334 (0.339) data 0.000 (0.003) loss 0.0010 (0.0007) lr 2.3873e-04 eta 0:00:28\n",
      "epoch [10/10] batch [140/204] time 0.331 (0.339) data 0.000 (0.003) loss 0.0012 (0.0006) lr 2.3873e-04 eta 0:00:21\n",
      "epoch [10/10] batch [160/204] time 0.333 (0.338) data 0.000 (0.002) loss 0.0001 (0.0006) lr 2.3873e-04 eta 0:00:14\n",
      "epoch [10/10] batch [180/204] time 0.338 (0.338) data 0.000 (0.002) loss 0.0006 (0.0006) lr 2.3873e-04 eta 0:00:08\n",
      "epoch [10/10] batch [200/204] time 0.337 (0.338) data 0.000 (0.002) loss 0.0014 (0.0006) lr 2.3873e-04 eta 0:00:01\n",
      "Checkpoint saved to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2/VLPromptLearner/model.pth.tar-10\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 11/11 [00:02<00:00,  4.36it/s]\n",
      "=> result\n",
      "* total: 1,053\n",
      "* correct: 1,031\n",
      "* accuracy: 97.9%\n",
      "* error: 2.1%\n",
      "* macro_f1: 97.8%\n",
      "Elapsed: 0:11:25\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_train.sh oxford_flowers 2 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44dcec3-fe12-41ea-953c-f6b1a7f118fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdb281d3-ee09-4948-b1b9-6c935f7d9620",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 10\n",
      "model_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']\n",
      "output_dir: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: new\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_2.pkl\n",
      "SUBSAMPLE NEW CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,410\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'text_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.VPT', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'prompt_learner.ctx', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "Loading weights to VLPromptLearner from \"output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2/VLPromptLearner/model.pth.tar-10\" (epoch = 10)\n",
      "Evaluate on the *test* set\n",
      "100%|███████████████████████████████████████████| 15/15 [00:02<00:00,  5.88it/s]\n",
      "=> result\n",
      "* total: 1,410\n",
      "* correct: 1,088\n",
      "* accuracy: 77.2%\n",
      "* error: 22.8%\n",
      "* macro_f1: 72.1%\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_test.sh oxford_flowers 2 2 new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62509f-cbdd-41e7-b5e8-eb1bcb33157b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "018e7e33-4a26-418f-bbad-a3e18094bbbe",
   "metadata": {},
   "source": [
    "### seed 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "821b3fc0-5283-4065-8c66-323251ace6de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this job and save the output to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "Setting fixed seed: 3\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']\n",
      "output_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 3\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: base\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "RESUME: \n",
      "SEED: 3\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_3.pkl\n",
      "SUBSAMPLE BASE CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,053\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'image_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'prompt_learner.ctx', 'image_encoder.VPT', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3/tensorboard)\n",
      "/home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "epoch [1/10] batch [20/204] time 0.258 (0.290) data 0.000 (0.021) loss 0.0017 (0.0012) lr 1.0000e-05 eta 0:09:46\n",
      "epoch [1/10] batch [40/204] time 0.257 (0.294) data 0.000 (0.011) loss 0.0028 (0.0012) lr 1.0000e-05 eta 0:09:47\n",
      "epoch [1/10] batch [60/204] time 0.287 (0.288) data 0.000 (0.007) loss 0.0012 (0.0012) lr 1.0000e-05 eta 0:09:30\n",
      "epoch [1/10] batch [80/204] time 0.264 (0.286) data 0.000 (0.005) loss 0.0010 (0.0012) lr 1.0000e-05 eta 0:09:21\n",
      "epoch [1/10] batch [100/204] time 0.265 (0.283) data 0.000 (0.004) loss 0.0052 (0.0013) lr 1.0000e-05 eta 0:09:08\n",
      "epoch [1/10] batch [120/204] time 0.264 (0.280) data 0.000 (0.004) loss 0.0007 (0.0013) lr 1.0000e-05 eta 0:08:57\n",
      "epoch [1/10] batch [140/204] time 0.327 (0.279) data 0.000 (0.003) loss 0.0008 (0.0013) lr 1.0000e-05 eta 0:08:49\n",
      "epoch [1/10] batch [160/204] time 0.324 (0.285) data 0.000 (0.003) loss 0.0016 (0.0013) lr 1.0000e-05 eta 0:08:56\n",
      "epoch [1/10] batch [180/204] time 0.323 (0.290) data 0.000 (0.002) loss 0.0009 (0.0013) lr 1.0000e-05 eta 0:08:58\n",
      "epoch [1/10] batch [200/204] time 0.322 (0.293) data 0.000 (0.002) loss 0.0001 (0.0013) lr 1.0000e-05 eta 0:08:59\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [2/10] batch [20/204] time 0.323 (0.345) data 0.000 (0.020) loss 0.0011 (0.0009) lr 2.5000e-03 eta 0:10:25\n",
      "epoch [2/10] batch [40/204] time 0.323 (0.336) data 0.000 (0.010) loss 0.0003 (0.0008) lr 2.5000e-03 eta 0:10:02\n",
      "epoch [2/10] batch [60/204] time 0.325 (0.332) data 0.000 (0.007) loss 0.0008 (0.0009) lr 2.5000e-03 eta 0:09:49\n",
      "epoch [2/10] batch [80/204] time 0.325 (0.331) data 0.000 (0.005) loss 0.0002 (0.0009) lr 2.5000e-03 eta 0:09:41\n",
      "epoch [2/10] batch [100/204] time 0.323 (0.330) data 0.000 (0.004) loss 0.0001 (0.0009) lr 2.5000e-03 eta 0:09:33\n",
      "epoch [2/10] batch [120/204] time 0.322 (0.329) data 0.000 (0.003) loss 0.0006 (0.0009) lr 2.5000e-03 eta 0:09:24\n",
      "epoch [2/10] batch [140/204] time 0.325 (0.328) data 0.000 (0.003) loss 0.0003 (0.0009) lr 2.5000e-03 eta 0:09:16\n",
      "epoch [2/10] batch [160/204] time 0.347 (0.330) data 0.000 (0.003) loss 0.0011 (0.0009) lr 2.5000e-03 eta 0:09:13\n",
      "epoch [2/10] batch [180/204] time 0.349 (0.332) data 0.000 (0.002) loss 0.0013 (0.0009) lr 2.5000e-03 eta 0:09:09\n",
      "epoch [2/10] batch [200/204] time 0.346 (0.334) data 0.000 (0.002) loss 0.0003 (0.0008) lr 2.5000e-03 eta 0:09:05\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [3/10] batch [20/204] time 0.337 (0.368) data 0.000 (0.019) loss 0.0003 (0.0008) lr 2.4388e-03 eta 0:09:53\n",
      "epoch [3/10] batch [40/204] time 0.463 (0.363) data 0.000 (0.010) loss 0.0002 (0.0007) lr 2.4388e-03 eta 0:09:38\n",
      "epoch [3/10] batch [60/204] time 0.272 (0.337) data 0.000 (0.007) loss 0.0004 (0.0008) lr 2.4388e-03 eta 0:08:49\n",
      "epoch [3/10] batch [80/204] time 0.271 (0.320) data 0.000 (0.005) loss 0.0003 (0.0009) lr 2.4388e-03 eta 0:08:16\n",
      "epoch [3/10] batch [100/204] time 0.293 (0.315) data 0.000 (0.004) loss 0.0000 (0.0009) lr 2.4388e-03 eta 0:08:02\n",
      "epoch [3/10] batch [120/204] time 0.289 (0.311) data 0.000 (0.003) loss 0.0011 (0.0008) lr 2.4388e-03 eta 0:07:50\n",
      "epoch [3/10] batch [140/204] time 0.349 (0.309) data 0.000 (0.003) loss 0.0002 (0.0008) lr 2.4388e-03 eta 0:07:41\n",
      "epoch [3/10] batch [160/204] time 0.354 (0.315) data 0.000 (0.003) loss 0.0011 (0.0008) lr 2.4388e-03 eta 0:07:43\n",
      "epoch [3/10] batch [180/204] time 0.353 (0.319) data 0.000 (0.002) loss 0.0007 (0.0008) lr 2.4388e-03 eta 0:07:43\n",
      "epoch [3/10] batch [200/204] time 0.339 (0.322) data 0.000 (0.002) loss 0.0002 (0.0008) lr 2.4388e-03 eta 0:07:41\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [4/10] batch [20/204] time 0.326 (0.350) data 0.000 (0.019) loss 0.0021 (0.0007) lr 2.2613e-03 eta 0:08:12\n",
      "epoch [4/10] batch [40/204] time 0.326 (0.338) data 0.000 (0.010) loss 0.0011 (0.0006) lr 2.2613e-03 eta 0:07:49\n",
      "epoch [4/10] batch [60/204] time 0.327 (0.335) data 0.000 (0.006) loss 0.0011 (0.0007) lr 2.2613e-03 eta 0:07:37\n",
      "epoch [4/10] batch [80/204] time 0.327 (0.332) data 0.000 (0.005) loss 0.0001 (0.0006) lr 2.2613e-03 eta 0:07:27\n",
      "epoch [4/10] batch [100/204] time 0.325 (0.332) data 0.000 (0.004) loss 0.0001 (0.0006) lr 2.2613e-03 eta 0:07:21\n",
      "epoch [4/10] batch [120/204] time 0.320 (0.330) data 0.000 (0.003) loss 0.0005 (0.0006) lr 2.2613e-03 eta 0:07:11\n",
      "epoch [4/10] batch [140/204] time 0.317 (0.329) data 0.000 (0.003) loss 0.0009 (0.0007) lr 2.2613e-03 eta 0:07:03\n",
      "epoch [4/10] batch [160/204] time 0.316 (0.327) data 0.000 (0.003) loss 0.0004 (0.0007) lr 2.2613e-03 eta 0:06:54\n",
      "epoch [4/10] batch [180/204] time 0.324 (0.327) data 0.001 (0.002) loss 0.0004 (0.0007) lr 2.2613e-03 eta 0:06:47\n",
      "epoch [4/10] batch [200/204] time 0.328 (0.327) data 0.000 (0.002) loss 0.0006 (0.0007) lr 2.2613e-03 eta 0:06:41\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [5/10] batch [20/204] time 0.327 (0.344) data 0.000 (0.017) loss 0.0004 (0.0006) lr 1.9847e-03 eta 0:06:53\n",
      "epoch [5/10] batch [40/204] time 0.324 (0.337) data 0.000 (0.009) loss 0.0008 (0.0008) lr 1.9847e-03 eta 0:06:38\n",
      "epoch [5/10] batch [60/204] time 0.324 (0.333) data 0.000 (0.006) loss 0.0008 (0.0008) lr 1.9847e-03 eta 0:06:27\n",
      "epoch [5/10] batch [80/204] time 0.322 (0.331) data 0.000 (0.004) loss 0.0008 (0.0008) lr 1.9847e-03 eta 0:06:18\n",
      "epoch [5/10] batch [100/204] time 0.324 (0.330) data 0.000 (0.004) loss 0.0009 (0.0008) lr 1.9847e-03 eta 0:06:11\n",
      "epoch [5/10] batch [120/204] time 0.324 (0.330) data 0.000 (0.003) loss 0.0000 (0.0008) lr 1.9847e-03 eta 0:06:04\n",
      "epoch [5/10] batch [140/204] time 0.324 (0.329) data 0.000 (0.003) loss 0.0010 (0.0007) lr 1.9847e-03 eta 0:05:56\n",
      "epoch [5/10] batch [160/204] time 0.326 (0.329) data 0.000 (0.002) loss 0.0001 (0.0007) lr 1.9847e-03 eta 0:05:49\n",
      "epoch [5/10] batch [180/204] time 0.323 (0.328) data 0.000 (0.002) loss 0.0008 (0.0007) lr 1.9847e-03 eta 0:05:42\n",
      "epoch [5/10] batch [200/204] time 0.323 (0.328) data 0.000 (0.002) loss 0.0006 (0.0007) lr 1.9847e-03 eta 0:05:35\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [6/10] batch [20/204] time 0.344 (0.357) data 0.000 (0.015) loss 0.0008 (0.0008) lr 1.6363e-03 eta 0:05:56\n",
      "epoch [6/10] batch [40/204] time 0.324 (0.350) data 0.000 (0.007) loss 0.0008 (0.0007) lr 1.6363e-03 eta 0:05:42\n",
      "epoch [6/10] batch [60/204] time 0.327 (0.342) data 0.000 (0.005) loss 0.0010 (0.0007) lr 1.6363e-03 eta 0:05:28\n",
      "epoch [6/10] batch [80/204] time 0.327 (0.339) data 0.000 (0.004) loss 0.0013 (0.0007) lr 1.6363e-03 eta 0:05:18\n",
      "epoch [6/10] batch [100/204] time 0.324 (0.336) data 0.000 (0.003) loss 0.0009 (0.0007) lr 1.6363e-03 eta 0:05:09\n",
      "epoch [6/10] batch [120/204] time 0.326 (0.335) data 0.000 (0.003) loss 0.0000 (0.0007) lr 1.6363e-03 eta 0:05:01\n",
      "epoch [6/10] batch [140/204] time 0.319 (0.334) data 0.000 (0.002) loss 0.0014 (0.0007) lr 1.6363e-03 eta 0:04:53\n",
      "epoch [6/10] batch [160/204] time 0.327 (0.333) data 0.000 (0.002) loss 0.0003 (0.0007) lr 1.6363e-03 eta 0:04:46\n",
      "epoch [6/10] batch [180/204] time 0.328 (0.332) data 0.000 (0.002) loss 0.0006 (0.0007) lr 1.6363e-03 eta 0:04:39\n",
      "epoch [6/10] batch [200/204] time 0.321 (0.332) data 0.000 (0.002) loss 0.0010 (0.0007) lr 1.6363e-03 eta 0:04:31\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [7/10] batch [20/204] time 0.341 (0.361) data 0.000 (0.015) loss 0.0011 (0.0007) lr 1.2500e-03 eta 0:04:47\n",
      "epoch [7/10] batch [40/204] time 0.341 (0.351) data 0.000 (0.008) loss 0.0006 (0.0007) lr 1.2500e-03 eta 0:04:32\n",
      "epoch [7/10] batch [60/204] time 0.338 (0.348) data 0.000 (0.005) loss 0.0009 (0.0007) lr 1.2500e-03 eta 0:04:22\n",
      "epoch [7/10] batch [80/204] time 0.340 (0.347) data 0.000 (0.004) loss 0.0003 (0.0006) lr 1.2500e-03 eta 0:04:15\n",
      "epoch [7/10] batch [100/204] time 0.326 (0.345) data 0.000 (0.003) loss 0.0002 (0.0006) lr 1.2500e-03 eta 0:04:06\n",
      "epoch [7/10] batch [120/204] time 0.325 (0.344) data 0.000 (0.003) loss 0.0001 (0.0006) lr 1.2500e-03 eta 0:03:59\n",
      "epoch [7/10] batch [140/204] time 0.326 (0.341) data 0.000 (0.002) loss 0.0010 (0.0006) lr 1.2500e-03 eta 0:03:50\n",
      "epoch [7/10] batch [160/204] time 0.326 (0.339) data 0.000 (0.002) loss 0.0005 (0.0007) lr 1.2500e-03 eta 0:03:42\n",
      "epoch [7/10] batch [180/204] time 0.324 (0.338) data 0.000 (0.002) loss 0.0001 (0.0007) lr 1.2500e-03 eta 0:03:35\n",
      "epoch [7/10] batch [200/204] time 0.327 (0.337) data 0.000 (0.002) loss 0.0020 (0.0007) lr 1.2500e-03 eta 0:03:27\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [8/10] batch [20/204] time 0.344 (0.342) data 0.000 (0.015) loss 0.0003 (0.0008) lr 8.6373e-04 eta 0:03:22\n",
      "epoch [8/10] batch [40/204] time 0.325 (0.337) data 0.000 (0.008) loss 0.0009 (0.0006) lr 8.6373e-04 eta 0:03:12\n",
      "epoch [8/10] batch [60/204] time 0.327 (0.333) data 0.000 (0.005) loss 0.0011 (0.0007) lr 8.6373e-04 eta 0:03:03\n",
      "epoch [8/10] batch [80/204] time 0.327 (0.332) data 0.000 (0.004) loss 0.0001 (0.0006) lr 8.6373e-04 eta 0:02:56\n",
      "epoch [8/10] batch [100/204] time 0.326 (0.331) data 0.000 (0.003) loss 0.0003 (0.0007) lr 8.6373e-04 eta 0:02:49\n",
      "epoch [8/10] batch [120/204] time 0.342 (0.330) data 0.000 (0.003) loss 0.0006 (0.0007) lr 8.6373e-04 eta 0:02:42\n",
      "epoch [8/10] batch [140/204] time 0.325 (0.329) data 0.000 (0.002) loss 0.0002 (0.0007) lr 8.6373e-04 eta 0:02:35\n",
      "epoch [8/10] batch [160/204] time 0.325 (0.329) data 0.000 (0.002) loss 0.0007 (0.0007) lr 8.6373e-04 eta 0:02:28\n",
      "epoch [8/10] batch [180/204] time 0.324 (0.329) data 0.000 (0.002) loss 0.0000 (0.0007) lr 8.6373e-04 eta 0:02:22\n",
      "epoch [8/10] batch [200/204] time 0.325 (0.329) data 0.000 (0.002) loss 0.0007 (0.0007) lr 8.6373e-04 eta 0:02:15\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [9/10] batch [20/204] time 0.325 (0.341) data 0.000 (0.014) loss 0.0008 (0.0005) lr 5.1527e-04 eta 0:02:12\n",
      "epoch [9/10] batch [40/204] time 0.326 (0.335) data 0.000 (0.007) loss 0.0000 (0.0005) lr 5.1527e-04 eta 0:02:03\n",
      "epoch [9/10] batch [60/204] time 0.323 (0.332) data 0.000 (0.005) loss 0.0006 (0.0005) lr 5.1527e-04 eta 0:01:55\n",
      "epoch [9/10] batch [80/204] time 0.325 (0.330) data 0.000 (0.004) loss 0.0002 (0.0005) lr 5.1527e-04 eta 0:01:48\n",
      "epoch [9/10] batch [100/204] time 0.324 (0.329) data 0.000 (0.003) loss 0.0004 (0.0006) lr 5.1527e-04 eta 0:01:41\n",
      "epoch [9/10] batch [120/204] time 0.325 (0.329) data 0.000 (0.002) loss 0.0019 (0.0006) lr 5.1527e-04 eta 0:01:34\n",
      "epoch [9/10] batch [140/204] time 0.326 (0.329) data 0.000 (0.002) loss 0.0004 (0.0006) lr 5.1527e-04 eta 0:01:28\n",
      "epoch [9/10] batch [160/204] time 0.323 (0.329) data 0.000 (0.002) loss 0.0012 (0.0006) lr 5.1527e-04 eta 0:01:21\n",
      "epoch [9/10] batch [180/204] time 0.325 (0.328) data 0.000 (0.002) loss 0.0001 (0.0006) lr 5.1527e-04 eta 0:01:14\n",
      "epoch [9/10] batch [200/204] time 0.324 (0.328) data 0.000 (0.002) loss 0.0010 (0.0006) lr 5.1527e-04 eta 0:01:08\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [10/10] batch [20/204] time 0.326 (0.346) data 0.000 (0.015) loss 0.0004 (0.0007) lr 2.3873e-04 eta 0:01:03\n",
      "epoch [10/10] batch [40/204] time 0.323 (0.336) data 0.000 (0.008) loss 0.0000 (0.0006) lr 2.3873e-04 eta 0:00:55\n",
      "epoch [10/10] batch [60/204] time 0.326 (0.333) data 0.000 (0.005) loss 0.0004 (0.0007) lr 2.3873e-04 eta 0:00:47\n",
      "epoch [10/10] batch [80/204] time 0.327 (0.331) data 0.000 (0.004) loss 0.0006 (0.0007) lr 2.3873e-04 eta 0:00:41\n",
      "epoch [10/10] batch [100/204] time 0.324 (0.330) data 0.000 (0.003) loss 0.0002 (0.0007) lr 2.3873e-04 eta 0:00:34\n",
      "epoch [10/10] batch [120/204] time 0.328 (0.330) data 0.000 (0.003) loss 0.0008 (0.0007) lr 2.3873e-04 eta 0:00:27\n",
      "epoch [10/10] batch [140/204] time 0.324 (0.330) data 0.000 (0.002) loss 0.0001 (0.0007) lr 2.3873e-04 eta 0:00:21\n",
      "epoch [10/10] batch [160/204] time 0.328 (0.329) data 0.000 (0.002) loss 0.0009 (0.0007) lr 2.3873e-04 eta 0:00:14\n",
      "epoch [10/10] batch [180/204] time 0.326 (0.329) data 0.000 (0.002) loss 0.0014 (0.0007) lr 2.3873e-04 eta 0:00:07\n",
      "epoch [10/10] batch [200/204] time 0.321 (0.329) data 0.000 (0.002) loss 0.0004 (0.0007) lr 2.3873e-04 eta 0:00:01\n",
      "Checkpoint saved to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3/VLPromptLearner/model.pth.tar-10\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 11/11 [00:02<00:00,  4.53it/s]\n",
      "=> result\n",
      "* total: 1,053\n",
      "* correct: 1,030\n",
      "* accuracy: 97.8%\n",
      "* error: 2.2%\n",
      "* macro_f1: 97.8%\n",
      "Elapsed: 0:11:10\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_train.sh oxford_flowers 3 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b20c03-4e7e-4c39-8ae0-42ec9d364fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9591c467-d6f3-4e93-bf91-b8c7973b4b75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting fixed seed: 3\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 10\n",
      "model_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']\n",
      "output_dir: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 3\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: new\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "RESUME: \n",
      "SEED: 3\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_3.pkl\n",
      "SUBSAMPLE NEW CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,410\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'image_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.VPT', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'prompt_learner.ctx', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.6.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "Loading weights to VLPromptLearner from \"output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3/VLPromptLearner/model.pth.tar-10\" (epoch = 10)\n",
      "Evaluate on the *test* set\n",
      "100%|███████████████████████████████████████████| 15/15 [00:02<00:00,  5.03it/s]\n",
      "=> result\n",
      "* total: 1,410\n",
      "* correct: 1,025\n",
      "* accuracy: 72.7%\n",
      "* error: 27.3%\n",
      "* macro_f1: 66.3%\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_test.sh oxford_flowers 3 2 new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b085e-f8ff-4b70-a7e8-d19c5f996544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a708320-911b-4b18-9c30-9889fc405979",
   "metadata": {},
   "source": [
    "## prometar2(maml2) 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8728590-5b23-4fc2-a0eb-3f654163f9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hbcho991/prometar/ProMetaR\n"
     ]
    }
   ],
   "source": [
    "%cd /home/hbcho991/prometar/ProMetaR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d03e742-d078-4857-a3f9-6c82527c8454",
   "metadata": {},
   "source": [
    "### seed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eeba00c-e061-43cb-9b96-634b40274c71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this job and save the output to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']\n",
      "output_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: base\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_1.pkl\n",
      "SUBSAMPLE BASE CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,053\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.ctx', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.VPT', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1/tensorboard)\n",
      "/home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "epoch [1/10] batch [20/204] time 0.331 (0.358) data 0.000 (0.018) loss 0.0014 (0.0014) lr 1.0000e-05 eta 0:12:02\n",
      "epoch [1/10] batch [40/204] time 0.361 (0.353) data 0.000 (0.009) loss 0.0016 (0.0015) lr 1.0000e-05 eta 0:11:45\n",
      "epoch [1/10] batch [60/204] time 0.404 (0.371) data 0.000 (0.006) loss 0.0012 (0.0015) lr 1.0000e-05 eta 0:12:13\n",
      "epoch [1/10] batch [80/204] time 0.399 (0.378) data 0.000 (0.005) loss 0.0014 (0.0014) lr 1.0000e-05 eta 0:12:21\n",
      "epoch [1/10] batch [100/204] time 0.401 (0.383) data 0.000 (0.004) loss 0.0010 (0.0013) lr 1.0000e-05 eta 0:12:23\n",
      "epoch [1/10] batch [120/204] time 0.401 (0.387) data 0.000 (0.003) loss 0.0007 (0.0013) lr 1.0000e-05 eta 0:12:22\n",
      "epoch [1/10] batch [140/204] time 0.404 (0.390) data 0.000 (0.003) loss 0.0011 (0.0012) lr 1.0000e-05 eta 0:12:20\n",
      "epoch [1/10] batch [160/204] time 0.404 (0.391) data 0.000 (0.002) loss 0.0012 (0.0012) lr 1.0000e-05 eta 0:12:15\n",
      "epoch [1/10] batch [180/204] time 0.397 (0.393) data 0.000 (0.002) loss 0.0015 (0.0012) lr 1.0000e-05 eta 0:12:10\n",
      "epoch [1/10] batch [200/204] time 0.402 (0.394) data 0.000 (0.002) loss 0.0014 (0.0012) lr 1.0000e-05 eta 0:12:04\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [2/10] batch [20/204] time 0.425 (0.448) data 0.000 (0.016) loss 0.0004 (0.0010) lr 2.5000e-03 eta 0:13:33\n",
      "epoch [2/10] batch [40/204] time 0.423 (0.437) data 0.001 (0.008) loss 0.0012 (0.0010) lr 2.5000e-03 eta 0:13:05\n",
      "epoch [2/10] batch [60/204] time 0.424 (0.433) data 0.000 (0.006) loss 0.0011 (0.0009) lr 2.5000e-03 eta 0:12:49\n",
      "epoch [2/10] batch [80/204] time 0.429 (0.432) data 0.000 (0.004) loss 0.0013 (0.0010) lr 2.5000e-03 eta 0:12:39\n",
      "epoch [2/10] batch [100/204] time 0.426 (0.432) data 0.000 (0.003) loss 0.0009 (0.0010) lr 2.5000e-03 eta 0:12:29\n",
      "epoch [2/10] batch [120/204] time 0.422 (0.431) data 0.000 (0.003) loss 0.0002 (0.0009) lr 2.5000e-03 eta 0:12:19\n",
      "epoch [2/10] batch [140/204] time 0.425 (0.430) data 0.000 (0.003) loss 0.0003 (0.0010) lr 2.5000e-03 eta 0:12:09\n",
      "epoch [2/10] batch [160/204] time 0.424 (0.430) data 0.000 (0.002) loss 0.0015 (0.0010) lr 2.5000e-03 eta 0:12:00\n",
      "epoch [2/10] batch [180/204] time 0.424 (0.430) data 0.000 (0.002) loss 0.0012 (0.0010) lr 2.5000e-03 eta 0:11:52\n",
      "epoch [2/10] batch [200/204] time 0.428 (0.430) data 0.001 (0.002) loss 0.0001 (0.0010) lr 2.5000e-03 eta 0:11:43\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [3/10] batch [20/204] time 0.426 (0.445) data 0.000 (0.018) loss 0.0005 (0.0007) lr 2.4388e-03 eta 0:11:56\n",
      "epoch [3/10] batch [40/204] time 0.425 (0.438) data 0.000 (0.009) loss 0.0018 (0.0007) lr 2.4388e-03 eta 0:11:37\n",
      "epoch [3/10] batch [60/204] time 0.429 (0.434) data 0.001 (0.006) loss 0.0008 (0.0008) lr 2.4388e-03 eta 0:11:23\n",
      "epoch [3/10] batch [80/204] time 0.427 (0.434) data 0.000 (0.005) loss 0.0013 (0.0008) lr 2.4388e-03 eta 0:11:13\n",
      "epoch [3/10] batch [100/204] time 0.429 (0.432) data 0.000 (0.004) loss 0.0019 (0.0008) lr 2.4388e-03 eta 0:11:02\n",
      "epoch [3/10] batch [120/204] time 0.428 (0.432) data 0.000 (0.003) loss 0.0004 (0.0008) lr 2.4388e-03 eta 0:10:53\n",
      "epoch [3/10] batch [140/204] time 0.426 (0.431) data 0.001 (0.003) loss 0.0013 (0.0008) lr 2.4388e-03 eta 0:10:43\n",
      "epoch [3/10] batch [160/204] time 0.425 (0.431) data 0.000 (0.002) loss 0.0002 (0.0008) lr 2.4388e-03 eta 0:10:34\n",
      "epoch [3/10] batch [180/204] time 0.426 (0.431) data 0.000 (0.002) loss 0.0007 (0.0008) lr 2.4388e-03 eta 0:10:25\n",
      "epoch [3/10] batch [200/204] time 0.427 (0.431) data 0.000 (0.002) loss 0.0007 (0.0008) lr 2.4388e-03 eta 0:10:16\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [4/10] batch [20/204] time 0.425 (0.444) data 0.000 (0.017) loss 0.0004 (0.0008) lr 2.2613e-03 eta 0:10:24\n",
      "epoch [4/10] batch [40/204] time 0.426 (0.435) data 0.000 (0.009) loss 0.0013 (0.0008) lr 2.2613e-03 eta 0:10:03\n",
      "epoch [4/10] batch [60/204] time 0.456 (0.434) data 0.000 (0.006) loss 0.0005 (0.0008) lr 2.2613e-03 eta 0:09:54\n",
      "epoch [4/10] batch [80/204] time 0.424 (0.433) data 0.000 (0.004) loss 0.0000 (0.0008) lr 2.2613e-03 eta 0:09:44\n",
      "epoch [4/10] batch [100/204] time 0.426 (0.432) data 0.000 (0.004) loss 0.0015 (0.0008) lr 2.2613e-03 eta 0:09:33\n",
      "epoch [4/10] batch [120/204] time 0.426 (0.431) data 0.000 (0.003) loss 0.0001 (0.0008) lr 2.2613e-03 eta 0:09:24\n",
      "epoch [4/10] batch [140/204] time 0.426 (0.431) data 0.000 (0.003) loss 0.0010 (0.0008) lr 2.2613e-03 eta 0:09:15\n",
      "epoch [4/10] batch [160/204] time 0.426 (0.431) data 0.000 (0.002) loss 0.0007 (0.0008) lr 2.2613e-03 eta 0:09:06\n",
      "epoch [4/10] batch [180/204] time 0.422 (0.430) data 0.000 (0.002) loss 0.0000 (0.0008) lr 2.2613e-03 eta 0:08:56\n",
      "epoch [4/10] batch [200/204] time 0.425 (0.430) data 0.000 (0.002) loss 0.0002 (0.0007) lr 2.2613e-03 eta 0:08:47\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [5/10] batch [20/204] time 0.424 (0.447) data 0.000 (0.017) loss 0.0003 (0.0006) lr 1.9847e-03 eta 0:08:57\n",
      "epoch [5/10] batch [40/204] time 0.420 (0.436) data 0.000 (0.009) loss 0.0004 (0.0006) lr 1.9847e-03 eta 0:08:35\n",
      "epoch [5/10] batch [60/204] time 0.428 (0.434) data 0.000 (0.006) loss 0.0008 (0.0006) lr 1.9847e-03 eta 0:08:24\n",
      "epoch [5/10] batch [80/204] time 0.427 (0.432) data 0.000 (0.004) loss 0.0012 (0.0007) lr 1.9847e-03 eta 0:08:14\n",
      "epoch [5/10] batch [100/204] time 0.427 (0.432) data 0.000 (0.004) loss 0.0019 (0.0007) lr 1.9847e-03 eta 0:08:05\n",
      "epoch [5/10] batch [120/204] time 0.436 (0.431) data 0.000 (0.003) loss 0.0006 (0.0007) lr 1.9847e-03 eta 0:07:56\n",
      "epoch [5/10] batch [140/204] time 0.436 (0.432) data 0.000 (0.003) loss 0.0015 (0.0007) lr 1.9847e-03 eta 0:07:48\n",
      "epoch [5/10] batch [160/204] time 0.528 (0.434) data 0.000 (0.002) loss 0.0011 (0.0007) lr 1.9847e-03 eta 0:07:41\n",
      "epoch [5/10] batch [180/204] time 0.437 (0.439) data 0.000 (0.002) loss 0.0001 (0.0008) lr 1.9847e-03 eta 0:07:38\n",
      "epoch [5/10] batch [200/204] time 0.438 (0.439) data 0.000 (0.002) loss 0.0010 (0.0008) lr 1.9847e-03 eta 0:07:29\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [6/10] batch [20/204] time 0.437 (0.454) data 0.000 (0.015) loss 0.0010 (0.0006) lr 1.6363e-03 eta 0:07:34\n",
      "epoch [6/10] batch [40/204] time 0.436 (0.452) data 0.000 (0.008) loss 0.0010 (0.0006) lr 1.6363e-03 eta 0:07:22\n",
      "epoch [6/10] batch [60/204] time 0.438 (0.449) data 0.000 (0.005) loss 0.0002 (0.0007) lr 1.6363e-03 eta 0:07:10\n",
      "epoch [6/10] batch [80/204] time 0.438 (0.446) data 0.000 (0.004) loss 0.0005 (0.0007) lr 1.6363e-03 eta 0:06:58\n",
      "epoch [6/10] batch [100/204] time 0.443 (0.444) data 0.000 (0.003) loss 0.0010 (0.0007) lr 1.6363e-03 eta 0:06:48\n",
      "epoch [6/10] batch [120/204] time 0.438 (0.444) data 0.000 (0.003) loss 0.0009 (0.0007) lr 1.6363e-03 eta 0:06:39\n",
      "epoch [6/10] batch [140/204] time 0.437 (0.443) data 0.000 (0.002) loss 0.0009 (0.0007) lr 1.6363e-03 eta 0:06:29\n",
      "epoch [6/10] batch [160/204] time 0.518 (0.443) data 0.001 (0.002) loss 0.0010 (0.0007) lr 1.6363e-03 eta 0:06:20\n",
      "epoch [6/10] batch [180/204] time 0.437 (0.442) data 0.000 (0.002) loss 0.0014 (0.0007) lr 1.6363e-03 eta 0:06:11\n",
      "epoch [6/10] batch [200/204] time 0.436 (0.442) data 0.000 (0.002) loss 0.0000 (0.0007) lr 1.6363e-03 eta 0:06:02\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [7/10] batch [20/204] time 0.434 (0.454) data 0.000 (0.016) loss 0.0001 (0.0007) lr 1.2500e-03 eta 0:06:01\n",
      "epoch [7/10] batch [40/204] time 0.463 (0.446) data 0.000 (0.008) loss 0.0007 (0.0009) lr 1.2500e-03 eta 0:05:45\n",
      "epoch [7/10] batch [60/204] time 0.446 (0.446) data 0.000 (0.005) loss 0.0004 (0.0008) lr 1.2500e-03 eta 0:05:36\n",
      "epoch [7/10] batch [80/204] time 0.436 (0.443) data 0.000 (0.004) loss 0.0001 (0.0007) lr 1.2500e-03 eta 0:05:26\n",
      "epoch [7/10] batch [100/204] time 0.425 (0.441) data 0.000 (0.003) loss 0.0010 (0.0007) lr 1.2500e-03 eta 0:05:15\n",
      "epoch [7/10] batch [120/204] time 0.427 (0.439) data 0.000 (0.003) loss 0.0009 (0.0007) lr 1.2500e-03 eta 0:05:05\n",
      "epoch [7/10] batch [140/204] time 0.357 (0.435) data 0.000 (0.003) loss 0.0016 (0.0007) lr 1.2500e-03 eta 0:04:53\n",
      "epoch [7/10] batch [160/204] time 0.424 (0.428) data 0.000 (0.002) loss 0.0014 (0.0007) lr 1.2500e-03 eta 0:04:40\n",
      "epoch [7/10] batch [180/204] time 0.426 (0.427) data 0.000 (0.002) loss 0.0012 (0.0008) lr 1.2500e-03 eta 0:04:31\n",
      "epoch [7/10] batch [200/204] time 0.445 (0.427) data 0.000 (0.002) loss 0.0012 (0.0007) lr 1.2500e-03 eta 0:04:23\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [8/10] batch [20/204] time 0.426 (0.444) data 0.000 (0.015) loss 0.0020 (0.0007) lr 8.6373e-04 eta 0:04:23\n",
      "epoch [8/10] batch [40/204] time 0.430 (0.436) data 0.000 (0.008) loss 0.0007 (0.0008) lr 8.6373e-04 eta 0:04:09\n",
      "epoch [8/10] batch [60/204] time 0.426 (0.434) data 0.000 (0.005) loss 0.0000 (0.0008) lr 8.6373e-04 eta 0:03:59\n",
      "epoch [8/10] batch [80/204] time 0.428 (0.432) data 0.000 (0.004) loss 0.0009 (0.0007) lr 8.6373e-04 eta 0:03:49\n",
      "epoch [8/10] batch [100/204] time 0.422 (0.432) data 0.000 (0.003) loss 0.0005 (0.0007) lr 8.6373e-04 eta 0:03:41\n",
      "epoch [8/10] batch [120/204] time 0.428 (0.431) data 0.000 (0.003) loss 0.0000 (0.0007) lr 8.6373e-04 eta 0:03:31\n",
      "epoch [8/10] batch [140/204] time 0.425 (0.430) data 0.000 (0.002) loss 0.0010 (0.0007) lr 8.6373e-04 eta 0:03:23\n",
      "epoch [8/10] batch [160/204] time 0.426 (0.430) data 0.000 (0.002) loss 0.0004 (0.0007) lr 8.6373e-04 eta 0:03:14\n",
      "epoch [8/10] batch [180/204] time 0.423 (0.430) data 0.000 (0.002) loss 0.0003 (0.0007) lr 8.6373e-04 eta 0:03:05\n",
      "epoch [8/10] batch [200/204] time 0.425 (0.429) data 0.000 (0.002) loss 0.0007 (0.0008) lr 8.6373e-04 eta 0:02:56\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [9/10] batch [20/204] time 0.447 (0.442) data 0.000 (0.015) loss 0.0025 (0.0006) lr 5.1527e-04 eta 0:02:51\n",
      "epoch [9/10] batch [40/204] time 0.425 (0.436) data 0.000 (0.007) loss 0.0002 (0.0006) lr 5.1527e-04 eta 0:02:40\n",
      "epoch [9/10] batch [60/204] time 0.424 (0.434) data 0.000 (0.005) loss 0.0002 (0.0006) lr 5.1527e-04 eta 0:02:31\n",
      "epoch [9/10] batch [80/204] time 0.428 (0.432) data 0.000 (0.004) loss 0.0003 (0.0006) lr 5.1527e-04 eta 0:02:21\n",
      "epoch [9/10] batch [100/204] time 0.428 (0.431) data 0.000 (0.003) loss 0.0006 (0.0007) lr 5.1527e-04 eta 0:02:12\n",
      "epoch [9/10] batch [120/204] time 0.424 (0.431) data 0.000 (0.003) loss 0.0010 (0.0007) lr 5.1527e-04 eta 0:02:04\n",
      "epoch [9/10] batch [140/204] time 0.427 (0.431) data 0.000 (0.002) loss 0.0003 (0.0007) lr 5.1527e-04 eta 0:01:55\n",
      "epoch [9/10] batch [160/204] time 0.428 (0.430) data 0.000 (0.002) loss 0.0003 (0.0007) lr 5.1527e-04 eta 0:01:46\n",
      "epoch [9/10] batch [180/204] time 0.428 (0.430) data 0.000 (0.002) loss 0.0004 (0.0007) lr 5.1527e-04 eta 0:01:38\n",
      "epoch [9/10] batch [200/204] time 0.427 (0.430) data 0.000 (0.002) loss 0.0001 (0.0007) lr 5.1527e-04 eta 0:01:29\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [10/10] batch [20/204] time 0.426 (0.443) data 0.000 (0.016) loss 0.0007 (0.0006) lr 2.3873e-04 eta 0:01:21\n",
      "epoch [10/10] batch [40/204] time 0.425 (0.437) data 0.000 (0.008) loss 0.0001 (0.0006) lr 2.3873e-04 eta 0:01:11\n",
      "epoch [10/10] batch [60/204] time 0.426 (0.435) data 0.000 (0.005) loss 0.0004 (0.0007) lr 2.3873e-04 eta 0:01:02\n",
      "epoch [10/10] batch [80/204] time 0.424 (0.433) data 0.000 (0.004) loss 0.0003 (0.0007) lr 2.3873e-04 eta 0:00:53\n",
      "epoch [10/10] batch [100/204] time 0.427 (0.431) data 0.000 (0.003) loss 0.0015 (0.0007) lr 2.3873e-04 eta 0:00:44\n",
      "epoch [10/10] batch [120/204] time 0.451 (0.431) data 0.000 (0.003) loss 0.0004 (0.0007) lr 2.3873e-04 eta 0:00:36\n",
      "epoch [10/10] batch [140/204] time 0.444 (0.432) data 0.000 (0.002) loss 0.0008 (0.0007) lr 2.3873e-04 eta 0:00:27\n",
      "epoch [10/10] batch [160/204] time 0.448 (0.434) data 0.000 (0.002) loss 0.0000 (0.0007) lr 2.3873e-04 eta 0:00:19\n",
      "epoch [10/10] batch [180/204] time 0.429 (0.434) data 0.000 (0.002) loss 0.0010 (0.0007) lr 2.3873e-04 eta 0:00:10\n",
      "epoch [10/10] batch [200/204] time 0.427 (0.433) data 0.000 (0.002) loss 0.0007 (0.0007) lr 2.3873e-04 eta 0:00:01\n",
      "Checkpoint saved to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1/VLPromptLearner/model.pth.tar-10\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 11/11 [00:02<00:00,  4.63it/s]\n",
      "=> result\n",
      "* total: 1,053\n",
      "* correct: 1,028\n",
      "* accuracy: 97.6%\n",
      "* error: 2.4%\n",
      "* macro_f1: 97.6%\n",
      "Elapsed: 0:14:39\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_train.sh oxford_flowers 1 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e378f230-0428-41d7-b4f7-d29df2a6ca07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3377ff77-ba2e-43eb-9334-78e483f8f335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 10\n",
      "model_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']\n",
      "output_dir: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: new\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_1.pkl\n",
      "SUBSAMPLE NEW CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,410\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'image_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.VPT', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'prompt_learner.ctx', 'image_encoder.transformer.resblocks.7.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "Loading weights to VLPromptLearner from \"output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1/VLPromptLearner/model.pth.tar-10\" (epoch = 10)\n",
      "Evaluate on the *test* set\n",
      "100%|███████████████████████████████████████████| 15/15 [00:02<00:00,  6.01it/s]\n",
      "=> result\n",
      "* total: 1,410\n",
      "* correct: 1,076\n",
      "* accuracy: 76.3%\n",
      "* error: 23.7%\n",
      "* macro_f1: 70.8%\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_test.sh oxford_flowers 1 2 new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb376d-30bb-4c32-bbcf-7b200ee0081d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "153e9a2e-a539-4b53-bb50-5a3efe2fec16",
   "metadata": {},
   "source": [
    "### seed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "673f5735-9d2c-4fe8-baef-e9f66ab2a1ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this job and save the output to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']\n",
      "output_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: base\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_2.pkl\n",
      "SUBSAMPLE BASE CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,053\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'text_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.VPT', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'prompt_learner.ctx', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2/tensorboard)\n",
      "/home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "epoch [1/10] batch [20/204] time 0.334 (0.362) data 0.000 (0.017) loss 0.0031 (0.0012) lr 1.0000e-05 eta 0:12:11\n",
      "epoch [1/10] batch [40/204] time 0.358 (0.354) data 0.000 (0.009) loss 0.0015 (0.0012) lr 1.0000e-05 eta 0:11:48\n",
      "epoch [1/10] batch [60/204] time 0.342 (0.349) data 0.000 (0.006) loss 0.0006 (0.0011) lr 1.0000e-05 eta 0:11:31\n",
      "epoch [1/10] batch [80/204] time 0.338 (0.347) data 0.000 (0.004) loss 0.0006 (0.0011) lr 1.0000e-05 eta 0:11:20\n",
      "epoch [1/10] batch [100/204] time 0.370 (0.348) data 0.000 (0.004) loss 0.0018 (0.0011) lr 1.0000e-05 eta 0:11:15\n",
      "epoch [1/10] batch [120/204] time 0.436 (0.358) data 0.000 (0.003) loss 0.0020 (0.0012) lr 1.0000e-05 eta 0:11:28\n",
      "epoch [1/10] batch [140/204] time 0.437 (0.370) data 0.000 (0.003) loss 0.0013 (0.0011) lr 1.0000e-05 eta 0:11:42\n",
      "epoch [1/10] batch [160/204] time 0.439 (0.379) data 0.000 (0.002) loss 0.0010 (0.0011) lr 1.0000e-05 eta 0:11:52\n",
      "epoch [1/10] batch [180/204] time 0.437 (0.385) data 0.000 (0.002) loss 0.0008 (0.0011) lr 1.0000e-05 eta 0:11:56\n",
      "epoch [1/10] batch [200/204] time 0.438 (0.391) data 0.000 (0.002) loss 0.0003 (0.0011) lr 1.0000e-05 eta 0:11:59\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [2/10] batch [20/204] time 0.442 (0.458) data 0.000 (0.018) loss 0.0011 (0.0008) lr 2.5000e-03 eta 0:13:52\n",
      "epoch [2/10] batch [40/204] time 0.438 (0.450) data 0.000 (0.009) loss 0.0005 (0.0008) lr 2.5000e-03 eta 0:13:27\n",
      "epoch [2/10] batch [60/204] time 0.439 (0.446) data 0.000 (0.006) loss 0.0007 (0.0008) lr 2.5000e-03 eta 0:13:12\n",
      "epoch [2/10] batch [80/204] time 0.439 (0.445) data 0.000 (0.005) loss 0.0004 (0.0009) lr 2.5000e-03 eta 0:13:01\n",
      "epoch [2/10] batch [100/204] time 0.440 (0.445) data 0.000 (0.004) loss 0.0002 (0.0009) lr 2.5000e-03 eta 0:12:52\n",
      "epoch [2/10] batch [120/204] time 0.442 (0.444) data 0.000 (0.003) loss 0.0013 (0.0009) lr 2.5000e-03 eta 0:12:42\n",
      "epoch [2/10] batch [140/204] time 0.438 (0.443) data 0.000 (0.003) loss 0.0000 (0.0009) lr 2.5000e-03 eta 0:12:32\n",
      "epoch [2/10] batch [160/204] time 0.439 (0.443) data 0.000 (0.002) loss 0.0009 (0.0009) lr 2.5000e-03 eta 0:12:22\n",
      "epoch [2/10] batch [180/204] time 0.438 (0.443) data 0.000 (0.002) loss 0.0006 (0.0008) lr 2.5000e-03 eta 0:12:12\n",
      "epoch [2/10] batch [200/204] time 0.438 (0.442) data 0.000 (0.002) loss 0.0019 (0.0009) lr 2.5000e-03 eta 0:12:03\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [3/10] batch [20/204] time 0.437 (0.455) data 0.000 (0.017) loss 0.0004 (0.0008) lr 2.4388e-03 eta 0:12:13\n",
      "epoch [3/10] batch [40/204] time 0.462 (0.448) data 0.001 (0.008) loss 0.0005 (0.0008) lr 2.4388e-03 eta 0:11:53\n",
      "epoch [3/10] batch [60/204] time 0.440 (0.445) data 0.000 (0.006) loss 0.0011 (0.0007) lr 2.4388e-03 eta 0:11:39\n",
      "epoch [3/10] batch [80/204] time 0.437 (0.444) data 0.000 (0.004) loss 0.0003 (0.0007) lr 2.4388e-03 eta 0:11:29\n",
      "epoch [3/10] batch [100/204] time 0.437 (0.443) data 0.000 (0.004) loss 0.0010 (0.0007) lr 2.4388e-03 eta 0:11:18\n",
      "epoch [3/10] batch [120/204] time 0.435 (0.443) data 0.000 (0.003) loss 0.0014 (0.0007) lr 2.4388e-03 eta 0:11:09\n",
      "epoch [3/10] batch [140/204] time 0.441 (0.442) data 0.000 (0.003) loss 0.0015 (0.0008) lr 2.4388e-03 eta 0:10:59\n",
      "epoch [3/10] batch [160/204] time 0.440 (0.442) data 0.000 (0.002) loss 0.0006 (0.0008) lr 2.4388e-03 eta 0:10:50\n",
      "epoch [3/10] batch [180/204] time 0.437 (0.442) data 0.000 (0.002) loss 0.0007 (0.0007) lr 2.4388e-03 eta 0:10:41\n",
      "epoch [3/10] batch [200/204] time 0.436 (0.442) data 0.000 (0.002) loss 0.0009 (0.0007) lr 2.4388e-03 eta 0:10:32\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [4/10] batch [20/204] time 0.439 (0.457) data 0.000 (0.017) loss 0.0010 (0.0007) lr 2.2613e-03 eta 0:10:42\n",
      "epoch [4/10] batch [40/204] time 0.435 (0.447) data 0.000 (0.009) loss 0.0009 (0.0007) lr 2.2613e-03 eta 0:10:20\n",
      "epoch [4/10] batch [60/204] time 0.437 (0.445) data 0.000 (0.006) loss 0.0006 (0.0007) lr 2.2613e-03 eta 0:10:08\n",
      "epoch [4/10] batch [80/204] time 0.439 (0.444) data 0.000 (0.004) loss 0.0007 (0.0007) lr 2.2613e-03 eta 0:09:58\n",
      "epoch [4/10] batch [100/204] time 0.439 (0.443) data 0.000 (0.004) loss 0.0011 (0.0007) lr 2.2613e-03 eta 0:09:48\n",
      "epoch [4/10] batch [120/204] time 0.450 (0.443) data 0.001 (0.003) loss 0.0006 (0.0007) lr 2.2613e-03 eta 0:09:39\n",
      "epoch [4/10] batch [140/204] time 0.455 (0.445) data 0.000 (0.003) loss 0.0024 (0.0007) lr 2.2613e-03 eta 0:09:33\n",
      "epoch [4/10] batch [160/204] time 0.450 (0.446) data 0.000 (0.002) loss 0.0007 (0.0007) lr 2.2613e-03 eta 0:09:25\n",
      "epoch [4/10] batch [180/204] time 0.455 (0.447) data 0.000 (0.002) loss 0.0008 (0.0007) lr 2.2613e-03 eta 0:09:17\n",
      "epoch [4/10] batch [200/204] time 0.449 (0.447) data 0.000 (0.002) loss 0.0011 (0.0007) lr 2.2613e-03 eta 0:09:09\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [5/10] batch [20/204] time 0.467 (0.490) data 0.000 (0.020) loss 0.0006 (0.0007) lr 1.9847e-03 eta 0:09:49\n",
      "epoch [5/10] batch [40/204] time 0.449 (0.473) data 0.000 (0.010) loss 0.0003 (0.0008) lr 1.9847e-03 eta 0:09:20\n",
      "epoch [5/10] batch [60/204] time 0.447 (0.465) data 0.001 (0.007) loss 0.0008 (0.0008) lr 1.9847e-03 eta 0:09:00\n",
      "epoch [5/10] batch [80/204] time 0.435 (0.459) data 0.000 (0.005) loss 0.0000 (0.0007) lr 1.9847e-03 eta 0:08:45\n",
      "epoch [5/10] batch [100/204] time 0.436 (0.455) data 0.000 (0.004) loss 0.0014 (0.0008) lr 1.9847e-03 eta 0:08:31\n",
      "epoch [5/10] batch [120/204] time 0.469 (0.455) data 0.000 (0.004) loss 0.0001 (0.0007) lr 1.9847e-03 eta 0:08:22\n",
      "epoch [5/10] batch [140/204] time 0.443 (0.454) data 0.001 (0.003) loss 0.0002 (0.0007) lr 1.9847e-03 eta 0:08:12\n",
      "epoch [5/10] batch [160/204] time 0.448 (0.453) data 0.000 (0.003) loss 0.0001 (0.0007) lr 1.9847e-03 eta 0:08:02\n",
      "epoch [5/10] batch [180/204] time 0.436 (0.451) data 0.000 (0.003) loss 0.0016 (0.0007) lr 1.9847e-03 eta 0:07:51\n",
      "epoch [5/10] batch [200/204] time 0.433 (0.450) data 0.000 (0.002) loss 0.0001 (0.0007) lr 1.9847e-03 eta 0:07:40\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [6/10] batch [20/204] time 0.453 (0.479) data 0.000 (0.018) loss 0.0009 (0.0007) lr 1.6363e-03 eta 0:07:59\n",
      "epoch [6/10] batch [40/204] time 0.454 (0.467) data 0.000 (0.009) loss 0.0014 (0.0007) lr 1.6363e-03 eta 0:07:37\n",
      "epoch [6/10] batch [60/204] time 0.452 (0.464) data 0.000 (0.006) loss 0.0005 (0.0007) lr 1.6363e-03 eta 0:07:25\n",
      "epoch [6/10] batch [80/204] time 0.452 (0.462) data 0.000 (0.005) loss 0.0000 (0.0006) lr 1.6363e-03 eta 0:07:13\n",
      "epoch [6/10] batch [100/204] time 0.451 (0.463) data 0.000 (0.004) loss 0.0010 (0.0007) lr 1.6363e-03 eta 0:07:05\n",
      "epoch [6/10] batch [120/204] time 0.341 (0.450) data 0.000 (0.003) loss 0.0008 (0.0007) lr 1.6363e-03 eta 0:06:45\n",
      "epoch [6/10] batch [140/204] time 0.341 (0.435) data 0.000 (0.003) loss 0.0000 (0.0006) lr 1.6363e-03 eta 0:06:22\n",
      "epoch [6/10] batch [160/204] time 0.405 (0.424) data 0.000 (0.002) loss 0.0011 (0.0007) lr 1.6363e-03 eta 0:06:04\n",
      "epoch [6/10] batch [180/204] time 0.344 (0.415) data 0.000 (0.002) loss 0.0018 (0.0007) lr 1.6363e-03 eta 0:05:48\n",
      "epoch [6/10] batch [200/204] time 0.440 (0.412) data 0.000 (0.002) loss 0.0006 (0.0007) lr 1.6363e-03 eta 0:05:37\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [7/10] batch [20/204] time 0.440 (0.457) data 0.000 (0.015) loss 0.0008 (0.0008) lr 1.2500e-03 eta 0:06:03\n",
      "epoch [7/10] batch [40/204] time 0.443 (0.448) data 0.000 (0.008) loss 0.0015 (0.0007) lr 1.2500e-03 eta 0:05:48\n",
      "epoch [7/10] batch [60/204] time 0.438 (0.448) data 0.000 (0.005) loss 0.0000 (0.0007) lr 1.2500e-03 eta 0:05:38\n",
      "epoch [7/10] batch [80/204] time 0.439 (0.446) data 0.000 (0.004) loss 0.0006 (0.0007) lr 1.2500e-03 eta 0:05:28\n",
      "epoch [7/10] batch [100/204] time 0.440 (0.445) data 0.000 (0.003) loss 0.0007 (0.0007) lr 1.2500e-03 eta 0:05:18\n",
      "epoch [7/10] batch [120/204] time 0.442 (0.444) data 0.000 (0.003) loss 0.0006 (0.0007) lr 1.2500e-03 eta 0:05:09\n",
      "epoch [7/10] batch [140/204] time 0.435 (0.444) data 0.000 (0.002) loss 0.0006 (0.0007) lr 1.2500e-03 eta 0:05:00\n",
      "epoch [7/10] batch [160/204] time 0.440 (0.444) data 0.000 (0.002) loss 0.0025 (0.0007) lr 1.2500e-03 eta 0:04:51\n",
      "epoch [7/10] batch [180/204] time 0.439 (0.443) data 0.000 (0.002) loss 0.0001 (0.0007) lr 1.2500e-03 eta 0:04:42\n",
      "epoch [7/10] batch [200/204] time 0.438 (0.443) data 0.000 (0.002) loss 0.0014 (0.0007) lr 1.2500e-03 eta 0:04:32\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [8/10] batch [20/204] time 0.440 (0.455) data 0.000 (0.014) loss 0.0007 (0.0009) lr 8.6373e-04 eta 0:04:29\n",
      "epoch [8/10] batch [40/204] time 0.436 (0.447) data 0.000 (0.007) loss 0.0002 (0.0008) lr 8.6373e-04 eta 0:04:15\n",
      "epoch [8/10] batch [60/204] time 0.440 (0.446) data 0.000 (0.005) loss 0.0009 (0.0008) lr 8.6373e-04 eta 0:04:06\n",
      "epoch [8/10] batch [80/204] time 0.437 (0.445) data 0.000 (0.004) loss 0.0013 (0.0008) lr 8.6373e-04 eta 0:03:56\n",
      "epoch [8/10] batch [100/204] time 0.439 (0.444) data 0.000 (0.003) loss 0.0013 (0.0008) lr 8.6373e-04 eta 0:03:47\n",
      "epoch [8/10] batch [120/204] time 0.437 (0.443) data 0.000 (0.003) loss 0.0007 (0.0008) lr 8.6373e-04 eta 0:03:38\n",
      "epoch [8/10] batch [140/204] time 0.439 (0.443) data 0.000 (0.002) loss 0.0007 (0.0008) lr 8.6373e-04 eta 0:03:29\n",
      "epoch [8/10] batch [160/204] time 0.441 (0.443) data 0.000 (0.002) loss 0.0005 (0.0007) lr 8.6373e-04 eta 0:03:20\n",
      "epoch [8/10] batch [180/204] time 0.441 (0.442) data 0.000 (0.002) loss 0.0006 (0.0008) lr 8.6373e-04 eta 0:03:11\n",
      "epoch [8/10] batch [200/204] time 0.438 (0.442) data 0.000 (0.002) loss 0.0009 (0.0008) lr 8.6373e-04 eta 0:03:02\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [9/10] batch [20/204] time 0.441 (0.458) data 0.000 (0.015) loss 0.0001 (0.0006) lr 5.1527e-04 eta 0:02:57\n",
      "epoch [9/10] batch [40/204] time 0.438 (0.448) data 0.000 (0.007) loss 0.0005 (0.0007) lr 5.1527e-04 eta 0:02:45\n",
      "epoch [9/10] batch [60/204] time 0.437 (0.447) data 0.000 (0.005) loss 0.0010 (0.0007) lr 5.1527e-04 eta 0:02:35\n",
      "epoch [9/10] batch [80/204] time 0.439 (0.445) data 0.000 (0.004) loss 0.0000 (0.0006) lr 5.1527e-04 eta 0:02:25\n",
      "epoch [9/10] batch [100/204] time 0.436 (0.444) data 0.000 (0.003) loss 0.0005 (0.0006) lr 5.1527e-04 eta 0:02:16\n",
      "epoch [9/10] batch [120/204] time 0.436 (0.443) data 0.000 (0.003) loss 0.0003 (0.0006) lr 5.1527e-04 eta 0:02:07\n",
      "epoch [9/10] batch [140/204] time 0.437 (0.442) data 0.000 (0.002) loss 0.0017 (0.0006) lr 5.1527e-04 eta 0:01:58\n",
      "epoch [9/10] batch [160/204] time 0.436 (0.442) data 0.000 (0.002) loss 0.0012 (0.0006) lr 5.1527e-04 eta 0:01:49\n",
      "epoch [9/10] batch [180/204] time 0.443 (0.442) data 0.000 (0.002) loss 0.0002 (0.0006) lr 5.1527e-04 eta 0:01:40\n",
      "epoch [9/10] batch [200/204] time 0.439 (0.442) data 0.000 (0.002) loss 0.0006 (0.0007) lr 5.1527e-04 eta 0:01:31\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [10/10] batch [20/204] time 0.438 (0.454) data 0.000 (0.015) loss 0.0001 (0.0006) lr 2.3873e-04 eta 0:01:23\n",
      "epoch [10/10] batch [40/204] time 0.439 (0.450) data 0.000 (0.007) loss 0.0001 (0.0005) lr 2.3873e-04 eta 0:01:13\n",
      "epoch [10/10] batch [60/204] time 0.440 (0.447) data 0.000 (0.005) loss 0.0008 (0.0006) lr 2.3873e-04 eta 0:01:04\n",
      "epoch [10/10] batch [80/204] time 0.441 (0.445) data 0.000 (0.004) loss 0.0008 (0.0006) lr 2.3873e-04 eta 0:00:55\n",
      "epoch [10/10] batch [100/204] time 0.435 (0.444) data 0.000 (0.003) loss 0.0000 (0.0006) lr 2.3873e-04 eta 0:00:46\n",
      "epoch [10/10] batch [120/204] time 0.441 (0.444) data 0.000 (0.003) loss 0.0000 (0.0006) lr 2.3873e-04 eta 0:00:37\n",
      "epoch [10/10] batch [140/204] time 0.441 (0.444) data 0.000 (0.002) loss 0.0008 (0.0006) lr 2.3873e-04 eta 0:00:28\n",
      "epoch [10/10] batch [160/204] time 0.437 (0.443) data 0.000 (0.002) loss 0.0007 (0.0006) lr 2.3873e-04 eta 0:00:19\n",
      "epoch [10/10] batch [180/204] time 0.439 (0.443) data 0.001 (0.002) loss 0.0006 (0.0006) lr 2.3873e-04 eta 0:00:10\n",
      "epoch [10/10] batch [200/204] time 0.436 (0.443) data 0.000 (0.002) loss 0.0012 (0.0006) lr 2.3873e-04 eta 0:00:01\n",
      "Checkpoint saved to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2/VLPromptLearner/model.pth.tar-10\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 11/11 [00:02<00:00,  4.58it/s]\n",
      "=> result\n",
      "* total: 1,053\n",
      "* correct: 1,033\n",
      "* accuracy: 98.1%\n",
      "* error: 1.9%\n",
      "* macro_f1: 98.0%\n",
      "Elapsed: 0:14:53\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_train.sh oxford_flowers 2 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b9ddf-43bd-4708-a188-c752eb32e8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aeecace-3f80-4277-aa95-7eea799f9ff1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 10\n",
      "model_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']\n",
      "output_dir: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: new\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_2.pkl\n",
      "SUBSAMPLE NEW CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,410\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'text_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.VPT', 'prompt_learner.ctx', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "Loading weights to VLPromptLearner from \"output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2/VLPromptLearner/model.pth.tar-10\" (epoch = 10)\n",
      "Evaluate on the *test* set\n",
      "100%|███████████████████████████████████████████| 15/15 [00:02<00:00,  6.11it/s]\n",
      "=> result\n",
      "* total: 1,410\n",
      "* correct: 1,087\n",
      "* accuracy: 77.1%\n",
      "* error: 22.9%\n",
      "* macro_f1: 72.1%\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_test.sh oxford_flowers 2 2 new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21c1c44-de28-4e11-ab07-a4f7ef3de686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41307319-421b-4ddf-bfeb-b1591807fa9d",
   "metadata": {},
   "source": [
    "### seed3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9b78934-166d-41e2-8261-d1ee8b6fca99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this job and save the output to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "Setting fixed seed: 3\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']\n",
      "output_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 3\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: base\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "RESUME: \n",
      "SEED: 3\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_3.pkl\n",
      "SUBSAMPLE BASE CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,053\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'image_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.VPT', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'prompt_learner.ctx', 'image_encoder.transformer.resblocks.5.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3/tensorboard)\n",
      "/home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "epoch [1/10] batch [20/204] time 0.326 (0.348) data 0.000 (0.017) loss 0.0011 (0.0011) lr 1.0000e-05 eta 0:11:43\n",
      "epoch [1/10] batch [40/204] time 0.372 (0.339) data 0.000 (0.009) loss 0.0014 (0.0011) lr 1.0000e-05 eta 0:11:18\n",
      "epoch [1/10] batch [60/204] time 0.330 (0.348) data 0.000 (0.006) loss 0.0041 (0.0012) lr 1.0000e-05 eta 0:11:29\n",
      "epoch [1/10] batch [80/204] time 0.397 (0.353) data 0.000 (0.004) loss 0.0009 (0.0012) lr 1.0000e-05 eta 0:11:31\n",
      "epoch [1/10] batch [100/204] time 0.398 (0.362) data 0.000 (0.004) loss 0.0001 (0.0011) lr 1.0000e-05 eta 0:11:42\n",
      "epoch [1/10] batch [120/204] time 0.398 (0.369) data 0.000 (0.003) loss 0.0003 (0.0012) lr 1.0000e-05 eta 0:11:48\n",
      "epoch [1/10] batch [140/204] time 0.423 (0.376) data 0.000 (0.003) loss 0.0019 (0.0011) lr 1.0000e-05 eta 0:11:54\n",
      "epoch [1/10] batch [160/204] time 0.418 (0.382) data 0.000 (0.002) loss 0.0007 (0.0011) lr 1.0000e-05 eta 0:11:58\n",
      "epoch [1/10] batch [180/204] time 0.422 (0.387) data 0.000 (0.002) loss 0.0011 (0.0011) lr 1.0000e-05 eta 0:11:59\n",
      "epoch [1/10] batch [200/204] time 0.422 (0.391) data 0.000 (0.002) loss 0.0010 (0.0011) lr 1.0000e-05 eta 0:11:58\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [2/10] batch [20/204] time 0.480 (0.455) data 0.000 (0.018) loss 0.0000 (0.0006) lr 2.5000e-03 eta 0:13:46\n",
      "epoch [2/10] batch [40/204] time 0.422 (0.439) data 0.000 (0.009) loss 0.0023 (0.0009) lr 2.5000e-03 eta 0:13:08\n",
      "epoch [2/10] batch [60/204] time 0.424 (0.434) data 0.000 (0.006) loss 0.0006 (0.0010) lr 2.5000e-03 eta 0:12:50\n",
      "epoch [2/10] batch [80/204] time 0.422 (0.432) data 0.000 (0.005) loss 0.0003 (0.0009) lr 2.5000e-03 eta 0:12:38\n",
      "epoch [2/10] batch [100/204] time 0.423 (0.431) data 0.000 (0.004) loss 0.0010 (0.0009) lr 2.5000e-03 eta 0:12:27\n",
      "epoch [2/10] batch [120/204] time 0.424 (0.429) data 0.000 (0.003) loss 0.0001 (0.0009) lr 2.5000e-03 eta 0:12:16\n",
      "epoch [2/10] batch [140/204] time 0.421 (0.428) data 0.000 (0.003) loss 0.0002 (0.0009) lr 2.5000e-03 eta 0:12:06\n",
      "epoch [2/10] batch [160/204] time 0.419 (0.428) data 0.000 (0.003) loss 0.0004 (0.0009) lr 2.5000e-03 eta 0:11:56\n",
      "epoch [2/10] batch [180/204] time 0.421 (0.427) data 0.000 (0.002) loss 0.0001 (0.0009) lr 2.5000e-03 eta 0:11:47\n",
      "epoch [2/10] batch [200/204] time 0.417 (0.427) data 0.000 (0.002) loss 0.0006 (0.0009) lr 2.5000e-03 eta 0:11:37\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [3/10] batch [20/204] time 0.422 (0.438) data 0.000 (0.017) loss 0.0009 (0.0009) lr 2.4388e-03 eta 0:11:46\n",
      "epoch [3/10] batch [40/204] time 0.422 (0.429) data 0.000 (0.009) loss 0.0011 (0.0008) lr 2.4388e-03 eta 0:11:23\n",
      "epoch [3/10] batch [60/204] time 0.422 (0.428) data 0.000 (0.006) loss 0.0006 (0.0008) lr 2.4388e-03 eta 0:11:12\n",
      "epoch [3/10] batch [80/204] time 0.421 (0.427) data 0.000 (0.005) loss 0.0012 (0.0008) lr 2.4388e-03 eta 0:11:02\n",
      "epoch [3/10] batch [100/204] time 0.420 (0.426) data 0.000 (0.004) loss 0.0016 (0.0008) lr 2.4388e-03 eta 0:10:52\n",
      "epoch [3/10] batch [120/204] time 0.420 (0.425) data 0.000 (0.003) loss 0.0011 (0.0008) lr 2.4388e-03 eta 0:10:42\n",
      "epoch [3/10] batch [140/204] time 0.422 (0.425) data 0.000 (0.003) loss 0.0008 (0.0008) lr 2.4388e-03 eta 0:10:33\n",
      "epoch [3/10] batch [160/204] time 0.422 (0.424) data 0.000 (0.002) loss 0.0006 (0.0008) lr 2.4388e-03 eta 0:10:24\n",
      "epoch [3/10] batch [180/204] time 0.421 (0.424) data 0.000 (0.002) loss 0.0006 (0.0008) lr 2.4388e-03 eta 0:10:16\n",
      "epoch [3/10] batch [200/204] time 0.419 (0.424) data 0.000 (0.002) loss 0.0006 (0.0008) lr 2.4388e-03 eta 0:10:07\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [4/10] batch [20/204] time 0.422 (0.439) data 0.000 (0.017) loss 0.0000 (0.0007) lr 2.2613e-03 eta 0:10:18\n",
      "epoch [4/10] batch [40/204] time 0.422 (0.430) data 0.000 (0.009) loss 0.0006 (0.0007) lr 2.2613e-03 eta 0:09:57\n",
      "epoch [4/10] batch [60/204] time 0.424 (0.427) data 0.000 (0.006) loss 0.0008 (0.0007) lr 2.2613e-03 eta 0:09:44\n",
      "epoch [4/10] batch [80/204] time 0.420 (0.428) data 0.000 (0.005) loss 0.0016 (0.0007) lr 2.2613e-03 eta 0:09:36\n",
      "epoch [4/10] batch [100/204] time 0.420 (0.426) data 0.000 (0.004) loss 0.0010 (0.0008) lr 2.2613e-03 eta 0:09:26\n",
      "epoch [4/10] batch [120/204] time 0.421 (0.426) data 0.000 (0.003) loss 0.0014 (0.0008) lr 2.2613e-03 eta 0:09:16\n",
      "epoch [4/10] batch [140/204] time 0.420 (0.425) data 0.000 (0.003) loss 0.0010 (0.0008) lr 2.2613e-03 eta 0:09:07\n",
      "epoch [4/10] batch [160/204] time 0.423 (0.425) data 0.000 (0.002) loss 0.0004 (0.0008) lr 2.2613e-03 eta 0:08:59\n",
      "epoch [4/10] batch [180/204] time 0.422 (0.425) data 0.000 (0.002) loss 0.0007 (0.0008) lr 2.2613e-03 eta 0:08:50\n",
      "epoch [4/10] batch [200/204] time 0.425 (0.425) data 0.000 (0.002) loss 0.0005 (0.0008) lr 2.2613e-03 eta 0:08:41\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [5/10] batch [20/204] time 0.421 (0.439) data 0.000 (0.018) loss 0.0004 (0.0006) lr 1.9847e-03 eta 0:08:49\n",
      "epoch [5/10] batch [40/204] time 0.419 (0.432) data 0.000 (0.009) loss 0.0005 (0.0005) lr 1.9847e-03 eta 0:08:31\n",
      "epoch [5/10] batch [60/204] time 0.420 (0.430) data 0.000 (0.006) loss 0.0008 (0.0006) lr 1.9847e-03 eta 0:08:20\n",
      "epoch [5/10] batch [80/204] time 0.430 (0.428) data 0.000 (0.005) loss 0.0000 (0.0006) lr 1.9847e-03 eta 0:08:09\n",
      "epoch [5/10] batch [100/204] time 0.421 (0.426) data 0.000 (0.004) loss 0.0006 (0.0007) lr 1.9847e-03 eta 0:07:59\n",
      "epoch [5/10] batch [120/204] time 0.420 (0.426) data 0.000 (0.003) loss 0.0002 (0.0007) lr 1.9847e-03 eta 0:07:50\n",
      "epoch [5/10] batch [140/204] time 0.421 (0.426) data 0.000 (0.003) loss 0.0004 (0.0007) lr 1.9847e-03 eta 0:07:41\n",
      "epoch [5/10] batch [160/204] time 0.422 (0.425) data 0.000 (0.002) loss 0.0002 (0.0007) lr 1.9847e-03 eta 0:07:32\n",
      "epoch [5/10] batch [180/204] time 0.421 (0.425) data 0.000 (0.002) loss 0.0003 (0.0007) lr 1.9847e-03 eta 0:07:23\n",
      "epoch [5/10] batch [200/204] time 0.421 (0.424) data 0.000 (0.002) loss 0.0009 (0.0007) lr 1.9847e-03 eta 0:07:14\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [6/10] batch [20/204] time 0.426 (0.437) data 0.000 (0.014) loss 0.0018 (0.0009) lr 1.6363e-03 eta 0:07:16\n",
      "epoch [6/10] batch [40/204] time 0.425 (0.430) data 0.000 (0.007) loss 0.0008 (0.0008) lr 1.6363e-03 eta 0:07:00\n",
      "epoch [6/10] batch [60/204] time 0.420 (0.432) data 0.000 (0.005) loss 0.0007 (0.0007) lr 1.6363e-03 eta 0:06:55\n",
      "epoch [6/10] batch [80/204] time 0.422 (0.430) data 0.000 (0.004) loss 0.0003 (0.0008) lr 1.6363e-03 eta 0:06:43\n",
      "epoch [6/10] batch [100/204] time 0.419 (0.428) data 0.000 (0.003) loss 0.0006 (0.0008) lr 1.6363e-03 eta 0:06:33\n",
      "epoch [6/10] batch [120/204] time 0.420 (0.427) data 0.000 (0.003) loss 0.0001 (0.0008) lr 1.6363e-03 eta 0:06:24\n",
      "epoch [6/10] batch [140/204] time 0.423 (0.427) data 0.001 (0.002) loss 0.0003 (0.0007) lr 1.6363e-03 eta 0:06:15\n",
      "epoch [6/10] batch [160/204] time 0.488 (0.427) data 0.000 (0.002) loss 0.0004 (0.0007) lr 1.6363e-03 eta 0:06:06\n",
      "epoch [6/10] batch [180/204] time 0.423 (0.426) data 0.000 (0.002) loss 0.0009 (0.0008) lr 1.6363e-03 eta 0:05:57\n",
      "epoch [6/10] batch [200/204] time 0.425 (0.426) data 0.000 (0.002) loss 0.0000 (0.0007) lr 1.6363e-03 eta 0:05:49\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [7/10] batch [20/204] time 0.432 (0.452) data 0.000 (0.014) loss 0.0009 (0.0007) lr 1.2500e-03 eta 0:06:00\n",
      "epoch [7/10] batch [40/204] time 0.432 (0.442) data 0.000 (0.007) loss 0.0010 (0.0008) lr 1.2500e-03 eta 0:05:43\n",
      "epoch [7/10] batch [60/204] time 0.434 (0.440) data 0.000 (0.005) loss 0.0007 (0.0008) lr 1.2500e-03 eta 0:05:32\n",
      "epoch [7/10] batch [80/204] time 0.439 (0.438) data 0.000 (0.004) loss 0.0004 (0.0008) lr 1.2500e-03 eta 0:05:22\n",
      "epoch [7/10] batch [100/204] time 0.433 (0.438) data 0.000 (0.003) loss 0.0004 (0.0008) lr 1.2500e-03 eta 0:05:13\n",
      "epoch [7/10] batch [120/204] time 0.433 (0.437) data 0.000 (0.003) loss 0.0013 (0.0008) lr 1.2500e-03 eta 0:05:03\n",
      "epoch [7/10] batch [140/204] time 0.434 (0.437) data 0.000 (0.002) loss 0.0012 (0.0007) lr 1.2500e-03 eta 0:04:55\n",
      "epoch [7/10] batch [160/204] time 0.430 (0.436) data 0.000 (0.002) loss 0.0013 (0.0007) lr 1.2500e-03 eta 0:04:46\n",
      "epoch [7/10] batch [180/204] time 0.431 (0.436) data 0.000 (0.002) loss 0.0013 (0.0008) lr 1.2500e-03 eta 0:04:37\n",
      "epoch [7/10] batch [200/204] time 0.430 (0.436) data 0.000 (0.002) loss 0.0009 (0.0007) lr 1.2500e-03 eta 0:04:28\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [8/10] batch [20/204] time 0.431 (0.449) data 0.000 (0.016) loss 0.0009 (0.0007) lr 8.6373e-04 eta 0:04:25\n",
      "epoch [8/10] batch [40/204] time 0.432 (0.443) data 0.000 (0.008) loss 0.0003 (0.0007) lr 8.6373e-04 eta 0:04:13\n",
      "epoch [8/10] batch [60/204] time 0.431 (0.440) data 0.000 (0.006) loss 0.0009 (0.0007) lr 8.6373e-04 eta 0:04:03\n",
      "epoch [8/10] batch [80/204] time 0.433 (0.438) data 0.000 (0.004) loss 0.0017 (0.0007) lr 8.6373e-04 eta 0:03:53\n",
      "epoch [8/10] batch [100/204] time 0.432 (0.437) data 0.000 (0.003) loss 0.0011 (0.0007) lr 8.6373e-04 eta 0:03:43\n",
      "epoch [8/10] batch [120/204] time 0.432 (0.437) data 0.000 (0.003) loss 0.0014 (0.0007) lr 8.6373e-04 eta 0:03:34\n",
      "epoch [8/10] batch [140/204] time 0.433 (0.436) data 0.000 (0.003) loss 0.0002 (0.0007) lr 8.6373e-04 eta 0:03:25\n",
      "epoch [8/10] batch [160/204] time 0.433 (0.436) data 0.000 (0.002) loss 0.0010 (0.0007) lr 8.6373e-04 eta 0:03:16\n",
      "epoch [8/10] batch [180/204] time 0.432 (0.436) data 0.000 (0.002) loss 0.0009 (0.0007) lr 8.6373e-04 eta 0:03:08\n",
      "epoch [8/10] batch [200/204] time 0.427 (0.436) data 0.000 (0.002) loss 0.0000 (0.0007) lr 8.6373e-04 eta 0:02:59\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [9/10] batch [20/204] time 0.356 (0.428) data 0.000 (0.016) loss 0.0008 (0.0008) lr 5.1527e-04 eta 0:02:46\n",
      "epoch [9/10] batch [40/204] time 0.328 (0.384) data 0.000 (0.008) loss 0.0008 (0.0007) lr 5.1527e-04 eta 0:02:21\n",
      "epoch [9/10] batch [60/204] time 0.346 (0.367) data 0.000 (0.005) loss 0.0001 (0.0007) lr 5.1527e-04 eta 0:02:07\n",
      "epoch [9/10] batch [80/204] time 0.331 (0.358) data 0.000 (0.004) loss 0.0011 (0.0007) lr 5.1527e-04 eta 0:01:57\n",
      "epoch [9/10] batch [100/204] time 0.396 (0.356) data 0.000 (0.003) loss 0.0013 (0.0007) lr 5.1527e-04 eta 0:01:49\n",
      "epoch [9/10] batch [120/204] time 0.421 (0.358) data 0.000 (0.003) loss 0.0003 (0.0006) lr 5.1527e-04 eta 0:01:43\n",
      "epoch [9/10] batch [140/204] time 0.351 (0.359) data 0.000 (0.002) loss 0.0009 (0.0007) lr 5.1527e-04 eta 0:01:36\n",
      "epoch [9/10] batch [160/204] time 0.354 (0.359) data 0.000 (0.002) loss 0.0008 (0.0007) lr 5.1527e-04 eta 0:01:29\n",
      "epoch [9/10] batch [180/204] time 0.349 (0.359) data 0.000 (0.002) loss 0.0005 (0.0007) lr 5.1527e-04 eta 0:01:21\n",
      "epoch [9/10] batch [200/204] time 0.355 (0.358) data 0.000 (0.002) loss 0.0006 (0.0007) lr 5.1527e-04 eta 0:01:14\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [10/10] batch [20/204] time 0.398 (0.408) data 0.000 (0.014) loss 0.0010 (0.0007) lr 2.3873e-04 eta 0:01:15\n",
      "epoch [10/10] batch [40/204] time 0.399 (0.407) data 0.000 (0.007) loss 0.0007 (0.0007) lr 2.3873e-04 eta 0:01:06\n",
      "epoch [10/10] batch [60/204] time 0.419 (0.406) data 0.001 (0.005) loss 0.0005 (0.0007) lr 2.3873e-04 eta 0:00:58\n",
      "epoch [10/10] batch [80/204] time 0.400 (0.406) data 0.000 (0.004) loss 0.0007 (0.0007) lr 2.3873e-04 eta 0:00:50\n",
      "epoch [10/10] batch [100/204] time 0.398 (0.405) data 0.000 (0.003) loss 0.0012 (0.0007) lr 2.3873e-04 eta 0:00:42\n",
      "epoch [10/10] batch [120/204] time 0.423 (0.407) data 0.000 (0.003) loss 0.0004 (0.0007) lr 2.3873e-04 eta 0:00:34\n",
      "epoch [10/10] batch [140/204] time 0.423 (0.410) data 0.000 (0.002) loss 0.0007 (0.0007) lr 2.3873e-04 eta 0:00:26\n",
      "epoch [10/10] batch [160/204] time 0.426 (0.411) data 0.000 (0.002) loss 0.0010 (0.0007) lr 2.3873e-04 eta 0:00:18\n",
      "epoch [10/10] batch [180/204] time 0.422 (0.412) data 0.000 (0.002) loss 0.0000 (0.0007) lr 2.3873e-04 eta 0:00:09\n",
      "epoch [10/10] batch [200/204] time 0.423 (0.414) data 0.000 (0.002) loss 0.0011 (0.0007) lr 2.3873e-04 eta 0:00:01\n",
      "Checkpoint saved to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3/VLPromptLearner/model.pth.tar-10\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 11/11 [00:02<00:00,  4.71it/s]\n",
      "=> result\n",
      "* total: 1,053\n",
      "* correct: 1,032\n",
      "* accuracy: 98.0%\n",
      "* error: 2.0%\n",
      "* macro_f1: 97.9%\n",
      "Elapsed: 0:14:13\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_train.sh oxford_flowers 3 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b412cde-dbea-4395-bd0f-c1efad80bd89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09ddf7ba-28ff-4db8-85d0-3fd577d7fc4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting fixed seed: 3\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 10\n",
      "model_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']\n",
      "output_dir: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 3\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: new\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3\n",
      "RESUME: \n",
      "SEED: 3\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_3.pkl\n",
      "SUBSAMPLE NEW CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,410\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'prompt_learner.ctx', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.VPT', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "Loading weights to VLPromptLearner from \"output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed3/VLPromptLearner/model.pth.tar-10\" (epoch = 10)\n",
      "Evaluate on the *test* set\n",
      "100%|███████████████████████████████████████████| 15/15 [00:02<00:00,  6.07it/s]\n",
      "=> result\n",
      "* total: 1,410\n",
      "* correct: 1,084\n",
      "* accuracy: 76.9%\n",
      "* error: 23.1%\n",
      "* macro_f1: 71.6%\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_test.sh oxford_flowers 3 2 new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4e816-fa96-4573-8965-5816fcba317c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f570a3a-90db-4b7a-b8e7-0dfe0230a7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6463abfb-91f3-4420-98f0-ae7f96d54e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d4f1d-cc1c-4b0e-a125-fa73d672e4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf97e421-b565-4af8-872f-56397d58cb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ee6f1-d8e3-42ae-b39a-7ed3fc8d369f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac8aeb59-f7f9-477a-a3c7-89309b9af190",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09d1fb-4925-41d3-815f-0c7fb5904306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9260f1ec-60b7-451c-93d1-292ea9efdf58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this job and save the output to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']\n",
      "output_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: base\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_1.pkl\n",
      "SUBSAMPLE BASE CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,053\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'image_encoder.transformer.resblocks.5.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'prompt_learner.ctx', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.VPT', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1/tensorboard)\n",
      "/home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "epoch [1/10] batch [20/204] time 0.335 (0.355) data 0.000 (0.014) loss 0.0005 (0.0010) lr 1.0000e-05 eta 0:11:57\n",
      "epoch [1/10] batch [40/204] time 0.343 (0.347) data 0.000 (0.007) loss 0.0011 (0.0011) lr 1.0000e-05 eta 0:11:33\n",
      "epoch [1/10] batch [60/204] time 0.335 (0.346) data 0.000 (0.005) loss 0.0030 (0.0011) lr 1.0000e-05 eta 0:11:25\n",
      "epoch [1/10] batch [80/204] time 0.340 (0.346) data 0.000 (0.004) loss 0.0010 (0.0011) lr 1.0000e-05 eta 0:11:18\n",
      "epoch [1/10] batch [100/204] time 0.343 (0.345) data 0.000 (0.003) loss 0.0007 (0.0011) lr 1.0000e-05 eta 0:11:09\n",
      "epoch [1/10] batch [120/204] time 0.348 (0.345) data 0.000 (0.003) loss 0.0013 (0.0012) lr 1.0000e-05 eta 0:11:02\n",
      "epoch [1/10] batch [140/204] time 0.358 (0.345) data 0.000 (0.003) loss 0.0003 (0.0012) lr 1.0000e-05 eta 0:10:56\n",
      "epoch [1/10] batch [160/204] time 0.346 (0.347) data 0.000 (0.003) loss 0.0013 (0.0011) lr 1.0000e-05 eta 0:10:53\n",
      "epoch [1/10] batch [180/204] time 0.338 (0.347) data 0.000 (0.002) loss 0.0004 (0.0011) lr 1.0000e-05 eta 0:10:44\n",
      "epoch [1/10] batch [200/204] time 0.367 (0.347) data 0.000 (0.002) loss 0.0002 (0.0011) lr 1.0000e-05 eta 0:10:38\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [2/10] batch [20/204] time 0.361 (0.373) data 0.001 (0.013) loss 0.0019 (0.0017) lr 2.5000e-03 eta 0:11:17\n",
      "epoch [2/10] batch [40/204] time 0.357 (0.368) data 0.000 (0.007) loss 0.0019 (0.0018) lr 2.5000e-03 eta 0:11:00\n",
      "epoch [2/10] batch [60/204] time 0.343 (0.363) data 0.002 (0.005) loss 0.0019 (0.0018) lr 2.5000e-03 eta 0:10:43\n",
      "epoch [2/10] batch [80/204] time 0.350 (0.358) data 0.000 (0.004) loss 0.0004 (0.0018) lr 2.5000e-03 eta 0:10:29\n",
      "epoch [2/10] batch [100/204] time 0.346 (0.355) data 0.006 (0.004) loss 0.0017 (0.0016) lr 2.5000e-03 eta 0:10:16\n",
      "epoch [2/10] batch [120/204] time 0.342 (0.353) data 0.000 (0.003) loss 0.0006 (0.0015) lr 2.5000e-03 eta 0:10:06\n",
      "epoch [2/10] batch [140/204] time 0.350 (0.352) data 0.000 (0.003) loss 0.0005 (0.0014) lr 2.5000e-03 eta 0:09:57\n",
      "epoch [2/10] batch [160/204] time 0.362 (0.351) data 0.000 (0.003) loss 0.0001 (0.0014) lr 2.5000e-03 eta 0:09:47\n",
      "epoch [2/10] batch [180/204] time 0.343 (0.350) data 0.001 (0.002) loss 0.0011 (0.0013) lr 2.5000e-03 eta 0:09:39\n",
      "epoch [2/10] batch [200/204] time 0.345 (0.349) data 0.000 (0.002) loss 0.0004 (0.0013) lr 2.5000e-03 eta 0:09:31\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [3/10] batch [20/204] time 0.364 (0.366) data 0.000 (0.014) loss 0.0006 (0.0008) lr 2.4388e-03 eta 0:09:50\n",
      "epoch [3/10] batch [40/204] time 0.345 (0.364) data 0.003 (0.007) loss 0.0002 (0.0008) lr 2.4388e-03 eta 0:09:40\n",
      "epoch [3/10] batch [60/204] time 0.339 (0.357) data 0.000 (0.005) loss 0.0007 (0.0009) lr 2.4388e-03 eta 0:09:21\n",
      "epoch [3/10] batch [80/204] time 0.343 (0.355) data 0.001 (0.004) loss 0.0005 (0.0010) lr 2.4388e-03 eta 0:09:10\n",
      "epoch [3/10] batch [100/204] time 0.345 (0.353) data 0.000 (0.004) loss 0.0014 (0.0010) lr 2.4388e-03 eta 0:09:00\n",
      "epoch [3/10] batch [120/204] time 0.343 (0.351) data 0.002 (0.003) loss 0.0004 (0.0010) lr 2.4388e-03 eta 0:08:50\n",
      "epoch [3/10] batch [140/204] time 0.353 (0.353) data 0.000 (0.003) loss 0.0007 (0.0009) lr 2.4388e-03 eta 0:08:46\n",
      "epoch [3/10] batch [160/204] time 0.339 (0.353) data 0.000 (0.003) loss 0.0009 (0.0009) lr 2.4388e-03 eta 0:08:39\n",
      "epoch [3/10] batch [180/204] time 0.342 (0.353) data 0.000 (0.002) loss 0.0013 (0.0009) lr 2.4388e-03 eta 0:08:31\n",
      "epoch [3/10] batch [200/204] time 0.347 (0.351) data 0.000 (0.002) loss 0.0003 (0.0009) lr 2.4388e-03 eta 0:08:23\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [4/10] batch [20/204] time 0.345 (0.357) data 0.003 (0.012) loss 0.0003 (0.0009) lr 2.2613e-03 eta 0:08:22\n",
      "epoch [4/10] batch [40/204] time 0.363 (0.350) data 0.001 (0.006) loss 0.0007 (0.0008) lr 2.2613e-03 eta 0:08:06\n",
      "epoch [4/10] batch [60/204] time 0.344 (0.350) data 0.000 (0.005) loss 0.0011 (0.0007) lr 2.2613e-03 eta 0:07:58\n",
      "epoch [4/10] batch [80/204] time 0.349 (0.348) data 0.000 (0.004) loss 0.0001 (0.0007) lr 2.2613e-03 eta 0:07:49\n",
      "epoch [4/10] batch [100/204] time 0.340 (0.347) data 0.001 (0.003) loss 0.0003 (0.0007) lr 2.2613e-03 eta 0:07:40\n",
      "epoch [4/10] batch [120/204] time 0.347 (0.346) data 0.000 (0.003) loss 0.0016 (0.0008) lr 2.2613e-03 eta 0:07:32\n",
      "epoch [4/10] batch [140/204] time 0.350 (0.346) data 0.001 (0.002) loss 0.0002 (0.0008) lr 2.2613e-03 eta 0:07:25\n",
      "epoch [4/10] batch [160/204] time 0.334 (0.346) data 0.000 (0.002) loss 0.0011 (0.0007) lr 2.2613e-03 eta 0:07:19\n",
      "epoch [4/10] batch [180/204] time 0.351 (0.346) data 0.000 (0.002) loss 0.0012 (0.0007) lr 2.2613e-03 eta 0:07:11\n",
      "epoch [4/10] batch [200/204] time 0.353 (0.347) data 0.000 (0.002) loss 0.0018 (0.0008) lr 2.2613e-03 eta 0:07:05\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [5/10] batch [20/204] time 0.343 (0.369) data 0.000 (0.013) loss 0.0010 (0.0006) lr 1.9847e-03 eta 0:07:24\n",
      "epoch [5/10] batch [40/204] time 0.344 (0.358) data 0.000 (0.007) loss 0.0013 (0.0007) lr 1.9847e-03 eta 0:07:03\n",
      "epoch [5/10] batch [60/204] time 0.342 (0.354) data 0.001 (0.005) loss 0.0004 (0.0007) lr 1.9847e-03 eta 0:06:51\n",
      "epoch [5/10] batch [80/204] time 0.347 (0.352) data 0.002 (0.004) loss 0.0007 (0.0007) lr 1.9847e-03 eta 0:06:42\n",
      "epoch [5/10] batch [100/204] time 0.349 (0.351) data 0.000 (0.003) loss 0.0010 (0.0008) lr 1.9847e-03 eta 0:06:35\n",
      "epoch [5/10] batch [120/204] time 0.345 (0.353) data 0.000 (0.003) loss 0.0004 (0.0008) lr 1.9847e-03 eta 0:06:29\n",
      "epoch [5/10] batch [140/204] time 0.353 (0.352) data 0.000 (0.002) loss 0.0009 (0.0007) lr 1.9847e-03 eta 0:06:21\n",
      "epoch [5/10] batch [160/204] time 0.385 (0.353) data 0.000 (0.002) loss 0.0011 (0.0008) lr 1.9847e-03 eta 0:06:15\n",
      "epoch [5/10] batch [180/204] time 0.350 (0.355) data 0.000 (0.002) loss 0.0001 (0.0008) lr 1.9847e-03 eta 0:06:10\n",
      "epoch [5/10] batch [200/204] time 0.359 (0.356) data 0.000 (0.002) loss 0.0009 (0.0008) lr 1.9847e-03 eta 0:06:04\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [6/10] batch [20/204] time 0.348 (0.371) data 0.000 (0.011) loss 0.0000 (0.0006) lr 1.6363e-03 eta 0:06:11\n",
      "epoch [6/10] batch [40/204] time 0.364 (0.368) data 0.001 (0.006) loss 0.0011 (0.0007) lr 1.6363e-03 eta 0:06:00\n",
      "epoch [6/10] batch [60/204] time 0.358 (0.368) data 0.000 (0.005) loss 0.0001 (0.0007) lr 1.6363e-03 eta 0:05:52\n",
      "epoch [6/10] batch [80/204] time 0.344 (0.364) data 0.000 (0.004) loss 0.0006 (0.0007) lr 1.6363e-03 eta 0:05:42\n",
      "epoch [6/10] batch [100/204] time 0.375 (0.365) data 0.001 (0.003) loss 0.0013 (0.0007) lr 1.6363e-03 eta 0:05:35\n",
      "epoch [6/10] batch [120/204] time 0.365 (0.365) data 0.000 (0.003) loss 0.0001 (0.0007) lr 1.6363e-03 eta 0:05:28\n",
      "epoch [6/10] batch [140/204] time 0.361 (0.366) data 0.003 (0.003) loss 0.0002 (0.0008) lr 1.6363e-03 eta 0:05:21\n",
      "epoch [6/10] batch [160/204] time 0.453 (0.366) data 0.000 (0.002) loss 0.0000 (0.0008) lr 1.6363e-03 eta 0:05:14\n",
      "epoch [6/10] batch [180/204] time 0.350 (0.365) data 0.000 (0.002) loss 0.0009 (0.0008) lr 1.6363e-03 eta 0:05:07\n",
      "epoch [6/10] batch [200/204] time 0.362 (0.365) data 0.000 (0.002) loss 0.0011 (0.0007) lr 1.6363e-03 eta 0:04:59\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [7/10] batch [20/204] time 0.353 (0.372) data 0.004 (0.012) loss 0.0012 (0.0008) lr 1.2500e-03 eta 0:04:55\n",
      "epoch [7/10] batch [40/204] time 0.358 (0.367) data 0.000 (0.006) loss 0.0007 (0.0008) lr 1.2500e-03 eta 0:04:44\n",
      "epoch [7/10] batch [60/204] time 0.354 (0.368) data 0.000 (0.004) loss 0.0000 (0.0008) lr 1.2500e-03 eta 0:04:38\n",
      "epoch [7/10] batch [80/204] time 0.360 (0.366) data 0.000 (0.004) loss 0.0004 (0.0008) lr 1.2500e-03 eta 0:04:29\n",
      "epoch [7/10] batch [100/204] time 0.362 (0.365) data 0.000 (0.003) loss 0.0000 (0.0008) lr 1.2500e-03 eta 0:04:21\n",
      "epoch [7/10] batch [120/204] time 0.356 (0.364) data 0.000 (0.003) loss 0.0011 (0.0008) lr 1.2500e-03 eta 0:04:13\n",
      "epoch [7/10] batch [140/204] time 0.361 (0.365) data 0.000 (0.002) loss 0.0013 (0.0008) lr 1.2500e-03 eta 0:04:06\n",
      "epoch [7/10] batch [160/204] time 0.352 (0.364) data 0.000 (0.002) loss 0.0006 (0.0007) lr 1.2500e-03 eta 0:03:58\n",
      "epoch [7/10] batch [180/204] time 0.373 (0.364) data 0.000 (0.002) loss 0.0004 (0.0007) lr 1.2500e-03 eta 0:03:51\n",
      "epoch [7/10] batch [200/204] time 0.364 (0.364) data 0.000 (0.002) loss 0.0003 (0.0007) lr 1.2500e-03 eta 0:03:44\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [8/10] batch [20/204] time 0.381 (0.386) data 0.000 (0.012) loss 0.0008 (0.0007) lr 8.6373e-04 eta 0:03:48\n",
      "epoch [8/10] batch [40/204] time 0.369 (0.377) data 0.000 (0.007) loss 0.0000 (0.0008) lr 8.6373e-04 eta 0:03:35\n",
      "epoch [8/10] batch [60/204] time 0.367 (0.374) data 0.003 (0.005) loss 0.0011 (0.0007) lr 8.6373e-04 eta 0:03:26\n",
      "epoch [8/10] batch [80/204] time 0.355 (0.371) data 0.000 (0.004) loss 0.0001 (0.0007) lr 8.6373e-04 eta 0:03:17\n",
      "epoch [8/10] batch [100/204] time 0.361 (0.369) data 0.000 (0.003) loss 0.0013 (0.0007) lr 8.6373e-04 eta 0:03:09\n",
      "epoch [8/10] batch [120/204] time 0.357 (0.368) data 0.000 (0.003) loss 0.0000 (0.0007) lr 8.6373e-04 eta 0:03:00\n",
      "epoch [8/10] batch [140/204] time 0.351 (0.367) data 0.000 (0.003) loss 0.0003 (0.0007) lr 8.6373e-04 eta 0:02:53\n",
      "epoch [8/10] batch [160/204] time 0.365 (0.366) data 0.002 (0.002) loss 0.0008 (0.0007) lr 8.6373e-04 eta 0:02:45\n",
      "epoch [8/10] batch [180/204] time 0.368 (0.366) data 0.000 (0.002) loss 0.0003 (0.0007) lr 8.6373e-04 eta 0:02:38\n",
      "epoch [8/10] batch [200/204] time 0.368 (0.367) data 0.000 (0.002) loss 0.0003 (0.0007) lr 8.6373e-04 eta 0:02:31\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [9/10] batch [20/204] time 0.351 (0.371) data 0.000 (0.012) loss 0.0010 (0.0009) lr 5.1527e-04 eta 0:02:24\n",
      "epoch [9/10] batch [40/204] time 0.365 (0.365) data 0.000 (0.006) loss 0.0000 (0.0008) lr 5.1527e-04 eta 0:02:14\n",
      "epoch [9/10] batch [60/204] time 0.370 (0.365) data 0.000 (0.004) loss 0.0016 (0.0008) lr 5.1527e-04 eta 0:02:07\n",
      "epoch [9/10] batch [80/204] time 0.367 (0.364) data 0.001 (0.004) loss 0.0008 (0.0008) lr 5.1527e-04 eta 0:01:59\n",
      "epoch [9/10] batch [100/204] time 0.366 (0.364) data 0.000 (0.003) loss 0.0008 (0.0008) lr 5.1527e-04 eta 0:01:52\n",
      "epoch [9/10] batch [120/204] time 0.355 (0.363) data 0.000 (0.003) loss 0.0005 (0.0008) lr 5.1527e-04 eta 0:01:44\n",
      "epoch [9/10] batch [140/204] time 0.367 (0.363) data 0.000 (0.003) loss 0.0012 (0.0008) lr 5.1527e-04 eta 0:01:37\n",
      "epoch [9/10] batch [160/204] time 0.351 (0.363) data 0.000 (0.002) loss 0.0009 (0.0007) lr 5.1527e-04 eta 0:01:29\n",
      "epoch [9/10] batch [180/204] time 0.366 (0.363) data 0.003 (0.002) loss 0.0002 (0.0007) lr 5.1527e-04 eta 0:01:22\n",
      "epoch [9/10] batch [200/204] time 0.350 (0.362) data 0.000 (0.002) loss 0.0008 (0.0007) lr 5.1527e-04 eta 0:01:15\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [10/10] batch [20/204] time 0.354 (0.369) data 0.000 (0.011) loss 0.0007 (0.0006) lr 2.3873e-04 eta 0:01:07\n",
      "epoch [10/10] batch [40/204] time 0.347 (0.365) data 0.000 (0.006) loss 0.0003 (0.0008) lr 2.3873e-04 eta 0:00:59\n",
      "epoch [10/10] batch [60/204] time 0.367 (0.364) data 0.000 (0.004) loss 0.0003 (0.0007) lr 2.3873e-04 eta 0:00:52\n",
      "epoch [10/10] batch [80/204] time 0.354 (0.363) data 0.000 (0.004) loss 0.0002 (0.0007) lr 2.3873e-04 eta 0:00:45\n",
      "epoch [10/10] batch [100/204] time 0.360 (0.363) data 0.000 (0.003) loss 0.0003 (0.0007) lr 2.3873e-04 eta 0:00:37\n",
      "epoch [10/10] batch [120/204] time 0.356 (0.363) data 0.000 (0.003) loss 0.0010 (0.0007) lr 2.3873e-04 eta 0:00:30\n",
      "epoch [10/10] batch [140/204] time 0.359 (0.363) data 0.002 (0.002) loss 0.0004 (0.0007) lr 2.3873e-04 eta 0:00:23\n",
      "epoch [10/10] batch [160/204] time 0.358 (0.363) data 0.000 (0.002) loss 0.0010 (0.0007) lr 2.3873e-04 eta 0:00:15\n",
      "epoch [10/10] batch [180/204] time 0.360 (0.363) data 0.000 (0.002) loss 0.0009 (0.0007) lr 2.3873e-04 eta 0:00:08\n",
      "epoch [10/10] batch [200/204] time 0.360 (0.362) data 0.000 (0.002) loss 0.0002 (0.0007) lr 2.3873e-04 eta 0:00:01\n",
      "Checkpoint saved to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1/VLPromptLearner/model.pth.tar-10\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 11/11 [00:02<00:00,  5.43it/s]\n",
      "=> result\n",
      "* total: 1,053\n",
      "* correct: 1,029\n",
      "* accuracy: 97.7%\n",
      "* error: 2.3%\n",
      "* macro_f1: 97.7%\n",
      "Elapsed: 0:12:12\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_train.sh oxford_flowers 1 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b53ad-b277-403a-9bd9-534634842712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c95d69f-73c8-42e4-bb7c-61ad39f5ca36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting fixed seed: 1\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 10\n",
      "model_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']\n",
      "output_dir: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 1\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: new\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1\n",
      "RESUME: \n",
      "SEED: 1\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_1.pkl\n",
      "SUBSAMPLE NEW CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,410\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'image_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'prompt_learner.ctx', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.VPT', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "Loading weights to VLPromptLearner from \"output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed1/VLPromptLearner/model.pth.tar-10\" (epoch = 10)\n",
      "Evaluate on the *test* set\n",
      "100%|███████████████████████████████████████████| 15/15 [00:02<00:00,  5.94it/s]\n",
      "=> result\n",
      "* total: 1,410\n",
      "* correct: 1,078\n",
      "* accuracy: 76.5%\n",
      "* error: 23.5%\n",
      "* macro_f1: 71.6%\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_test.sh oxford_flowers 1 2 new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3409f28-0221-4596-bffd-5b3dfeca88cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04cd9d01-4624-4762-a9c9-435caba882d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this job and save the output to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: False\n",
      "head: \n",
      "load_epoch: None\n",
      "model_dir: \n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']\n",
      "output_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: base\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_2.pkl\n",
      "SUBSAMPLE BASE CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,053\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'image_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'prompt_learner.ctx', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.VPT', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "No checkpoint found, train from scratch\n",
      "Initialize tensorboard (log_dir=output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2/tensorboard)\n",
      "/home/hbcho991/.conda/envs/prometar/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "epoch [1/10] batch [20/204] time 0.365 (0.387) data 0.000 (0.016) loss 0.0008 (0.0013) lr 1.0000e-05 eta 0:13:01\n",
      "epoch [1/10] batch [40/204] time 0.352 (0.373) data 0.001 (0.009) loss 0.0001 (0.0012) lr 1.0000e-05 eta 0:12:26\n",
      "epoch [1/10] batch [60/204] time 0.353 (0.369) data 0.003 (0.006) loss 0.0004 (0.0012) lr 1.0000e-05 eta 0:12:09\n",
      "epoch [1/10] batch [80/204] time 0.352 (0.365) data 0.000 (0.005) loss 0.0009 (0.0011) lr 1.0000e-05 eta 0:11:55\n",
      "epoch [1/10] batch [100/204] time 0.355 (0.362) data 0.000 (0.004) loss 0.0011 (0.0011) lr 1.0000e-05 eta 0:11:42\n",
      "epoch [1/10] batch [120/204] time 0.353 (0.361) data 0.000 (0.004) loss 0.0009 (0.0011) lr 1.0000e-05 eta 0:11:33\n",
      "epoch [1/10] batch [140/204] time 0.357 (0.360) data 0.003 (0.003) loss 0.0002 (0.0011) lr 1.0000e-05 eta 0:11:23\n",
      "epoch [1/10] batch [160/204] time 0.345 (0.358) data 0.000 (0.003) loss 0.0015 (0.0011) lr 1.0000e-05 eta 0:11:13\n",
      "epoch [1/10] batch [180/204] time 0.364 (0.358) data 0.001 (0.003) loss 0.0004 (0.0011) lr 1.0000e-05 eta 0:11:05\n",
      "epoch [1/10] batch [200/204] time 0.345 (0.357) data 0.000 (0.003) loss 0.0004 (0.0011) lr 1.0000e-05 eta 0:10:57\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [2/10] batch [20/204] time 0.380 (0.373) data 0.000 (0.016) loss 0.0003 (0.0008) lr 2.5000e-03 eta 0:11:16\n",
      "epoch [2/10] batch [40/204] time 0.339 (0.361) data 0.000 (0.008) loss 0.0005 (0.0009) lr 2.5000e-03 eta 0:10:49\n",
      "epoch [2/10] batch [60/204] time 0.356 (0.358) data 0.000 (0.006) loss 0.0013 (0.0009) lr 2.5000e-03 eta 0:10:35\n",
      "epoch [2/10] batch [80/204] time 0.348 (0.358) data 0.000 (0.004) loss 0.0015 (0.0010) lr 2.5000e-03 eta 0:10:28\n",
      "epoch [2/10] batch [100/204] time 0.356 (0.357) data 0.000 (0.004) loss 0.0008 (0.0009) lr 2.5000e-03 eta 0:10:19\n",
      "epoch [2/10] batch [120/204] time 0.357 (0.356) data 0.000 (0.003) loss 0.0020 (0.0009) lr 2.5000e-03 eta 0:10:10\n",
      "epoch [2/10] batch [140/204] time 0.363 (0.355) data 0.000 (0.003) loss 0.0006 (0.0009) lr 2.5000e-03 eta 0:10:02\n",
      "epoch [2/10] batch [160/204] time 0.348 (0.355) data 0.000 (0.003) loss 0.0009 (0.0009) lr 2.5000e-03 eta 0:09:54\n",
      "epoch [2/10] batch [180/204] time 0.358 (0.354) data 0.000 (0.002) loss 0.0015 (0.0009) lr 2.5000e-03 eta 0:09:46\n",
      "epoch [2/10] batch [200/204] time 0.342 (0.354) data 0.000 (0.002) loss 0.0012 (0.0009) lr 2.5000e-03 eta 0:09:39\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [3/10] batch [20/204] time 0.348 (0.368) data 0.000 (0.014) loss 0.0000 (0.0010) lr 2.4388e-03 eta 0:09:53\n",
      "epoch [3/10] batch [40/204] time 0.361 (0.361) data 0.000 (0.008) loss 0.0001 (0.0010) lr 2.4388e-03 eta 0:09:34\n",
      "epoch [3/10] batch [60/204] time 0.348 (0.359) data 0.000 (0.005) loss 0.0013 (0.0010) lr 2.4388e-03 eta 0:09:24\n",
      "epoch [3/10] batch [80/204] time 0.349 (0.358) data 0.004 (0.004) loss 0.0013 (0.0009) lr 2.4388e-03 eta 0:09:16\n",
      "epoch [3/10] batch [100/204] time 0.358 (0.358) data 0.007 (0.004) loss 0.0003 (0.0009) lr 2.4388e-03 eta 0:09:08\n",
      "epoch [3/10] batch [120/204] time 0.341 (0.357) data 0.000 (0.003) loss 0.0007 (0.0009) lr 2.4388e-03 eta 0:09:00\n",
      "epoch [3/10] batch [140/204] time 0.347 (0.357) data 0.000 (0.003) loss 0.0006 (0.0009) lr 2.4388e-03 eta 0:08:52\n",
      "epoch [3/10] batch [160/204] time 0.341 (0.356) data 0.000 (0.003) loss 0.0012 (0.0009) lr 2.4388e-03 eta 0:08:44\n",
      "epoch [3/10] batch [180/204] time 0.346 (0.356) data 0.000 (0.002) loss 0.0007 (0.0009) lr 2.4388e-03 eta 0:08:36\n",
      "epoch [3/10] batch [200/204] time 0.345 (0.355) data 0.000 (0.002) loss 0.0005 (0.0009) lr 2.4388e-03 eta 0:08:28\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [4/10] batch [20/204] time 0.341 (0.365) data 0.000 (0.015) loss 0.0000 (0.0007) lr 2.2613e-03 eta 0:08:34\n",
      "epoch [4/10] batch [40/204] time 0.368 (0.362) data 0.006 (0.008) loss 0.0005 (0.0006) lr 2.2613e-03 eta 0:08:22\n",
      "epoch [4/10] batch [60/204] time 0.349 (0.360) data 0.000 (0.006) loss 0.0012 (0.0006) lr 2.2613e-03 eta 0:08:12\n",
      "epoch [4/10] batch [80/204] time 0.355 (0.358) data 0.000 (0.005) loss 0.0019 (0.0006) lr 2.2613e-03 eta 0:08:02\n",
      "epoch [4/10] batch [100/204] time 0.339 (0.356) data 0.000 (0.004) loss 0.0011 (0.0006) lr 2.2613e-03 eta 0:07:53\n",
      "epoch [4/10] batch [120/204] time 0.359 (0.355) data 0.000 (0.003) loss 0.0009 (0.0007) lr 2.2613e-03 eta 0:07:44\n",
      "epoch [4/10] batch [140/204] time 0.340 (0.355) data 0.000 (0.003) loss 0.0010 (0.0007) lr 2.2613e-03 eta 0:07:36\n",
      "epoch [4/10] batch [160/204] time 0.353 (0.354) data 0.002 (0.003) loss 0.0025 (0.0007) lr 2.2613e-03 eta 0:07:28\n",
      "epoch [4/10] batch [180/204] time 0.343 (0.353) data 0.000 (0.002) loss 0.0018 (0.0007) lr 2.2613e-03 eta 0:07:20\n",
      "epoch [4/10] batch [200/204] time 0.359 (0.353) data 0.000 (0.002) loss 0.0005 (0.0007) lr 2.2613e-03 eta 0:07:13\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [5/10] batch [20/204] time 0.361 (0.371) data 0.006 (0.016) loss 0.0003 (0.0008) lr 1.9847e-03 eta 0:07:27\n",
      "epoch [5/10] batch [40/204] time 0.347 (0.361) data 0.000 (0.008) loss 0.0006 (0.0009) lr 1.9847e-03 eta 0:07:06\n",
      "epoch [5/10] batch [60/204] time 0.350 (0.359) data 0.000 (0.006) loss 0.0007 (0.0008) lr 1.9847e-03 eta 0:06:57\n",
      "epoch [5/10] batch [80/204] time 0.335 (0.356) data 0.000 (0.005) loss 0.0001 (0.0007) lr 1.9847e-03 eta 0:06:47\n",
      "epoch [5/10] batch [100/204] time 0.357 (0.355) data 0.000 (0.004) loss 0.0007 (0.0007) lr 1.9847e-03 eta 0:06:39\n",
      "epoch [5/10] batch [120/204] time 0.372 (0.357) data 0.000 (0.003) loss 0.0011 (0.0008) lr 1.9847e-03 eta 0:06:33\n",
      "epoch [5/10] batch [140/204] time 0.347 (0.358) data 0.000 (0.003) loss 0.0007 (0.0008) lr 1.9847e-03 eta 0:06:28\n",
      "epoch [5/10] batch [160/204] time 0.355 (0.358) data 0.000 (0.003) loss 0.0009 (0.0007) lr 1.9847e-03 eta 0:06:20\n",
      "epoch [5/10] batch [180/204] time 0.348 (0.357) data 0.000 (0.002) loss 0.0015 (0.0008) lr 1.9847e-03 eta 0:06:12\n",
      "epoch [5/10] batch [200/204] time 0.350 (0.356) data 0.000 (0.002) loss 0.0004 (0.0008) lr 1.9847e-03 eta 0:06:04\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [6/10] batch [20/204] time 0.360 (0.369) data 0.000 (0.013) loss 0.0009 (0.0007) lr 1.6363e-03 eta 0:06:08\n",
      "epoch [6/10] batch [40/204] time 0.368 (0.370) data 0.002 (0.007) loss 0.0002 (0.0007) lr 1.6363e-03 eta 0:06:02\n",
      "epoch [6/10] batch [60/204] time 0.356 (0.364) data 0.004 (0.005) loss 0.0001 (0.0006) lr 1.6363e-03 eta 0:05:49\n",
      "epoch [6/10] batch [80/204] time 0.344 (0.360) data 0.000 (0.004) loss 0.0001 (0.0006) lr 1.6363e-03 eta 0:05:38\n",
      "epoch [6/10] batch [100/204] time 0.343 (0.358) data 0.000 (0.004) loss 0.0004 (0.0006) lr 1.6363e-03 eta 0:05:29\n",
      "epoch [6/10] batch [120/204] time 0.407 (0.357) data 0.000 (0.003) loss 0.0007 (0.0006) lr 1.6363e-03 eta 0:05:21\n",
      "epoch [6/10] batch [140/204] time 0.352 (0.356) data 0.003 (0.003) loss 0.0008 (0.0007) lr 1.6363e-03 eta 0:05:13\n",
      "epoch [6/10] batch [160/204] time 0.359 (0.355) data 0.000 (0.003) loss 0.0011 (0.0007) lr 1.6363e-03 eta 0:05:05\n",
      "epoch [6/10] batch [180/204] time 0.344 (0.355) data 0.000 (0.002) loss 0.0007 (0.0007) lr 1.6363e-03 eta 0:04:58\n",
      "epoch [6/10] batch [200/204] time 0.340 (0.354) data 0.000 (0.002) loss 0.0009 (0.0007) lr 1.6363e-03 eta 0:04:50\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [7/10] batch [20/204] time 0.352 (0.368) data 0.000 (0.013) loss 0.0006 (0.0007) lr 1.2500e-03 eta 0:04:53\n",
      "epoch [7/10] batch [40/204] time 0.347 (0.359) data 0.000 (0.007) loss 0.0004 (0.0007) lr 1.2500e-03 eta 0:04:38\n",
      "epoch [7/10] batch [60/204] time 0.351 (0.357) data 0.000 (0.005) loss 0.0006 (0.0007) lr 1.2500e-03 eta 0:04:29\n",
      "epoch [7/10] batch [80/204] time 0.356 (0.354) data 0.000 (0.004) loss 0.0002 (0.0007) lr 1.2500e-03 eta 0:04:20\n",
      "epoch [7/10] batch [100/204] time 0.368 (0.353) data 0.004 (0.004) loss 0.0010 (0.0007) lr 1.2500e-03 eta 0:04:13\n",
      "epoch [7/10] batch [120/204] time 0.374 (0.354) data 0.000 (0.003) loss 0.0003 (0.0007) lr 1.2500e-03 eta 0:04:06\n",
      "epoch [7/10] batch [140/204] time 0.366 (0.354) data 0.003 (0.003) loss 0.0016 (0.0007) lr 1.2500e-03 eta 0:03:59\n",
      "epoch [7/10] batch [160/204] time 0.346 (0.354) data 0.000 (0.003) loss 0.0009 (0.0007) lr 1.2500e-03 eta 0:03:52\n",
      "epoch [7/10] batch [180/204] time 0.360 (0.353) data 0.000 (0.003) loss 0.0009 (0.0007) lr 1.2500e-03 eta 0:03:44\n",
      "epoch [7/10] batch [200/204] time 0.347 (0.352) data 0.000 (0.002) loss 0.0002 (0.0007) lr 1.2500e-03 eta 0:03:36\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [8/10] batch [20/204] time 0.357 (0.363) data 0.000 (0.014) loss 0.0006 (0.0006) lr 8.6373e-04 eta 0:03:34\n",
      "epoch [8/10] batch [40/204] time 0.368 (0.358) data 0.000 (0.007) loss 0.0020 (0.0007) lr 8.6373e-04 eta 0:03:25\n",
      "epoch [8/10] batch [60/204] time 0.327 (0.357) data 0.000 (0.005) loss 0.0005 (0.0007) lr 8.6373e-04 eta 0:03:16\n",
      "epoch [8/10] batch [80/204] time 0.337 (0.349) data 0.003 (0.004) loss 0.0007 (0.0008) lr 8.6373e-04 eta 0:03:05\n",
      "epoch [8/10] batch [100/204] time 0.332 (0.347) data 0.000 (0.004) loss 0.0006 (0.0008) lr 8.6373e-04 eta 0:02:57\n",
      "epoch [8/10] batch [120/204] time 0.326 (0.344) data 0.000 (0.003) loss 0.0005 (0.0007) lr 8.6373e-04 eta 0:02:49\n",
      "epoch [8/10] batch [140/204] time 0.326 (0.342) data 0.002 (0.003) loss 0.0006 (0.0008) lr 8.6373e-04 eta 0:02:41\n",
      "epoch [8/10] batch [160/204] time 0.324 (0.340) data 0.000 (0.003) loss 0.0017 (0.0007) lr 8.6373e-04 eta 0:02:33\n",
      "epoch [8/10] batch [180/204] time 0.325 (0.338) data 0.000 (0.002) loss 0.0014 (0.0007) lr 8.6373e-04 eta 0:02:26\n",
      "epoch [8/10] batch [200/204] time 0.346 (0.339) data 0.002 (0.002) loss 0.0025 (0.0007) lr 8.6373e-04 eta 0:02:19\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [9/10] batch [20/204] time 0.346 (0.365) data 0.000 (0.014) loss 0.0005 (0.0007) lr 5.1527e-04 eta 0:02:21\n",
      "epoch [9/10] batch [40/204] time 0.346 (0.357) data 0.000 (0.007) loss 0.0004 (0.0006) lr 5.1527e-04 eta 0:02:11\n",
      "epoch [9/10] batch [60/204] time 0.347 (0.355) data 0.000 (0.005) loss 0.0007 (0.0007) lr 5.1527e-04 eta 0:02:03\n",
      "epoch [9/10] batch [80/204] time 0.353 (0.354) data 0.000 (0.004) loss 0.0000 (0.0007) lr 5.1527e-04 eta 0:01:56\n",
      "epoch [9/10] batch [100/204] time 0.342 (0.354) data 0.000 (0.004) loss 0.0008 (0.0007) lr 5.1527e-04 eta 0:01:49\n",
      "epoch [9/10] batch [120/204] time 0.352 (0.354) data 0.000 (0.003) loss 0.0014 (0.0007) lr 5.1527e-04 eta 0:01:41\n",
      "epoch [9/10] batch [140/204] time 0.351 (0.354) data 0.000 (0.003) loss 0.0011 (0.0007) lr 5.1527e-04 eta 0:01:34\n",
      "epoch [9/10] batch [160/204] time 0.358 (0.354) data 0.003 (0.003) loss 0.0012 (0.0007) lr 5.1527e-04 eta 0:01:27\n",
      "epoch [9/10] batch [180/204] time 0.339 (0.354) data 0.000 (0.003) loss 0.0006 (0.0007) lr 5.1527e-04 eta 0:01:20\n",
      "epoch [9/10] batch [200/204] time 0.370 (0.355) data 0.001 (0.002) loss 0.0007 (0.0007) lr 5.1527e-04 eta 0:01:13\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "epoch [10/10] batch [20/204] time 0.348 (0.380) data 0.000 (0.016) loss 0.0007 (0.0005) lr 2.3873e-04 eta 0:01:09\n",
      "epoch [10/10] batch [40/204] time 0.346 (0.367) data 0.000 (0.008) loss 0.0011 (0.0006) lr 2.3873e-04 eta 0:01:00\n",
      "epoch [10/10] batch [60/204] time 0.355 (0.362) data 0.004 (0.006) loss 0.0007 (0.0007) lr 2.3873e-04 eta 0:00:52\n",
      "epoch [10/10] batch [80/204] time 0.371 (0.359) data 0.000 (0.005) loss 0.0014 (0.0006) lr 2.3873e-04 eta 0:00:44\n",
      "epoch [10/10] batch [100/204] time 0.342 (0.358) data 0.000 (0.004) loss 0.0000 (0.0006) lr 2.3873e-04 eta 0:00:37\n",
      "epoch [10/10] batch [120/204] time 0.347 (0.356) data 0.000 (0.003) loss 0.0004 (0.0006) lr 2.3873e-04 eta 0:00:29\n",
      "epoch [10/10] batch [140/204] time 0.343 (0.355) data 0.000 (0.003) loss 0.0000 (0.0006) lr 2.3873e-04 eta 0:00:22\n",
      "epoch [10/10] batch [160/204] time 0.354 (0.356) data 0.000 (0.003) loss 0.0007 (0.0006) lr 2.3873e-04 eta 0:00:15\n",
      "epoch [10/10] batch [180/204] time 0.340 (0.355) data 0.000 (0.003) loss 0.0003 (0.0006) lr 2.3873e-04 eta 0:00:08\n",
      "epoch [10/10] batch [200/204] time 0.354 (0.354) data 0.000 (0.002) loss 0.0010 (0.0006) lr 2.3873e-04 eta 0:00:01\n",
      "Checkpoint saved to output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2/VLPromptLearner/model.pth.tar-10\n",
      "Finish training\n",
      "Deploy the last-epoch model\n",
      "Evaluate on the *test* set\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "100%|███████████████████████████████████████████| 11/11 [00:02<00:00,  5.05it/s]\n",
      "=> result\n",
      "* total: 1,053\n",
      "* correct: 1,024\n",
      "* accuracy: 97.2%\n",
      "* error: 2.8%\n",
      "* macro_f1: 97.1%\n",
      "Elapsed: 0:12:04\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_train.sh oxford_flowers 2 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d85806-ffa7-42d7-be33-10512fefdda3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655e5711-8679-4369-8e58-29950223d58a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting fixed seed: 2\n",
      "***************\n",
      "** Arguments **\n",
      "***************\n",
      "backbone: \n",
      "config_file: configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\n",
      "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
      "eval_only: True\n",
      "head: \n",
      "load_epoch: 10\n",
      "model_dir: output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "no_train: False\n",
      "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']\n",
      "output_dir: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "resume: \n",
      "root: /home/dataset\n",
      "seed: 2\n",
      "source_domains: None\n",
      "target_domains: None\n",
      "trainer: ProMetaR\n",
      "transforms: None\n",
      "************\n",
      "** Config **\n",
      "************\n",
      "DATALOADER:\n",
      "  K_TRANSFORMS: 1\n",
      "  NUM_WORKERS: 4\n",
      "  RETURN_IMG0: False\n",
      "  TEST:\n",
      "    BATCH_SIZE: 100\n",
      "    SAMPLER: SequentialSampler\n",
      "  TRAIN_U:\n",
      "    BATCH_SIZE: 32\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAME_AS_X: True\n",
      "    SAMPLER: RandomSampler\n",
      "  TRAIN_X:\n",
      "    BATCH_SIZE: 4\n",
      "    N_DOMAIN: 0\n",
      "    N_INS: 16\n",
      "    SAMPLER: RandomSampler\n",
      "DATASET:\n",
      "  ALL_AS_UNLABELED: False\n",
      "  CIFAR_C_LEVEL: 1\n",
      "  CIFAR_C_TYPE: \n",
      "  NAME: OxfordFlowers\n",
      "  NUM_LABELED: -1\n",
      "  NUM_SHOTS: 16\n",
      "  ROOT: /home/dataset\n",
      "  SOURCE_DOMAINS: ()\n",
      "  STL10_FOLD: -1\n",
      "  SUBSAMPLE_CLASSES: new\n",
      "  TARGET_DOMAINS: ()\n",
      "  VAL_PERCENT: 0.1\n",
      "INPUT:\n",
      "  COLORJITTER_B: 0.4\n",
      "  COLORJITTER_C: 0.4\n",
      "  COLORJITTER_H: 0.1\n",
      "  COLORJITTER_S: 0.4\n",
      "  CROP_PADDING: 4\n",
      "  CUTOUT_LEN: 16\n",
      "  CUTOUT_N: 1\n",
      "  GB_K: 21\n",
      "  GB_P: 0.5\n",
      "  GN_MEAN: 0.0\n",
      "  GN_STD: 0.15\n",
      "  INTERPOLATION: bicubic\n",
      "  NO_TRANSFORM: False\n",
      "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
      "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
      "  RANDAUGMENT_M: 10\n",
      "  RANDAUGMENT_N: 2\n",
      "  RGS_P: 0.2\n",
      "  RRCROP_SCALE: (0.08, 1.0)\n",
      "  SIZE: (224, 224)\n",
      "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    NAME: ViT-B/16\n",
      "    PRETRAINED: True\n",
      "  HEAD:\n",
      "    ACTIVATION: relu\n",
      "    BN: True\n",
      "    DROPOUT: 0.0\n",
      "    HIDDEN_LAYERS: ()\n",
      "    NAME: \n",
      "  INIT_WEIGHTS: \n",
      "OPTIM:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.0025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-05\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OPTIM_VNET:\n",
      "  ADAM_BETA1: 0.9\n",
      "  ADAM_BETA2: 0.999\n",
      "  BASE_LR_MULT: 0.1\n",
      "  GAMMA: 0.1\n",
      "  LR: 0.00025\n",
      "  LR_SCHEDULER: cosine\n",
      "  MAX_EPOCH: 10\n",
      "  MOMENTUM: 0.9\n",
      "  NAME: sgd\n",
      "  NEW_LAYERS: ()\n",
      "  RMSPROP_ALPHA: 0.99\n",
      "  SGD_DAMPNING: 0\n",
      "  SGD_NESTEROV: False\n",
      "  STAGED_LR: False\n",
      "  STEPSIZE: (-1,)\n",
      "  WARMUP_CONS_LR: 1e-06\n",
      "  WARMUP_EPOCH: 1\n",
      "  WARMUP_MIN_LR: 1e-05\n",
      "  WARMUP_RECOUNT: True\n",
      "  WARMUP_TYPE: constant\n",
      "  WEIGHT_DECAY: 0.0005\n",
      "OUTPUT_DIR: output/test_new/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2\n",
      "RESUME: \n",
      "SEED: 2\n",
      "TEST:\n",
      "  COMPUTE_CMAT: False\n",
      "  EVALUATOR: Classification\n",
      "  FINAL_MODEL: last_step\n",
      "  NO_TEST: False\n",
      "  PER_CLASS_RESULT: False\n",
      "  SPLIT: test\n",
      "TRAIN:\n",
      "  CHECKPOINT_FREQ: 0\n",
      "  COUNT_ITER: train_x\n",
      "  PRINT_FREQ: 20\n",
      "TRAINER:\n",
      "  CDAC:\n",
      "    CLASS_LR_MULTI: 10\n",
      "    P_THRESH: 0.95\n",
      "    RAMPUP_COEF: 30\n",
      "    RAMPUP_ITRS: 1000\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    TOPK_MATCH: 5\n",
      "  CROSSGRAD:\n",
      "    ALPHA_D: 0.5\n",
      "    ALPHA_F: 0.5\n",
      "    EPS_D: 1.0\n",
      "    EPS_F: 1.0\n",
      "  DAEL:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DAELDG:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 0.5\n",
      "  DDAIG:\n",
      "    ALPHA: 0.5\n",
      "    CLAMP: False\n",
      "    CLAMP_MAX: 1.0\n",
      "    CLAMP_MIN: -1.0\n",
      "    G_ARCH: \n",
      "    LMDA: 0.3\n",
      "    WARMUP: 0\n",
      "  DOMAINMIX:\n",
      "    ALPHA: 1.0\n",
      "    BETA: 1.0\n",
      "    TYPE: crossdomain\n",
      "  ENTMIN:\n",
      "    LMDA: 0.001\n",
      "  FIXMATCH:\n",
      "    CONF_THRE: 0.95\n",
      "    STRONG_TRANSFORMS: ()\n",
      "    WEIGHT_U: 1.0\n",
      "  IVLP:\n",
      "    CTX_INIT: a photo of a\n",
      "    N_CTX_TEXT: 2\n",
      "    N_CTX_VISION: 2\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  M3SDA:\n",
      "    LMDA: 0.5\n",
      "    N_STEP_F: 4\n",
      "  MCD:\n",
      "    N_STEP_F: 4\n",
      "  MEANTEACHER:\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 5\n",
      "    WEIGHT_U: 1.0\n",
      "  MIXMATCH:\n",
      "    MIXUP_BETA: 0.75\n",
      "    RAMPUP: 20000\n",
      "    TEMP: 2.0\n",
      "    WEIGHT_U: 100.0\n",
      "  MME:\n",
      "    LMDA: 0.1\n",
      "  NAME: ProMetaR\n",
      "  PROMETAR:\n",
      "    ADAPT_LR: 0.0005\n",
      "    CTX_INIT: a photo of a\n",
      "    DIM_RATE: 8\n",
      "    FAST_ADAPTATION: False\n",
      "    LR_RATIO: 0.0005\n",
      "    MIXUP_ALPHA: 0.5\n",
      "    MIXUP_BETA: 0.5\n",
      "    N_CTX_TEXT: 4\n",
      "    N_CTX_VISION: 4\n",
      "    PREC: fp16\n",
      "    PROMPT_DEPTH_TEXT: 9\n",
      "    PROMPT_DEPTH_VISION: 9\n",
      "  SE:\n",
      "    CONF_THRE: 0.95\n",
      "    EMA_ALPHA: 0.999\n",
      "    RAMPUP: 300\n",
      "USE_CUDA: True\n",
      "VERBOSE: True\n",
      "VERSION: 1\n",
      "Collecting env info ...\n",
      "** System info **\n",
      "PyTorch version: 1.9.0+cu111\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 22.04.3 LTS (x86_64)\n",
      "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.22.1\n",
      "Libc version: glibc-2.35\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Python platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 12.3.103\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 4090\n",
      "GPU 4: NVIDIA GeForce RTX 4090\n",
      "GPU 5: NVIDIA GeForce RTX 4090\n",
      "GPU 6: NVIDIA GeForce RTX 4090\n",
      "GPU 7: NVIDIA GeForce RTX 4090\n",
      "\n",
      "Nvidia driver version: 535.129.03\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.24.4\n",
      "[pip3] torch==1.9.0+cu111\n",
      "[pip3] torchaudio==0.9.0\n",
      "[pip3] torchvision==0.10.0+cu111\n",
      "[conda] numpy                     1.24.4                   pypi_0    pypi\n",
      "[conda] torch                     1.9.0+cu111              pypi_0    pypi\n",
      "[conda] torchaudio                0.9.0                    pypi_0    pypi\n",
      "[conda] torchvision               0.10.0+cu111             pypi_0    pypi\n",
      "        Pillow (10.4.0)\n",
      "\n",
      "Loading trainer: ProMetaR\n",
      "Loading dataset: OxfordFlowers\n",
      "Reading split from /home/hbcho991/prometar/ProMetaR/split_zhou_OxfordFlowers.json\n",
      "Loading preprocessed few-shot data from /home/hbcho991/prometar/ProMetaR/split_fewshot/shot_16-seed_2.pkl\n",
      "SUBSAMPLE NEW CLASSES!\n",
      "Building transform_train\n",
      "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
      "+ random flip\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "Building transform_test\n",
      "+ resize the smaller edge to 224\n",
      "+ 224x224 center crop\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      "---------  -------------\n",
      "Dataset    OxfordFlowers\n",
      "# classes  51\n",
      "# train_x  816\n",
      "# val      204\n",
      "# test     1,410\n",
      "---------  -------------\n",
      "Loading CLIP (backbone: ViT-B/16)\n",
      "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
      "Building custom CLIP\n",
      "Independent V-L design\n",
      "Initial text context: \"a photo of a\"\n",
      "Number of context words (tokens) for Language prompting: 4\n",
      "Number of context words (tokens) for Vision prompting: 4\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'image_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.VPT', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'prompt_learner.ctx', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow'}\n",
      "Parameters count: 18\n",
      "Loading evaluator: Classification\n",
      "Loading weights to VLPromptLearner from \"output/base2new/train_base/oxford_flowers/shots_16/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx/seed2/VLPromptLearner/model.pth.tar-10\" (epoch = 10)\n",
      "Evaluate on the *test* set\n",
      "100%|███████████████████████████████████████████| 15/15 [00:02<00:00,  5.86it/s]\n",
      "=> result\n",
      "* total: 1,410\n",
      "* correct: 1,073\n",
      "* accuracy: 76.1%\n",
      "* error: 23.9%\n",
      "* macro_f1: 70.9%\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/base2new_test.sh oxford_flowers 2 2 new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3990017-805d-4fbf-911d-4f490859d248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df82dfbc-64a5-44a9-95e4-525a7bba0290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde835a3-4a9c-468f-8e34-fa1612a42160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c61a2-9ca8-4375-b185-0bb60dea4eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00345bd-2c1d-427b-b69b-1c9323f21bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aba2ba-f520-420d-b90b-7c075ad70e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa8689c-e72a-4755-a49a-da4731c3dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f5b3f9d-37e1-418b-9a58-ec9f2aa94739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1694a7cf-d5bd-40fe-b4e0-99afc72df187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc7a2115-520c-4057-a016-9c3b918223f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0+cu111'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fedbc340-40b5-4f9f-8d58-de422f4ff582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.collect_env import get_pretty_env_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9529783-333e-4fa9-8696-65d4ed7c4770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PyTorch version: 1.9.0+cu111\\nIs debug build: False\\nCUDA used to build PyTorch: 11.1\\nROCM used to build PyTorch: N/A\\n\\nOS: Ubuntu 22.04.3 LTS (x86_64)\\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\\nClang version: Could not collect\\nCMake version: version 3.22.1\\nLibc version: glibc-2.35\\n\\nPython version: 3.8 (64-bit runtime)\\nPython platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.17\\nIs CUDA available: True\\nCUDA runtime version: 12.3.103\\nGPU models and configuration: \\nGPU 0: NVIDIA GeForce RTX 4090\\nGPU 1: NVIDIA GeForce RTX 4090\\nGPU 2: NVIDIA GeForce RTX 4090\\nGPU 3: NVIDIA GeForce RTX 4090\\nGPU 4: NVIDIA GeForce RTX 4090\\nGPU 5: NVIDIA GeForce RTX 4090\\nGPU 6: NVIDIA GeForce RTX 4090\\nGPU 7: NVIDIA GeForce RTX 4090\\n\\nNvidia driver version: 535.129.03\\ncuDNN version: Could not collect\\nHIP runtime version: N/A\\nMIOpen runtime version: N/A\\n\\nVersions of relevant libraries:\\n[pip3] numpy==1.24.4\\n[pip3] torch==1.9.0+cu111\\n[pip3] torchaudio==0.9.0\\n[pip3] torchvision==0.10.0+cu111\\n[conda] numpy                     1.24.4                   pypi_0    pypi\\n[conda] torch                     1.9.0+cu111              pypi_0    pypi\\n[conda] torchaudio                0.9.0                    pypi_0    pypi\\n[conda] torchvision               0.10.0+cu111             pypi_0    pypi'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pretty_env_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7902427-9d3e-43b4-b9ce-cdc662a63426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0a620-b1de-428c-aa24-0843205ed511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b97b6-f127-4162-9e11-d5a49899dee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometar",
   "language": "python",
   "name": "prometar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
